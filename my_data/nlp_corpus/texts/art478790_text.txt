С момента выхода прошлой публикации в мире языка Julia произошло много интересного:
Она заняла все первые места в плане роста вспомогательных пакетов. За это я и люблю статистику — главное выбрать удобную единицу измерения, например проценты как в приведенном ресурсе.
Вышла версия 1.3.0 — из самых масштабных нововведений там модернизация менеджера пакетов и появление многопоточного параллелизма.
Джулия заручается поддержкой Nvidia.
Американский департамент перспективных исследований в области энергетики выделил кучу денег на решение задач оптимизации.
В то же время заметен рост интереса со стороны разработчиков, что выражается обильными бенчмаркингами:
Международное энергетическое агенство проверяет пакеты реализующие многомерную оптимизацию.
Датасаянтисты тестят работу с GPU.
Ни капли не предвзятые ребята сравнивают интеграторы для дифуров.
А энтузиасты сравнивают языки на базовых задачах.
Мы же просто радуемся новым и удобным инструментам и продолжаем их изучать. Сегодняшний вечер будет посвящен текстовому анализу, поиску скрытого смысла в выступлениях президентов и генерации текста в духе Шекспира и джулиа-программиста, а на сладкое — скормим рекуррентной сети 40000 пирожков.
Недавно здесь на Хабре был выполнен обзор пакетов для Julia позволяющие проводить исследования в области NLP — Julia NLP. Обрабатываем тексты. Так что сразу приступим у делу и начнем с пакета TextAnalysis.
TextAnalisys.
Пусть задан некоторый текст, который мы представляем в виде строкового документа:
Для удобной работы с большим количеством документов есть возможность менять поля, например заглавия, а также, чтоб упростить обработку, можем удалять пунктуацию и заглавные буквы:
что позволяет строить незахламленные n-граммы для слов:
Понятное дело, что знаки пунктуации и слова с заглавными буквами будут отдельными единицами в словаре, что будет мешать качественно оценить частотные вхождения конкретных слагаемых нашего текста, посему от них и избавились. Для n-грамм нетрудно найти множество всяческих интересных применений, например, с их помощью можно осуществлять нечеткий поиск в тексте, ну а так как мы просто туристы, то обойдемся игрушечными примерами, а именно генерацией текста с помощью цепей Маркова.
Procházení modelového grafu.
Марковская цепь — это дискретная модель марковского процесса, состоящего в изменении системы, которое учитывает только ее (модели) предыдущее состояние. Образно говоря, можно воспринимать данную конструкцию как вероятностный клеточный автомат. N-граммы вполне уживаются с такой концепцией: любое слово из лексикона связано с каждым другим связью разной толщины, которая определяется частотой встречи конкретных пар слов (грамм) в тексте.
Марковская цепь для строки "ABABD".
Реализация алгоритма сама по себе уже отличное занятие на вечер, но на Julia уже есть замечательный пакет Markovify, который создавался как раз для этих целей. Тщательно пролистав мануал на чешском, приступим к нашим лингвистическим экзекуциям.
Разбив текст на токены (например, на слова).
составляем модель первого порядка (учитываются только ближайшие соседи):
Засим приступим к реализации функции генерирующей фразы на основе предоставленной модели. Она принимает, собственно, модель, метод обхода, и количество фраз, которое хочется получить:
Разработчиком пакета предоставлены две функции обхода: walk и walk2 (вторая работает дольше, но дает более уникальные конструкции), а также вы всегда можете определить свой вариант. Опробуем же:
Конечно, велик соблазн пробовать на русских текстах, особенно на белых стихах. Для русского языка, в силу его сложности, большая часть фраз генерируется нечитабельной. Плюс, как уже упоминалось, спецсимволы требуют особой внимательности, посему либо сохраняем документы, с которых собирается текст с кодировкой UTF-8, либо используем дополнительные средства.
По совету своей сестры, почистив пару книг Остера от спецсимволов и всяких разделителей и задав второй порядок для n-грамм, получил такой набор фразеологизмов:
Она заверила, что именно по такой методике конструируются мысли в женском мозгу… кхм, и кто я такой чтобы спорить.
Analyze it.
В директории пакета TextAnalysis можно найти примеры текстовых данных, одним из которых является сборник выступлений американских президентов перед конгрессом.
Считав эти файлы и сформировав из них корпус, а также почистив его от пунктуации просмотрим общий лексикон всех выступлений:
Может быть интересно посмотреть в каких документах есть конкретные слова, например, глянем как у нас дела с обещаниями:
или с частотами местоимений:
Так наверное ученые и насилуют журналистов и проявляется превратное отношение к изучаемым данным.
Математрицы.
По настоящему дистрибутивная семантика начинается тогда, когда тексты, граммы и лексемы превращаются в вектора и матрицы.
Терм-документная матрица ( DTM ) — это матрица которая имеет размер, где — количество документов в корпусе, а — размер словаря корпуса т.е. количество слов (уникальных) которые встречаются в нашем корпусе. В i -й строке, j -м столбце матрицы находится число — сколько раз в i -м тексте встретилось j -е слово.
Здесь исходными единицами являются термы.
Погодите.
Надо будет почитать поподробней.
Из матриц термов также можно извлекать всякие интересные данные. Скажем, частоты вхождения специфичных слов в документы.
или схожесть документов по неким скрытым темам:
Графики показывают, насколько каждая из трех тем раскрыта в текстах выступлений.
или кластеризацию слов по темам, или к примеру, схожесть лексиконов и предпочтение определенных тем в разных документах.
Вполне закономерные результаты, выступления-то однотипные. На самом деле NLP довольно интересная наука, и из правильно подготовленных данных можно извлечь много полезной информации: на данном ресурсе можно найти множество примеров ( Распознание автора в комментариях, применение LDA и тд).
Ну и чтоб далеко не ходить, сгенерируем фразы для идеального президента:
Long Short Term Memory.
Ну и как же без нейронных сетей! Они на этом поприще собирают лавры с нарастающей скоростью, и окружение языка Julia всячески этому способствует. Для любознательных можно посоветовать пакет Knet, который, в отличие от ранее рассмотренного нами Flux, производит работу с архитектурами нейронных сетей не как с конструктором из модулей, а по большей части работает с итераторами и потоками. Это может представлять академический интерес и способствовать более глубокому пониманию процесса обучения, а также дает высокую производительность вычислений. Пройдя по предоставленной выше ссылке вы найдете руководство, примеры и материал для самообучения (скажем, там показано как на рекуррентных сетях создать генератор шекспировского текста или джулиакода). Однако некоторые функции пакета Knet реализованы только для графического процессора, так что пока продолжим обкатывать Flux.
Примеры выполненные с использованием Flux — от разметки по частям речи до выявления эмоциональной тональности.
RNN в Flux.
RNN в Knet.
Статья про LSTM и русская версия.
Сетки на TPU-шках.
Одним из типовых примеров работы рекуррентных сетей частенько служит модель, которой посимвольно скармливают сонеты Шекспира:
Если смотреть прищурившись и не знать английский, то пьеса кажется вполне настоящей.
На русском понять проще.
Но гораздо интересней опробовать на великом и могучем, и хотя он лексически весьма сложнее, можно в качестве данных использовать литературу попримитивней, а именно, еще недавно слывшие авангардным течением современной поэзии — стишки-пирожки.
Сбор данных.
Пирожки и порошки — ритмичные четверостишия, частенько без рифмы, набранные в нижнем регистре и без знаков препинания.
Выбор пал на сайт poetory.ru на котором админит товарищ hior. Долгое отсутствие ответа на запрос предоставить данные послужил поводом для начала изучения парсинга сайтов. Бегло просмотрев самоучитель HTML получаем зачаточное понимание устройства интернет-страниц. Далее, находим средства языка Julia для работы в подобного рода областях:
HTTP.jl — HTTP-клиент и функциональность сервера для Джулии.
Gumbo.jl — парсинг html-вёрстки и не только.
Cascadia.jl — вспомогательный пакет для Gumbo.
Засим реализуем скрипт, листающий страницы поэтория и сохраняющий пирожки в текстовый документ:
Более детально он разобран в юпитерском блокноте. Соберем пирожки и порохи в единую строку:
И посмотрим на используемый алфавит:
Перед запуском процесса проверяйте загруженные данные.
Ай-ай-ай, какое безобразие! Некоторые пользователи нарушают правила (бывает люди просто самовыражаются внося в данные шумы). Так что почистим наш символьный корпус от мусора.
Получили более приемлемый набор символов. Самое большое откровение сегодняшнего дня состоит в том, что с точки зрения машинного кода существует как минимум три разных пробела — трудно же живется охотникам за данными.
Теперь же можно подключать Flux с последующим представлением данных в виде onehot-векторов:
Задаем модель из парочки LSTM-слоев, полносвязного перцептрона и софтмакса, а также житейские мелочи, а ля функцию потерь и оптимизатор:
Модель готова к обучению, так что запустив нижепредставленную строку, можно идти заниматься своими делами, времезатратность которых подбирается в соответствии с мощностью вашей вычислительной машины. В моем случае это две лекции по философии, которые на кой-то чёрт нам поставили поздно вечером.
Собрав генератор семплов можно начать пожинать плоды своих трудов.
Легкое разочарование из-за немного завышенных ожиданий. Хотя сеть имеет на входе только последовательность символов и может оперировать лишь частотами их встречи друг за другом, она вполне уловила структуру набора данных, выделила некое подобие слов и в некоторых случаях даже проявила способность к сохранению ритма. Возможно, в улучшении поможет выявление семантической близости.
Веса натренированной сети можно сохранить на диск, а затем с легкостью считать.
С прозой тоже выходит только абстрактная кибер-психоделия. Были попытки повысить качество шириной и глубинной сети, а также разнообразием и обилием данных. За предоставленные текстовые корпуса отдельное спасибо величайшему популяризатору русского языка.
А вот если тренировать нейросеть на исходном коде языка Julia то выходит довольно прикольно:
Прибавив к этому возможность метапрограммирования, мы получим программу пишущую и выполняющую, может даже свой собственный, код! Ну или это будет находкой для дизайнеров фильмов про хакеров.
В общем, начало положено, а дальше уже как фантазия укажет. Во первых, следует обзавестись добротным оборудованием, чтобы долгие расчеты не душили желание экспериментировать. Во вторых, нужно глубже изучать методики и эвристики, что позволит конструировать более качественные и оптимизированные модели. На данном ресурсе достаточно найти все что связано с Natural Language Processing, после чего вполне можно научить свою нейросеть генерировать стихи или пойти на хакатон по анализу текстов.
На этом позвольте откланяться. Данные для обучения в облаке, листинги — на гитхабе, огонь в глазах, игла в яйце, яйцо в утке, и всем спокойной ночи!