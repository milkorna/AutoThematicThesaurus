Построение моделей Трансформера для больших последовательностей с помощью методов разреженного внимания