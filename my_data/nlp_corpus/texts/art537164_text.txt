Исследователи из Обернского университета пришли к выводу, что многие ИИ, предназначенные для обработки естественного языка (Natural Language Processing, NLP), не замечают, когда слова в предложении перемешиваются, а его значение меняется. Это показывает, что ИИ на самом деле не понимают язык, и создает проблемы в обучении систем NLP.
Исследователи из Обернского университета в Алабаме совместно со специалистами компании Adobe Research попытались заставить систему NLP объяснить, почему разные предложения означают одно и то же. Они поняли, что перетасовка слов в предложении не влияла на объяснения.
«Это общая проблема для всех моделей NLP», — считает руководитель исследования Ан Нгуен.
Команда изучила несколько современных систем NLP, основанных на разработанной Google языковой модели BERT. Все эти системы показывают лучшие результаты, чем люди, в тестах GLUE (General Language Understanding Evaluation) — стандартных тестах, предназначенных для проверки понимания языка, и включающих такие задачи, как выявление перефразирований, определение того, выражает предложение положительный или отрицательный настрой, и задачи на вербальное мышление.
Ученые обнаружили, что ИИ-системы не могут определить, когда слова в предложении перемешаны, даже если новый порядок слов изменил значение предложения или сделал его бессмысленным. Например, системы правильно определили, что предложения «Вызывает ли марихуана рак?» и «Курение марихуаны может вызвать рак легких?» означали одно и то же, хоть и были перефразированы. Однако системы были уверены в том, что у предложений «Вы курите рак, как может дать легкое марихуаны?» и «Легкие могут дать курение марихуаны, как вы рак?» такой же смысл. Системы также решили, что предложения с противоположным значением — например, «Вызывает ли марихуана рак?» и «Вызывает ли рак марихуану?» — были одинаковыми по смыслу. Ответы тестируемых систем не изменились при перемешивании слов в от 75% до 90% случаев.
Как указывает журнал MIT Technology Review, модели улавливают несколько ключевых слов в предложении, в каком бы порядке они ни были. Они не понимают язык так, как люди. Кроме того, исследование показывает, что тест GLUE неспособен оценить понимание языка. Многие исследователи уже начали использовать более сложный набор тестов под названием SuperGLUE, но Нгуен подозревает, что и у него будут аналогичные недостатки.
Проблему непонимания машинами естественного языка подтвердил также ученый ​​Йошуа Бенжио, специализирующийся на нейросетях и глубоком обучении. Он и его коллеги  обнаружили, что изменение порядка слов в разговоре не всегда меняет ответы чат-ботов.
Исправить ситуацию можно, если принуждать модель сосредотачивать внимание на порядке слов — обучать ее выполнять задачи, для которых порядок слов имеет значение (например, обнаруживать грамматические ошибки). Тем не менее, как указывает Ан Нгуен, исследование его команды подчеркивает, насколько сложно создать ИИ, которые понимают и рассуждают как люди.