RuGPT3 - коллекция генеративных моделей от Сбер.
Проводим автоматическое тестирование циклическим перебором вариантов.
Работаем в Colab, тестируем Small, Mediub, Large.
Параметры генерации совершенно неоптимизированы - это первый заход, чтобы посмотреть исходную ситуацию и сравнивать по мере улучшения.
Обработка реплик тоже не проводится. Ни абзацы, ни предложения - ничего, только удаляются знаки переноса строки, если их много подряд.
Алгоритм тестирования.
полный последовательный перебор 10 вопросов 3 модели: Small, Medium, Large Несколько наборов параметров внутри каждой модели Реплики сохраняем в гугл-таблице.
Параметры.
max_length Максимальная длина В тестовых примерах применяется набор max_length = [20, 50, 100, 200].
Greedy search Аргмаксная генерация. «Жадная генерация» Каждый раз выбирается токен, у которого максимальная вероятность.
Beam search num_beams - кол-во путей с наибольшими неочевидными итоговыми вероятностными сочетаниями В тестовых примерах num_beams = 5 num_return_sequences - кол-во лучших вариантов генерации на вывод В тестовых примерах num_return_sequences = 3.
Temperature sampling «Температура» Можно трактовать как случайный выбор с учетом распределения вероятностей. Чем ближе к нулю, тем больше похоже на Greedy Search или «жадную генерацию». В тестовых примерах применяется набор temperature = [0.8, 1.3, 2.0].
Nucleus sampling.
Для запрета сэмплирования совсем некорректных токенов вводятcя top-k или top-p ограничения.
В этом случае генерация тоже происходит случайным образом, но заранее отсекаются все маловероятные токены.
top_k = n - определяется n слов, которые обладают наибольшей вероятностью из условного распределения вероятностей всех слов, что сужает выбор для модели и отбрасывает максимально неподходящие слова сразу. Это позволяет контролировать разнообразие генерируемого текста и избежать слишком случайного выбора.
top_p = n - определяется n слов, чья вероятностная масса вместе равна n%, то есть ограничения сэмплирования происходят динамически, исходя из начального набора. Другими словами, определяет суммарную вероятность набора наиболее вероятных токенов.
Будут рассмотрены токены, пока их суммарная вероятность не достигнет n. Это также позволяет контролировать разнообразие генерируемого текста и избежать слишком случайного выбора.
В тестовых примерах top_k  =  20, top_p = 0.8.
Пишем код и запускаем генерацию.
Спрашиваем ChatGPT 10 самых распространенных вопросов на школьных экзаменах.
Устанавливаем данные для гугл-таблицы и пропишем фразы в соответствующих листах.
Указываем модель и столбец.
Запускаем генерацию.
Конечно, код возможно оптимизировать, обернуть повторы в циклы и функции и прочее. В данном случае не стали на этом фокусироваться, так как вычисления пока не затратные и задача решается с приемлемой скоростью.
Результат.
Таблица с репликами выложена здесь.
Понимаем, что есть много чего подбирать, оптимизировать, обрабатывать, добавлять, mask, padding и так далее. В данном случае это принципиально чистый исходный заход для фиксации начального состояния. Развитие ситуации предполагается в следующих статьях.
Примечания.
Если хотите протестировать вопросы, модели и параметры - укажите в комментариях. Сделаем тестирование и выложим ссылку на таблицу с результатами. Желательно формировать блоки по 10 вопросов, чтобы ничего не менять в коде.
Если видите какие-то критические ошибки - пожалуйста, укажите в комментариях.Если можете предложить способы улучшения генерации - пожалуйста, укажите в комментариях.
Спасибо за прочтение статьи:-).
Хорошего дня:-).