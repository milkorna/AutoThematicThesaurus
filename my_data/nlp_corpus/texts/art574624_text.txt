Статья, которая поможет вам разобраться в принципе работы и идее, стоящей за Word2Vec.
В предыдущей статье я рассказывал об основах NLP (Natural Language Processing — обработка естественного языка), и сегодня мы продолжим изучение этой темы.
Если вы еще не читали мою предыдущую статью, то советую вам сделать это: NLP - Text Encoding: A Beginner's Guide.
Перед тем, как мы начнем, обратите внимание на несколько моментов, касаемых статьи:
Я использую в статье как полное название Word2Vec, так и сокращенное w2v.
Здесь мы не будем вдаваться в подробности математических формул или объяснений того, как создавался w2v. Вы можете найти подробное руководство в научно-исследовательской статье по ссылке.
Я не буду слишком углубляться в аспекты глубокого обучения w2v, потому как хотел бы, чтобы эта статья была кратким обзором процесса, его интуитивным объяснением.
Хорошо, теперь, когда мы обозначили все моменты, давайте начнем.
Word2Vec — буквально переводится как “слово, представленное в виде числового вектора”. В этом и заключается вся суть. Главный вопрос КАК это происходит? Что ж, в этом и кроется подвох. Без паники, позвольте мне разложить все вопросы по полочкам — мы во всем с вами разберемся.
Вопрос 1: Что такое w2v?
Word2Vec — это малослойная (shallow) искусственная нейронная сеть (ANN), состоящая из двух слоев, которая обрабатывает текст, преобразуя его в числовые “векторизованные” слова. Входные данные w2v — это громадный текстовый корпус, из которого на выходе мы получаем пространство векторов (линейное пространство), размерность которого обычно достигает сотен, где каждое уникальное слово в корпусе представлено вектором из сгенерированного пространства. Она используется для преобразования лингвистического контекста в числа. Векторы слов расположены в пространстве векторов таким образом, что слова с общим контекстом, располагаются в этом многомерном пространстве в непосредственной близости друг от друга. Проще говоря, слова, близкие по значению, будут помещены рядом. Эта модель фиксирует синтаксическое и семантическое сходство между словами.
Вопрос 2: Каким образом модель фиксирует семантическое и синтаксическое сходство в векторах слов?
Здесь нужно объяснить многое, но я постараюсь не усложнять.
Сначала давайте разберемся, как мы можем понять семантическое сходство между двумя словами? Как объяснить то, что при преобразовании слов в числа расстояние расположения слов по отношению друг к другу зависит от сходства значения? Это несколько очевидных вопросов, ответить на которые мы сможем, когда поймем, что из себя представляют выходные данные модели, и как измерить близость. Затем мы узнаем, как создаются эти векторы.
Давайте разберем следующий пример:
возьмем 4 слова: King (король), Queen (королева), Man (мужчина), Woman (женщина).
Если бы я группировал похожие слова вместе, очевидно я бы сделал это следующим образом: (King, Man) и (Queen, Woman). А если, я бы группировал противоположные слова вместе, я бы сделал это так (King, Queen) и (Man, Woman).
Вы могли обратить внимание, что слово King схоже по значению с Man и противоположно Queen, а слово Queen схоже с Woman и противоположно King.
Теперь главный вопрос: как векторное представление сможет уловить эту суть?
Возможно ли это вообще? Да, это возможно.
Давайте разберемся. Для начала, чтобы разобраться в фиксировании синонимов и антонимов, мы должны понять, как сравниваются два вектора и сделать вывод об их математическом сходстве. Нам необходимо сравнить метрики двух векторов.
Чаще всего метрики сходства делятся на две разные группы:
Метрики сходства:
Корреляция Пирсона Корреляция Спирмена Коэффициент корреляции Кендэлла Коэффициент Охаи Коэффициента Жаккарда.
2. Метрики расстояния:
Евклидово расстояние Манхэттенское расстояние.
Сейчас мы не будем подробно останавливаться на том, как рассчитывать метрики. Но объяснить это можно следующим образом: чем ближе два вектора, тем выше у них коэффициент сходства и меньше расстояние между ними. Чем дальше два вектора, тем меньше их коэффициент сходства и больше расстояние между ними соответственно.
Вы можете представить это как:
1 - Расстояние = Сходство или.
1 - Сходство = Расстояние.
Сходство и Расстояние обратно пропорциональны друг другу.
Итак, теперь мы понимаем принцип сравнения двух векторов по их сходству. Вернемся к нашему примеру с набором слов King, Queen, Man и Woman, наше общее понимание будет следующим:
Вектор слова King должен иметь высокий коэффициент сходства и меньшее расстояние с вектором слова Man, а вектор слова Queen должен иметь высокий  коэффициент сходства и меньшее расстояние с вектором слова Woman и наоборот.
Для того, чтобы лучше понять это, взгляните на график:
Теперь мы знаем, как можно измерять синонимы и антонимы слов математическим способом.
Возникает главный вопрос: как получить наши целевые векторы?
Вопрос 3: Как w2v преобразует слова в числовые векторы, сохраняя семантическое значение?
Это можно сделать двумя способами, а именно:
CBOW: попытаться предсказать целевое слово, используя слова из контекста.
2. Skip-Gram: попытаться предсказать слова контекста, используя целевое слово.
Давайте об этом поподробнее:
Но прежде чем мы начнем, давайте обозначим ключевые слова и их значение:
Целевое слово (Target Word): слово, которое предсказывается.
Контекстное слово (Context Word): каждое слово в предложении, кроме целевого слова.
Размерность вложения (Embedding Dimension): количество измерений пространства векторов, в которые мы хотим преобразовать слова.
Разберемся на примере:
Предложение: “The quick brown fox jumps over the lazy dog”(Перевод: “Шустрая бурая лиса прыгает через ленивого пса.”).
Допустим, мы пытаемся предсказать слово “fox”, тогда “fox” будет нашим целевым словом.
Остальные слова будут контекстными.
CBOW, Continuous Bag of Words, — это процесс, в котором мы пытаемся предсказать целевое слово с помощью контекста. Мы обучим нашу модель запомнить слово “fox” в контексте, который мы используем в качестве входных данных. CBOW обычно хорошо работает на небольших наборах данных.
Skip-Gram — это процесс, в котором мы пытаемся предсказать контекстные слова с помощью целевого слова, что в точности противоположно CBOW. Skip-gram лучше работает на больших наборах данных.
Входное слово унитарно кодируется (One-Hot) и отправляется в модель одно за другим. Скрытый слой пытается предсказать наиболее вероятное слово, основываясь на весах, собранных в слое.
Мы избрали размерность нашего вектора равной 300, поэтому каждое наше слово будет преобразовано в 300-мерный вектор.
Последний слой нашей модели w2v — это классификатор Softmax, который определяет наиболее вероятное выходное слово.
Входными данными сети является one-hot вектор, представляющий входное слово, и метка, которая также является one-hot вектором, представляющим целевое слово, однако выходные данные сети представляют собой вероятностное распределение целевых слов, но не обязательно one-hot вектор, как метки.
Строки матрицы весов скрытого слоя на самом деле являются векторами слов (встраивание слов)!
Скрытый слой работает как справочная таблица. Выходные данные скрытого слоя — это «вектор слов» для входного слова.
Количество нейронов на скрытом слое должно равняться размерности вложений, которую мы задаем.
Допустим, в нашем корпусе 5 слов:
Предложение 1: Have a Good Day.
Предложение 2: Have a Great Day.
Сколько уникальных слов в этих примерах? “have”, “a”, “good”, “great”, “day” = 5.
Теперь предположим, что наше целевое слово для предложения 1 — “good”.
Когда я скармливаю остальную часть контекстных слов своей модели, она должна понять, что “good” - это именно то слово, которое она должна предсказать для предложения 1.
Предположим, что для предложения 2 наше целевое слово — “great”.
Когда я скармливаю своей модели остальную часть контекстных слов, она должна понять, что для предложения 2 корректное слово для предсказания — “great”.
Если вы внимательно посмотрите, то увидите, что когда мы уберем целевые слова, останутся одинаковые для двух предложений контекстные слова. Так каким образом модель точно предскажет, какое из них подойдет? На самом деле это не имеет значения, потому что она предсказывает наиболее вероятное слово, и в нашем случае нет неправильного варианта — подойдут оба “good” и “great”. Таким образом, возвращаясь к фиксации синонимов, тут оба слова “good” и “great” будут по сути означать одно и тоже и являться очень вероятным выходным словом для этих конкретных контекстных слов. Именно так наша модель изучает семантическое значение слов. Говоря простыми словами, если я могу использовать слова взаимозаменяемо, тогда изменяется только мое целевое слово, а не контекстные слова, т.е. любые два похожих слова, имеющих один синоним, будут весьма вероятным результатом для контекстных слов, скормленных модели — они будут иметь почти одинаковые числовые значения. Если у них почти одинаковые числовые значения, их векторы должны располагаться близко согласно их метрикам сходства. Таким образом мы фиксируем семантические значения.
Вопрос 4: Как работает модель?
Мы разберемся с архитектурой модели за четыре шага.
Подготовка данных.
Входные данные.
Скрытый слой.
Выходные данные.
Собственно из этих структур и состоит модель.
Подготовка данных:
Для начала давайте разберемся, как происходит подготовка и передача данных в нашу модель.
Рассмотрим предыдущий пример еще раз — ‘have a good day’. Модель преобразует это предложение в пары слов вида (контекстное слово, целевое слово). Пользователем устанавливается размер окна для контекстного слова и если оно равно двум, то пары слов будут выглядеть следующим образом: ([have, good], a), ([a, day], good). С помощью этих пар слов модель пытается предсказать целевое слово с учетом контекстных слов.
Входные данные:
Входными данными этих моделей является не что иное, как большой one-hot encoded вектор.
Давайте разберем на примерах:
Предположим, что в нашем текстовом корпусе 10000 уникальных слов, и мы хотим, чтобы наша размерность вложения соответствовала 300-мерному вектору.
Таким образом, входными данными будет вектор с размерностью 10000 с 0 для всех других слов, в то время как контекстное слово будет 1. Размерность нашего входного вектора составляет 1 x 10000.
Скрытый слой:
Скрытый слой — это та самая матрица весов со всеми словами, которые в начале случайным образом преобразованы в по 300 измерениям. Затем мы обучаем нашу модель подбирать значения этих весов, чтобы на каждом этапе обучения модель лучше справлялась с предсказанием.
Теперь нейроны в скрытом слое точно равны размерности вложений, которую мы выбираем, т.е. 300.
Таким образом, размерность нашей матрицы весов составляет 10000 x 300.
Выходной слой:
Выходной слой — это не что иное, как слой softmax, который использует значения вероятности для прогнозирования результатов среди 10000 векторов, которые у нас есть. Размер нашего выходного вектора равен 1 x 300.
Теперь давайте попробуем понять базовую математику, которая за этим стоит.
Входные данные: 1 x 10000.
Скрытый слой: 10000 x 300.
Выходные данные = Softmax (Входные данные x Скрытый слой) (матричное умножение) = 1 x n * n x dim = 1 x dim = 1 x 300.
Выше я объяснил архитектуру CBOW, а чтобы понять Skip-gram, нам просто нужно изменить порядок ввода и вместо контекстного слова и определения целевых слов, вводить целевые слова и пытаться определить контекстные слова. CBOW — это несколько способов ввода и только один вывод, Skip-gram — это один ввод и несколько выводов.
В Skip-gram мы можем определять количество контекстных слов, которое по нашему желанию бы предсказывалось, и, соответственно, мы можем создать пары входных-выходных значений.
Например: ‘have a good day’.
Если мы установим размер окна равным 2, то обучающие пары будут следующими:
Целевое слово: good.
Контекстные слова: have, a, day.
(good, have).
(good, a).
(good, day).
Таким образом, модель учится и пытается предсказать слова, близкие к целевому слову.
Вопрос 5: Какова цель преобразования слов в векторы?
Сейчас синтаксическое и семантическое значения хорошо отражены в векторах слов, поэтому они могут идеально использоваться в качестве входных данных для любых других сложных программ NLP, где может появиться необходимость в понимании сложных разговоров людей или контекста языков.
W2V сама по себе является нейронной сетью, но она представляет из себя скорее вспомогательную функцию, которая облегчает работу другим NLP приложениям.
Реализация:
Мы не будем создавать модель w2v с нуля, поскольку у нас уже есть библиотека Gensim, которая может нам помочь, если у нас есть собственный конкретный доменно-ориентированный корпус данных.
Или, если вы работаете с общими текстовыми данными на английском языке, я бы порекомендовал вам попробовать GloveVectors, которая была создана Стэнфордским Университетом с использованием миллиардов текстов из Wikipedia, Twitter и Common Crawl.
Заключение:
Мы разобрались в концепциях Word2Vec и теперь лучше понимаем, что такое вложения (embedding) слов. Теперь мы понимаем разницу между Skip-Gram и CBOW. Мы также интуитивно понимаем, как создаются вложения слов, и что скрытый слой представляет собой гигантскую таблицу поиска вложений слов. Также у нас появилось понимание того, как фиксируются семантические и синтаксические значения. Вложения слов могут быть очень полезными, а для многих задач NLP даже фундаментальными, не только для традиционного текста, но и для генов, языков программирования и других типов языков.
В следующий раз мы рассмотрим BeRT преобразование текста, а пока желаю вам успехов в обучении!
Материал подготовлен в рамках курса "Deep Learning. Basic".