Эмбеддинги текстов (они же семантические векторные представления) обычно получают из нейросетевых языковых моделей и затем используют в прикладных задачах: классификации, кластеризации, извлечении именованных сущностей, рекомендательных системах, и т. д. Эмбеддинг текста — это числовой вектор, содержащий в сжатом виде информацию о его смысле.
Для общеязыковых тематик существует множество мультиязычных бенчмарков (наборов тестовых задач) для оценки качества эмбеддингов, полученных с помощью разных моделей. С помощью этих бенчмарков можно сравнивать модели и выбирать подходящую для своей задачи. К сожалению, в области эмбеддингов научных текстов выбор не такой широкий, особенно для русского языка. Для английского языка существует бенчмарк SciDocs и его расширенная версия SciRepEval, разработанные Allen Institute for AI. Для русского языка первый бенчмарк ruSciDocs был опубликован нами около года назад вместе с моделью ruSciBERT, и состоял из небольшого количества данных на русском языке, которые мы смогли собрать в открытом доступе (на порталах ЕГИСУ НИОКТР и Semantic Scholar).
В этом году, благодаря данным, которые предоставил нам портал eLibrary, мы смогли сделать следующий шаг и подготовили бенчмарк ruSciBench, который содержит гораздо большее количество данных по большему числу тематик. Кроме того, в ruSciBench почти для всех статей есть аннотация как на английском, так и на русском языках, что дает возможность тестировать сохранение семантики при смене языка.
В этом посте я расскажу про бенчмарк для научных текстов ruSciBench и маленькую, но достаточно сильную модель SciRus‑tiny, которые мы разработали в лаборатории Машинного обучения и семантического анализа Института ИИ МГУ при поддержке портала eLibrary.
Состав датасета и задач ruSciBench.
ruSciBench состоит из двух типов задач: классификации и поиск перевода, постановка которых более подробно описана ниже. Датасет, на котором проводятся замеры метрик, состоит из 194 071 названия+аннотации научной статьи на русском языке и 182 436 — на английском. Для каждой статьи известна его рубрика OECD и ГРНТИ. В задаче классификации используются 29 рубрик OECD и 28 рубрик ГРНТИ. Рубрики OECD детализированы до 2 уровня (например, 5.03), рубрики ГРНТИ до 1 (например, 76.). Наибольшую долю в датасете занимают гуманитарные науки, наименьшую — сельскохозяйственные (Рисунок 1).
Длины текстов соответствуют принятой длине названия и аннотации научной статьи: 99% названий короче 26 слов, аннотаций — 384 слов, и 99% итоговых текстов (название+аннотация) — 400 слов. Также 90% итоговых текстов короче 250 слов.
Классификация.
Задача построена аналогично задачам MAG и MeSH из бенчмарка SciDocs от Allen Institute for AI. Только вместо рубрик Microsoft Academic Graph и Medical Subject Headings используются рубрики OECD и ГРНТИ. Для получения метрик используется следующий пайплайн. На 90% датасета обучается классификатор на базе метода опорных векторов (LinearSVC), использующий оцениваемые эмбеддинги в качестве векторов признаков. Затем на отложенных 10% оцениваются метрики точности этого классификатора. Задача построена на предположении, что чем более информативные эмбеддинги поданы на вход классификатору, тем выше результат он сможет показать.
Поскольку некоторые модели работают только с русским или только с английским языком, задача поделена еще и по этому принципу на три: только на русском ( ru ), только на английском ( en ) и объединяющая оба языка ( full ). Таким образом, возникает 6 задач, в зависимости от рубрикатора и языка: oecd‑ru, oecd‑en, oecd‑full и grnti‑ru, grnti‑en, grnti‑full. Для каждой задачи вычисляются метрики: weighted‑f1, оценивающая то, как модель работает в среднем, и macro‑f1, в большей степени отражающая способность модели работать с равномерным качеством как на малых, так и на больших рубриках.
Рассмотрим для примера сравнение двух моделей на этой задаче: Multilingual‑E5-base с довольно большим количеством параметров и маленькую модель SciRus‑tiny, у которой параметров почти в десять раз меньше. Также Multilingual‑E5-base имеет размерность эмбеддинга более, чем в два раза превосходящий SciRus‑tiny. Тем не менее, разрыв в метриках не так велик, в среднем менее 10%. Кроме того, можно заметить, что на weighted‑f1 разрыв уменьшается, судя по всему, большая модель лучше решает задачи, связанные с небольшими рубриками классификаторов.
Поиск перевода.
Задача поиска перевода оценивает способность модели отражать в эмбеддингах смысл сказанного, вне зависимости от языка текста. Для этого проверяется, что самым близким из всего датасета к эмбеддингу аннотации статьи на одном языке является эмбеддинг аннотации этой же статьи на другом языке. Например, если взять эмбеддинг русской аннотации статьи А и сравнить его с эмбеддингами всех английских аннотаций в датасете, то самым близким должен оказаться эмбеддинг английской аннотации этой же статьи А. Таким образом, возникает две метрики: поиск русской аннотации по английской и, наоборот, английской по русской. В качестве метрики возвращается доля статей, где поиск был выполнен успешно.
Посмотрим на метрики для тех же двух моделей: Multilingual‑E5-base и SciRus‑tiny. Большая модель все так же лидирует, но тоже примерно на 10%, как и в предыдущей задаче.
Как применять.
Чтобы оценить свою модель на ruSciBench нужно сделать несколько довольно простых шагов. Мы подробно показали этот процесс в инструкции/примере в нашем ноутбуке в Google‑Colab. Текстовые данные для бенчмарка размещены на huggingface, а код доступен на github. Хороших вам метрик!
SciRus-tiny.
Помимо бенчмарка мы также публикуем небольшую, эффективную модель SciRus‑tiny (за название спасибо @girlinds ), обученную на датасете 2.5B токенов научных текстов на русском и английском языках. Это модель архитектуры RoBERTa с 23M параметров и размерностью эмбеддинга 312. Размер словаря модели составляет 50265 токенов, а максимальная длина контекста — 2K токенов.
Мы обучали SciRus‑tiny в три этапа. На первом этапе модель обучалась «с нуля» решению задачи предсказания маскированных токенов на 12M аннотаций научных статей на русском и английском. На втором этапе модель SciRus‑tiny дообучалась с использованием контрастной функции потерь, приближая эмбеддинги аннотаций одной и той же статьи. Таким образом модель обучается обращать внимание на смысл текста вне зависимости от языка, на котором он написан. На третьем этапе модель дообучалась также с использованием контрастной функции потерь, но приближая вектора статей, имеющих общие или близкие научные категории согласно классификатору OECD. В результате модель нацеливается на учёт научной тематики текстов при оценке их смысловой близости.
Далее в таблицах представлены результаты SciRus-tiny в сравнении с различными моделей на всех метриках бенчмарка ruSciBench.  Интересно, что моноязычные и разработанные специально для работы с научными текстами ruSciBERT и SciNCL показывают примерно такое же качество, как и лучшая модель, имея более, чем в 2 раза меньшее количество параметров.
SciRus‑tiny достигла высоких значений метрик на ruSciBench, сохранив скорость и эффективность. Мы использовали в качестве основной модели для сравнения ruBERT‑tiny2 и превзошли ее по 10 из 14 метрик, получили равный результат еще по 3 и уступили только на 1. Также SciRus‑tiny показывает очень высокий уровень качества на задаче поиска перевода, близкий к лучшей модели Multilingual‑E5-base.
SciRus‑tiny – это первая небольшая модель в линейке наших моделей для получения семантических эмбеддингов научных текстов на разных языках. Но уже она показывает, что выбранные методы дают хороший результат. В будущем мы планируем выпустить ряд моделей с большим количеством параметров и добиться максимальных метрик на ruSciBench и не только.
Как применять.
Мы опубликовали SciRus-tiny на huggingface, также реализовав возможность удобного инференса с помощью библиотеки sentence-transformers. Удачного использования!
Acknowledgement.
Исследования проводятся в рамках гранта 23-Ш05–21 Междисциплинарной научно‑образовательной школы МГУ им.М.В.Ломоносова «Математические методы анализа сложных систем», проект «Разработка математических методов машинного обучения для обработки текстовой научной информации большого объема». Благодарим портал eLibrary за предоставленный датасет.