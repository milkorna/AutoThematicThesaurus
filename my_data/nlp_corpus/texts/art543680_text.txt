Дисклеймер: здесь я собираю новости абсолютно субъективно.
Часть новостей - новости только для меня и могли появиться довольно давно. Просто я заметил их только сейчас.
Я сопровождаю новость своим комментарием, в котором излагаю причину, почему эта новость интересна мне.
Похоже, что Transformers from Hugginface сейчас становится основным репозиторием готовых моделей в разделе NLP ( Natural Language Processing ). ……. Сад моделей от Google тоже хорош. Но второй репозиторий от Google, TensorFlow Hub, скорее не помогает, а мешает. Найти нужную модель у Google становится нетривиальным делом.  У других лидеров в NLP, Microsoft и Facebook, модели и тулзы размазаны по многим сайтам и проектам. Обнимающие Колобки бьют в одну точку. Все у них просто и еще раз просто. Новые модели появляются вовремя, обычно сразу как для PyTorch, так и для TensorFlow. Последнее время, прочитав про новую модель, я сразу иду к Колобкам и чаще всего новая модель у них уже готова. Испытать новую модель теперь можно просто и быстро.
sktime - свежий пакет по time series. В области time series и forecasting был небольшой застой, который на глазах рассасывается. Facebook Prophet и Amazon DeepAR долго стояли особняком без нормальной конкуренции. Теперь появились новые интересные проекты, такие как sktime, PyTorch Forecasting, Amazon GluonTS. Я выделил sktime за его прекрасную документацию и за то, что он нравится open source народу.
Google сделал интересную заявку со SMITH моделью. Предыдущий лидер в NLP - BERT. SMITH, по утверждению авторов, позволяет работать с текстом в 4 раза длиннее, чем BERT (2K tokens против 0.5K). Авторы добавили несколько трюков в дизайне и тренировке модели. Не похоже, что эти изменения носят какую-ту принципиальную разницу. Похожие трюки можно наблюдать в больших количествах у других исследователей. Google не ограничен в ресурсах и может натренировать многие похожие модели до уровня states-of-art.