Большие языковые модели это конечно хорошо, но иногда требуется использовать что-то маленькое и быстрое.
Постановка задачи.
Дистилляция будет проводиться для модели BERT, обученной на задачу бинарной классификации. В качестве данных был выбран открытый корпус русскоязычных твитов. Вдохновлялся двумя статьями: по дистилляции данных из BERT в BiLSTM, и собственно по дистилляции BERT. Нового ничего не добавлю, хочется все причесать и сделать пошаговый туториал для простого использования. Весь код на github.
План работ.
Baseline 1: TF-IDF + RandomForest.
Baseline 2: BiLSTM.
Дистилляция BERT > BiLSTM.
Дистилляция BERT > tinyBERT.
TF-IDF + RandomForest.
Все стандартно: нижний регистр, лемматизация, удаление стоп-слов.  Полученные вектора классифицируем RandomForest. Получаем F1 чуть больше 0.75.
BiLSTM.
Попробуем улучшить бэйзлайн с помощью нейросетевого подхода. Все стандартно: учим токенизатор, учим сетку. В качестве базовой архитектуры берем BiLSTM. Получаем F1 чуть больше 0.79. Небольшой, но прирост есть.
Учим BERT.
Обучим модель-учитель. В качестве учителя я выбрал героя вышеупомянутой статьи по дистилляции - rubert-tiny от @cointegrated. Получаем F1 чуть больше 0.91. Я особо не игрался с обучением, можно думаю было получить метрику и получше, особенно если использовать большой BERT, но и так достаточно показательно. Как обучить BERT на бинарную классификацию можно глянуть в моей прошлой статье, или прямо тут:
Дистилляция BERT > BiLSTM.
Основная идея - приближение BiLSTM-учеником выхода BERT-учителя. Для этого при обучении используем функцию ошибки MSE. Можно использовать совместно с обучением на метках и CrossEntropyLoss. Подробнее можно почитать в статье по ссылке. На моих тестовых данных дистилляция докинула всего пару процентов: F1 чуть больше 0.82.
Дистилляция BERT > tinyBERT.
Основная идея, как и в прошлом пункте - приближать учеником поведение учителя. Есть много вариантов что и как приближать, я взял всего два:
Приближать [CLS]-токен по MSE.
Дистилляция распределения токенов по дивергенции Кульбака-Лейблера.
Дополнительно в процессе обучения решаем задачу MLM - предсказание замаскированных токенов. Уменьшение размера модели осуществляется за счет сокращения словаря и уменьшения количества голов внимания, а также  количества и размерности скрытых слоев.
Обучение итогового классификатора в итоге делится на 2 этапа:
Обучение языковой модели.
Обучение головы для классификации.
Я применял дистилляцию только для первого этапа, голову для классификации учил уже непосредственно на дистиллированной модели. Думаю можно было накинуть и вариант с MSE как в примере с BiLSTM, но оставил эти эксперименты на потом.
Ключевые моменты реализации:
Размер итоговой модели составил 16 Мб, метрика F1 0.86. Учил модель я 12 часов на макбук эйр 19 года с i5 и 8 Гб оперативной памяти. Думаю, если погонять подольше, то и результат будет получше.
Код и данные для обучения представлены на github, замечания, дополнения и исправления приветствуются.