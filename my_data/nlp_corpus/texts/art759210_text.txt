Всем привет, меня зовут Алан, я разработчик-исследователь в MTS AI, мы сейчас активно изучаем LLM, тестируя их возможности. В настоящее время в России вышло несколько коммерческих языковых моделей, в том числе GigaChat и YandexGPT, которые хорошо выполняют текстовые задачи. В этой статье показывается, что языковая модель меньшего размера, обученная на открытых данных за несколько часов, показывает сравнительно неплохую, а в некоторых случаях и лучшую производительность относительно больших коммерческих решений. На небольшом количестве примеров мы проверим способность моделей решать простые математические задачи, отвечать на вопрос по заданному контексту, в котором содержатся числа и выполнять простые текстовые инструкции. Затем кратко рассмотрим, как и на чем обучалась наша модель.
Ответ на вопрос по контексту.
В качестве открытой модели выступает первая версия LLaMa-7B, тонко настроенная на наборе данных IlyaGusev/ru_turbo_saiga.
Приведем два примера:
Prompt: "В магазине приложений есть следующие приложения: app1 стоит 1500 рублей, приложение app2 стоит 2000 рублей, оно самое популярное, приложение app3 стоит 3000 рублей. Сколько стоит app3?" Довольно простой промт, с которым управится ruT5-base, зафайнтюненная на squad. У GigaChat возникают проблемы с ответом:
У YandexGPT тоже проблемы с этим вопросом:
Как отвечает LLaMa (после "bot"):
Prompt: "У нас есть много курсов. 11 курсов по программирование, 10 курсов по русскому языку. Сколько курсов в сумме?"С этим вопросом ruT5 уже не справиться, так как она не умеет складывать, но GigaChat это должно быть по силам (или нет):
LLaMa ответила правильно:
При изменении чисел в промте LLaMa все равно отвечает правильно.
Если говорить про контекст, то коммерческие модели справляются хорошо, но они спотыкаются на подобных вопросах.
GigaChat:
YandexGPT:
LLaMa:
Небольшая оценка производительности моделей с помощью ChatGPT.
Важно: Для более объективной оценки нам понадобятся больше задач по математике. При этом HumanEval, на котором OpenAI валидирует свои модели, содержит всего 135 задач, а MMLU 113.
Сначала мы протестировали возможности LLM на 25 простых математических задачах, сгенерированных с помощью ChatGPT. Здесь LLaMa выигрывает у GigaChat, и при этом всего на две единицы отстает от большой YandexGPT, что довольно неплохо.
25 задач на "больше- меньше". Здесь GigaChat показал себя лучше чем LLaMa.
Для чистоты этого небольшого эксперимента под каждый вопрос контекст очищался, так как математические способности моделей проседали при большом контексте (кроме YandexGPT).
Далее мы попросили ChatGPT оценить несколько ответов моделей. При генерации больших текстов gigachat выигрывает,  LLaMa может повторяться и не умеет вовремя останавливаться, на некоторых коротких текстах модели показывают примерно одинаковую производительность. Примеры можно посмотреть ниже.
Finetuning LlaMa- 7B.
Поговорим о том, каким образом мы тонко настраивали модель. LLaMa-7B изначально обладает хорошими знания о мире благодаря обучению на огромном количестве текстов, но для того чтобы модель могла выполнять текстовые инструкции, мы должны тонко настроить ее на небольшом количестве качественных пар инструкция-ответ. Мы решили пойти немного другим способом и зафайнтюнить модель на диалогах между человеком и виртуальным ассистентом.
Набор данных.
Как ранее уже упоминалось, LLaMa- 7B обучена на наборе IlyaGusev/ru_turbo_saiga.
Набор данных представляет собой 37 тысяч сгенерированных с помощью СhatGPT небольших диалогов между пользователем и виртуальным ассистентом. Изначально планировалось обучить FRED- T5, поэтому набор данных был подготовлен для обучения text2text модели. В начало каждого сообщения добавлена роль user/bot. На вход T5 подается список сообщений, таргетом является последнее предложение в диалоге- ответ бота. FRED- T5 не подходит для подобной задачи, так как при инференсе на вход модель будет ожидать сразу список сообщений.
Датасет с ролями user/bot в начале сообщений доступен на Hugging Face.
Более подходящей для данной задачи архитектурой является decoder- only transformer. Для файнтюнинга была выбрана модель LLaMa-7B, после чего датасет был подготовлен для обучения языковой модели:
Здесь мы отфильтровываем фрагменты размером меньше чем context_length.
Тонкая настройка.
При обучении использовались метод LoRA и библиотека PEFT (Parameter-Efficient Fine-Tuning). Для этого мы заморозили слои языковой модели и "прикрепили" адаптеры меньшей размерности. Это позволяет обучать меньшее количество параметров, что актуально для LMM. В нашем случае модель вместо 7 миллиардов обучалась примерно на 300 миллионах параметров.
Код с LoRA выглядит следующим образом:
r- это ранг матриц адаптеров;
lora_alpha- это коэффициент масштабирования.
Код обучения:
Инференс:
Модель обучалась на 2xA100 с использованием data parallelism и accelerate в течение нескольких часов.
Выводы.
В целом LLaMa показывает сравнительно неплохие результаты, учитывая следующее:
LLaMa меньше, чем языковая модель GigaChat: 7 миллиардов параметров против 13.
LLaMa обучалась на открытом датасете с Hugging Face в течении нескольких часов.
При этом LLaMa показывает себя лучше, чем GigaChat в задачах, где нужно что- то посчитать или извлечь число из контекста (речь идет о конкретном домене задач). Также LlaMa сравнительно неплохо генерирует текст и поддерживает диалог с пользователем.