Привет, Хабр.
Довольно интересным направлением "прикладной статистики" и NLP (Natural Languages Processing а вовсе не то что многие сейчас подумали) является анализ текста. Появилось это направление задолго до компьютеров, и имело вполне практическую цель: определить автора того или иного текста. С помощью ПК это впрочем, гораздо легче и удобнее, да и результаты получаются весьма интересные. Посмотрим, какие закономерности можно выявить с помощью совсем простого кода на Python.
Для тех кому интересно, продолжение под катом.
История.
Одной из первых практических задач было определение авторства политических текстов The Federalist Papers, написанных в США в 1780 годах. Их авторами было несколько человек, но кто есть кто, окончательно было неизвестно. Первый подход к построению кривой распределения длины слов был предпринят еще в 1851 г, и можно представить, какой это был объем работы. Сейчас, слава богу, всё проще. Я рассмотрю простейший способ анализа с помощью несложных расчетов и пакета Natural Language Toolkit, что в совокупности с matplotlib позволяет получить интересные результаты буквально в несколько строк кода. Мы посмотрим, как все это можно визуализировать и какие закономерности можно увидеть.
Те, кому интересны результаты, главу "код" могут пропустить.
Код.
Перейдем к практическому примеру. Возьмем для анализа следующий текст:
Подключим библиотеку nltk:
Массив tokens содержит все слова и знаки пунктуации строки:
Отфильтруем массив, удалив из него знаки препинания и переведем слова в нижний регистр:
Теперь мы можем получить первый статистический параметр: лексическое разнообразие текста. Это соотношение числа уникальных слов к их общему количеству.
Для данного текста этот параметр равен 96.6%.
Несложно получить среднюю длину слова:
Множество set(tokens_) дает нам неповторяющийся список слов, далее мы просто вычисляем среднее, разделив сумму на количество. Для этого текста средняя длина слова равна 4.86.
Средняя длина предложени я вычисляется с помощью метода sent_tokenize в NLTK, который, как очевидно из названия, разбивает текст на предложения.
Для нашего текста длина предложения составляет 15 слов.
И последний параметр - частотность появления различных симолов. У каждого автора может быть свой стиль использования запятых, вопросов и кавычек, разных несклоняемых частей речи ("что", "в"). Для примера посчитаем частоту использования запятых на 1000 символов текста:
Для данного текста параметр составляет 57.14 запятых на 1000 символов.
Последнее, что нам нужно сделать - загружать текст из файла.
Как можно видеть, здесь есть два варианта. Часть файлов, скачанных из онлайн-библиотек, хранятся в кодировке 1251. Другая часть файлов сохранена методом copy-paste в Блокноте, и имеет более современную кодировку UTF-8. Вышеприведенный метод сначала пытается открыть файл как 1251, в случае неудачи мы считаем что это UTF-8, на практике такого подхода оказалось вполне достаточно.
Визуализация.
Пока все выглядит довольно скучно. Гораздо интереснее становится тогда, когда эти данные можно увидеть графически. Я взял наугад по одной книге от 4х известных авторов, тексты были взяты со всем известной Библиотеки Максима Мошкова Lib.ru.  Каждая книга разбивается на блоки одинаковой длины, для каждого блока параметры вычисляются вышеописанным способом.
Лексическое разнообразие и средняя длина слова не дают какой-либо заметной разницы:
Очевидно, все авторы люди образованные, тексты написаны хорошим и литературным русским языком, какой-либо значимой разницы в объеме используемых слов не видно. Параметр "длина слова" тоже не дает видимых отличий. А вот средняя длина предложения отличается весьма заметно:
У Набокова стиль, очевидно, отличается, и разница статистически хорошо видна. Это неудивительно - если открыть саму книгу, в тексте встречаются предложения типа таких:
Он боялся, Лужин старший, что, когда сын узнает, зачем так нужны были совершенно безликие Трувор и Синеус, и таблица слов, требующих ять, и главнейшие русские реки, с ним случится то же, что два   года  назад,  когда,  медленно  и  тяжко,  при  звуке скрипевших   ступеней,   стрелявших   половиц,    передвигаемых сундуков,  наполнив  собою  весь дом, появилась француженка.
Количество запятых на 1000 слов также отличается, и это очевидно - в длинном предложении их, очевидно, и должно быть больше:
Разумеется, анализ можно делать по любому символу, например, можно сравнить, как часто у разных авторов встречается знак двоеточия ":":
Несложно вывести частоты появления различных символов в виде кривой, взяв по одной книге от каждого автора:
Частоты употребления разных символов в русском языке похожи, но различия в стиле разных авторов все же есть. Для сравнения, вот так выглядит кривая для разных книг одного автора:
Идея, надеюсь, понятна. "Отпечаток" использования различных символов отличается для разных авторов, и как было показано, технологию можно использовать даже для выявления клонов на популярном англоязычном сайте reddit.com. Впрочем, насколько достоверно это работает для русского языка, автору неизвестно.
Мы же рассмотрим пример попроще. Популярная в СССР детская книга "Улица младшего сына" имеет двух авторов, Лев Кассиль и Макс Поляновский. На графике хорошо видно статистическое различие по Lexical Diversity. Можно предположить что начало книги писал один автор, а закончил другой:
И последний, наиболее любопытный пример. Ниже приведен график лексического разнообразия для 10 книг одной популярной дамской писательницы, чьи книги одно время продавались буквально на каждой остановке. Ещё тогда я удивлялся, можно ли писать столько книг в таком количестве. Результат интересен, на графике определенно видна статистическая аномалия - стиль как минимум, одной книги заметно отличается от остальных:
Но разумеется, может это и просто совпадение, теория вероятности такое, в принципе, допускает. Более того, только один параметр не является доказательством, для примера можно посмотреть, какое количество характеристик текста может использоваться для анализа авторства.
Заключение.
Вышеприведенный анализ показался довольно интересным. Используя несложные, практически школьные, формулы, можно получить довольно любопытные результаты. Разумеется, анализ можно и усложнить, например, можно попробовать определить, менялся ли стиль автора с годами, вариантов тут много.
Для желающих поэкспериментировать самостоятельно, исходный код для Python 3.7 приведен под спойлером.