6 - Attention is All You Need.
В этом разделе мы будем реализовывать слегкаизмененнуюверсию модели Transformer из статьи Attention is All You Need. Все изображения в этой части взяты из этой статьи. Для получения дополнительной информации о Transformer обращайтесь сюда, сюда и сюда. На русском языке здесь.
Введение.
Подобно свёрточной модели Sequence-to-Sequence, Transformer не использует никакой рекуррентности. Он также не использует свёрточные слои. Вместо этого модель полностью состоит из линейных слоев, механизмов внимания и нормализации.
По состоянию на январь 2020 года трансформеры являются доминирующей архитектурой в NLP и используются для достижения передовых результатов во многих задач, и похоже, что они будут доминировать в ближайшем будущем в области обработки языков.
Самый популярный Transformer вариант это BERT ( B idirectional E ncoder R epresentations from T ransformers) и предварительно обученные версии BERT обычно используются для замены слоёв эмбеддинга — если не больше - в NLP моделях.
Распространенной библиотекой, используемой при работе с предварительно обученными трансформаторами, является библиотека Transformers, смотрите здесь список всех доступных предварительно обученных моделей.
Различия между реализацией в этой части и в статье:
мы используем обученную позиционную кодировку вместо статической.
мы используем стандартный оптимизатор Adam со статической скоростью обучения вместо оптимизатора с динамически изменяющейся скоростью.
мы не используем сглаживание меток.
Мы вносим все эти изменения, поскольку они близки к настройкам BERT и большинство вариантов Transformer используют аналогичную настройку.
Как и ранее, если визуальный формат поста вас не удовлетворяет, то ниже ссылки на английскую и русскую версию jupyter notebook:
Исходная версия Open jupyter notebook In Colab.
Русская версия Open jupyter notebook In Colab.
Замечание: русская версия jupyter notebook отличается от исходной добавленным в конце тестом на инверсию предложения.
Подготовка данных.
Как всегда, давайте импортируем все необходимые модули и зададим случайные начальные числа для воспроизводимости.
Для загрузки в Google Colab используем следующие команды (После загрузки обязательно перезапустите colab runtime! Наибыстрейший способ через короткую комаду： Ctrl + M +. ):
Затем мы создадим наши токенизаторы, как и раньше.
Наши поля такие же, как и в предыдущих частях. Модель ожидает, что данные будут введены в первую очередь с размерностью батча, поэтому мы используем batch_first = True.
Затем мы загружаем набор данных Multi30k и создаем словарь.
Наконец, мы определяем устройство для обучения и итератор данных.
Построение модели.
Далее мы построим модель. Как и в предыдущих частях, она состоит из кодера и декодера, с кодером кодирующим входное предложение нанемецкомязыке в вектор контекста и декодера, который декодирует этот вектор контекста в выходное предложение наанглийскомязыке.
Кодировщик.
Подобно модели ConvSeq2Seq, кодировщик Transformer не пытается сжать всё исходное предложение в единый вектор контекста. Вместо этого он создает последовательность векторов контекста. Итак, если бы наша входная последовательность состояла из 5 токенов, у нас было бы. Почему мы называем этот тензор последовательностью контекстных векторов, а не последовательностью скрытых состояний? Скрытое состояние в момент времени в RNN видит только токены и все токены, что были перед ним. Однако здесь каждый вектор контекста видел все токены во всех позициях входной последовательности.
Сначала токены проходят через стандартный слой эмбеддинга. Поскольку модель не является рекуррентной, она не имеет представления о порядке токенов в последовательности. Мы решаем эту проблему, используя второй слой эмбеддинга, называемый позиционный слой эмбеддинга positionalembeddinglayer. Это стандартный эмбеддинг, для которого входом является не сам токен, а позиция токена в последовательности, начиная с первого токена, токена <sos> началопоследовательности в позиции 0. Позиционный эмбеддинг имеет размер словаря, равный 100, что означает, что наша модель может принимать предложения длиной до 100 токенов. Его можно увеличить, если мы хотим обрабатывать более длинные предложения.
Оригинальная реализация Transformer в статье Attention is All You Need не обучала позиционный эмбеддинг. Вместо этого в ней использовался фиксированный статический эмбеддинг. Современные архитектуры Transformer, такие как BERT, вместо этого используют позиционные эмбеддинги, поэтому мы решили использовать их в этой реализации. Обратитесь сюда, чтобы узнать больше о позиционных эмбеддингах, используемых в исходной модели Transformer.
Затем токен и результат прохождения позиционного эмбеддинга поэлементно суммируются для получения вектора, который содержит информацию о токене, а также его позицию в последовательности. Однако перед суммированием токенов эмбеддинга они умножаются на коэффициент масштабирования, равный, где размер скрытого измерения hid_dim. Это якобы уменьшает дисперсию эмбеддинга, и модель трудно обучить без этого коэффициента масштабирования. Затем применяется дропаут для комбинированного эмебеддинга.
Комбинированный эмебеддинг затем пропускаются через слоев кодировщика для получения, для вывода и использования декодером.
Исходная маска src_mask просто имеет ту же форму, что и исходное предложение, но имеет значение 1, когда токен в исходном предложении не является токеном <pad> и 0, когда это токен <pad>. Это используется в слоях кодировщика для маскировки механизмов многонаправленного внимания, которые используются для вычисления и применения внимания к исходному предложению, поэтому модель не обращает внимания на токены <pad>, которые не содержат полезной информации.
Слой кодировщика.
Слои кодировщика — это место, содержащее всю «соль» кодировщика. Сначала мы передаем исходное предложение и его маску в слой многонаправленного внимания, затем выполняем дропаут для его выхода, применяем остаточное соединение и передайте его через слой нормализации. Затем результат мы пропускаем через слой сети позиционно-зависимого прямого распространения и снова применяем дропаут, остаточное соединение, слой нормализации, чтобы получить вывод этого слоя, который передается на следующий слой. Параметры не разделяются неявляютсяобщими между слоями.
Слой многонаправленного внимания используется уровнем кодировщика для сосредоточения внимания на исходном предложению, то есть он вычисляет и применяет механизм внимание по отношению к себе, а не к другой последовательности, поэтому эту процедуру называются внутренним вниманием.
Эта статья подробно рассказывает о нормализации слоев. Суть в том, что в процедуре нормализации нормализуются значения признаков тоестьпоскрытымизмерениям, поэтому каждый признак имеет среднее значение 0 и стандартное отклонение 1. Это упрощает обучение нейронных сетей с большим количеством слоев, таких как Transformer.
Слой многонаправленного внимания.
Одна из ключевых, новых концепций, представленных в статье о Transformer, - это слой многонаправленного внимания.
Внимание можно рассматривать как запросы, ключи и значения - где запрос используется с ключом для получения вектора внимания (обычно для вывода используется операция softmax и выходные величины имеют значения от 0 до 1, которые в сумме равны 1), используемый для получения взвешенной суммы значений.
Трансформер использует масштабированное скалярное произведение внимания, для которого запрос и ключ объединяются путем взятия скалярного произведения между ними, затем применяя операцию softmax и масштабируя прежде чем, наконец, умножить на значение. это размер направления, head_dim, которое мы раскроем далее.
Это похоже на стандартное скалярное произведение для внимания, но масштабируется по, которое, как говорится в статье, используется, чтобы остановить результаты скалярных произведений, становящиеся большими, что приводит к тому, что градиенты становятся слишком маленькими.
Однако масштабированное скалярное произведение внимания применяется не просто к запросам, ключам и значениям. Вместо того, чтобы применять единственное внимание к запросам, ключам и значениям, их hid_dim разделить на направлений и масштабированное скалярное произведение внимания рассчитывается по всем направлениям параллельно. Это означает, что вместо того, чтобы уделять внимание одному понятию за одно применения внимания, мы обращаем внимание на понятий. Затем мы повторно объединяем направления в их hid_dim форму, таким образом, каждый hid_dim потенциально обращает внимание на разных понятий.
это линейный слой, применяемый в конце слоя внимания с несколькими направлениями, fc. линейные слои fc_q, fc_k и fc_v.
Проходя по модулю, сначала вычисляем, и с линейными слоями fc_q, fc_k и fc_v, дающие нам Q, K и V. Далее мы разбиваем hid_dim запроса, ключа и значения на n_heads, используя.view и правильно поменяв их порядок так, чтобы их можно было перемножить. Затем мы вычисляем energy ненормализованноевнимание путем умножения Q и K вместе и масштабируя её на квадратный корень из head_dim, которое рассчитывается как hid_dim // n_heads. Затем мы маскируем энергию, чтобы не обращать внимания на элементы последовательности, на которые не следует сосредотачиваться, затем применяем softmax и дропаут. Затем мы применяем внимание к значениям направлений V, перед объединением n_heads вместе. Наконец, мы умножаем результат на, представленное fc_o.
Обратите внимание, что в нашей реализации длины ключей и значений всегда одинаковы, поэтому при матричном умножении выход softmax, attention, с V у нас всегда будут правильного размера для умножения матриц. Это умножение выполняется с использованием torch.matmul который, когда оба тензора > 2-мерны, выполняет пакетное матричное умножение по последним двум измерениям каждого тензора. Это будет querylen,keylen x valuelen,headdim умножение матрицы на размер пакета и каждое направление, которая обеспечивает результат вида batchsize,nheads,querylen,headdim.
Слой позиционно-зависимого прямого распространения.
Другой основной блок внутри уровня кодировщика — это слой позиционно-зависимого прямого распространения. Он устроен относительно просто по сравнению со слоем многонаправленного внимания. Вход преобразуется из hid_dim в pf_dim, где pf_dim обычно намного больше, чем hid_dim. Оригинальный Трансформер использовал hid_dim из 512 и pf_dim из 2048. Функция активации ReLU и дропаут применяются до того, как он снова преобразуется в представление hid_dim.
Почему он используется? К сожалению, в статье это нигде не объясняется.
BERT использует функцию активации GELU, которую можно применить, просто переключив torch.relu на F.gelu. Почему они использовали GELU? Опять же, это никогда не объяснялось.
Декодер.
Задача декодера — получить закодированное представление исходного предложения и преобразовать его в предсказанные токены в целевом предложении. Затем мы сравниваем с фактическими токенами в целевом предложении для расчета потерь, которые будут использоваться для расчета градиентов параметров, а затем использованы в оптимизаторе для обновления весов с целью улучшить прогнозы.
Декодер похож на кодировщик, но в нём имеется два уровня внимания с несколькими направлениями. Слой многонаправленного внимания с маскировкой над целевой последовательностью и слоем многонаправленного внимания, который использует представление декодера в качестве запроса и представление кодера как ключ и значение.
Декодер использует позиционный эмбеддинг и комбинирование - через поэлементную сумму - с масштабированными целевыми токенами, прошедшими эмбеддинг, после чего следует дропаут. Опять же, наши позиционные кодировки имеют «словарь» равный 100, что означает, что они могут принимать последовательности длиной до 100 токенов. При желании его можно увеличить.
Комбинированные результаты эмбеддинга затем проходят через слоёв декодера, вместе с закодированным источником enc_src, а также исходной и целевой маской. Обратите внимание, что количество слоев в кодировщике не обязательно должно быть равно количеству слоев в декодере, даже если они оба обозначены как.
Представление декодера после N -го слоя затем пропускается через линейный слой fc_out. В PyTorch операция softmax содержится в нашей функции потерь, поэтому нам не нужно явно использовать здесь слой softmax.
Помимо использования исходной маски, как мы это делали в кодировщике, чтобы наша модель не влияла на токен <pad>, мы также используем целевую маску. Это будет объяснено далее в Seq2Seq модели, которая инкапсулирует как кодер, так и декодер, но суть ее в том, что она выполняет ту же операцию, что и заполнение декодера в свёрточной модели sequence-to-sequence model. Поскольку мы обрабатываем все целевые токены одновременно и параллельно, нам нужен метод остановки декодера от «обмана», просто «глядя» на следующий токен в целевой последовательности и выводя его.
Наш слой декодера также выводит нормализованные значения внимания, чтобы позже мы могли построить их график, чтобы увидеть, на что на самом деле обращает внимание наша модель.
Слой декодера.
Как упоминалось ранее, уровень декодера аналогичен уровню кодера, за исключением того, что теперь он имеет два уровня многонаправленного внимания self_attention и encoder_attention.
Первый, как и в кодировщике, осуществляет внутреннее внимание, используя представление декодера, вплоть до запроса, ключа и значения. Затем следует дропаут, остаточное соединение и слой нормализации. Этот слой self_attention использует маску целевой последовательности trg_mask, чтобы предотвратить «обман» декодера, обращая внимание на токены, которые «опережают» тот, который он обрабатывает в настоящее время, поскольку он обрабатывает все токены в целевом предложении параллельно.
Второй определяет как мы на самом деле подаём закодированное исходное предложение enc_src в наш декодер. В этом слое многонаправленного внимания запросы являются представлениями декодера, а ключи и значения — представлениями кодировщика. Здесь исходная маска src_mask используется для предотвращения того, чтобы слой многонаправленного внимания обращал внимание на токен <pad> в исходном предложении. Затем следуют уровни дропаута, остаточного соединения и уровень нормализации.
Наконец, мы передаем результат через слой позиционно-зависимого прямого распространения и еще одна последовательность дропаута, остаточного соединения и уровень нормализации.
Слой декодера не вводит никаких новых концепций, просто использует тот же набор слоев, что и кодировщик, но немного по-другому.
Seq2Seq.
Наконец, у нас есть модуль Seq2Seq, который инкапсулирует кодировщик и декодер, а также управляет созданием масок.
Исходная маска создается путем проверки того, что исходная последовательность не равна токену <pad>. Это 1, когда токен не является токеном <pad>, и 0, когда он является этим токеном. Затем он "разжимается", чтобы его можно было правильно транслировать при наложении маски на energy, которая имеет форму batchsize,nheads,seqlen,seqlen.
Целевая маска немного сложнее. Сначала мы создаем маску для токенов <pad>, как мы это делали для исходной маски. Далее, мы создаем «последующую» маску, trg_sub_mask, используя torch.tril. Таким образом создаётся диагональная матрица, в которой элементы над диагональю будут равны нулю, а элементы под диагональю будут установлены на любой входной тензор. В этом случае входным тензором будет тензор, заполненный единицами. Это означает, что наша trg_sub_mask будет выглядеть примерно так для цели с 5 токенами:
Это показывает, на что может смотреть каждый целевой токен строка столбец. Первый целевой токен имеет маску 1, 0, 0, 0, 0, что означает, что он может смотреть только на первый целевой токен. Второй целевой токен имеет маску 1, 1, 0, 0, 0, что означает, что он может просматривать как первый, так и второй целевые токены.
Затем «последующая» маска логически дополняется маской заполнения, которая объединяет две маски, гарантируя, что ни последующие токены, ни маркеры заполнения не могут быть обработаны. Например, если бы последние два токена были токенами <pad>, маска выглядела бы так:
После того, как маски созданы, они используются с кодировщиком и декодером вместе с исходным и целевым предложениями, чтобы получить наше предсказанное целевое предложение, «output», вместе с вниманием декодера к исходной последовательности.
Обучение модели Seq2Seq.
Теперь мы можем определить наш кодировщик и декодеры. Эта модель значительно меньше, чем Трансформеры, которые используются сегодня в исследованиях, но её можно быстро запустить на одном графическом процессоре.
Затем определяем и инкапсулирем модель sequence-to-sequence.
Мы можем проверить количество параметров, заметив, что оно значительно меньше, чем 37 M для модели свёрточной последовательности.
В статье не упоминается, какая схема инициализации веса использовалась, однако форма Xavier, кажется, распространена среди моделей Transformer, поэтому мы используем ее здесь.
Оптимизатор, использованный в исходной статье Transformer, использует Adam со скоростью обучения, которая включает периоды «ускорения» и «торможения». BERT и другие модели Transformer используют Adam с фиксированной скоростью обучения, поэтому мы реализуем это. Проверьте эту ссылку для получения более подробной информации о графике скорости обучения оригинального Transformer.
Обратите внимание, что скорость обучения должна быть ниже, чем по умолчанию, используемой Адамом, иначе обучение будет нестабильным.
Затем мы определяем нашу функцию потерь, игнорируя потери, рассчитанные по токенам <pad>.
Затем мы определим наш цикл обучения. Это то же самое, что использовалось в предыдущей части.
Поскольку мы хотим, чтобы наша модель предсказывала токен <eos>, но не выводила его в качестве выходных данных модели, мы просто отрезаем токен <eos> в конце последовательности. Таким образом:
обозначает фактический элемент целевой последовательности. Затем мы вводим это в модель, чтобы получить предсказанную последовательность, которая, мы надеемся, должна предсказывать токен <eos>:
обозначает предсказанный элемент целевой последовательности. Затем мы вычисляем потери, используя исходный целевой тензор trg с токеном <sos>, отрезанным спереди, оставляя токен <eos>:
Рассчитываем потери и обновляем параметры, как это обычно делается.
Цикл оценки такой же, как цикл обучения, только без вычислений градиента и обновления параметров.
Затем мы определяем небольшую функцию, которую можем использовать, чтобы сообщить нам, сколько времени занимает эпоха.
Наконец, мы обучаем нашу фактическую модель. Эта модель почти в 3 раза быстрее, чем модель сверточной последовательности, а также обеспечивает меньшую сложность проверки!
Мы загружаем наши «лучшие» параметры и добиваемся большей точности при тестировании, чем достигали все предыдущие модели.
Вывод.
Теперь мы можем переводить с помощьюв нашей модели используя функцию translate_sentence.
Были предприняты следующие шаги:
токенизируем исходное предложение, если оно не было токенизировано является строкой.
добавляем токены <sos> и <eos>.
оцифровываем исходное предложение.
преобразовываем его в тензор и добавляем размер батча.
создаём маску исходного предложения.
загружаем исходное предложение и маску в кодировщик.
создаём список для хранения выходного предложения, инициализированного токеном <sos>.
пока мы не достигли максимальной длиныпреобразовываем текущий прогноз выходного предложения в тензор с размерностью батчасоздаём маску целевого предложенияпоместим текущий выход, выход кодировщика и обе маски в декодерполучаем предсказание следующего выходного токена от декодера вместе с вниманиемдобавляем предсказание к предсказанию текущего выходного предложенияпрерываем, если предсказание было токеном <eos>.
преобразовываем выходное предложение из индексов в токены.
возвращаем выходное предложение (с удаленным токеном <sos>) и вниманием с последнего слоя.
Теперь мы определим функцию, которая отображает внимание к исходному предложению на каждом этапе декодирования. Поскольку у этой модели 8 направлений, мы можем наблюдать за вниманием для каждой из них.
Сначала возьмем пример из обучающей выборки.
BLEU.
Наконец, мы рассчитываем оценку BLEU для трансформатора.
Мы получили оценку BLEU 36,52, что превосходит ~ 34 для свёрточной модели sequence-to-sequence и ~ 28 для модели RNN, основанной на внимании. И все это с наименьшим количеством параметров и самым быстрым временем обучения!
Обучение сети инвертированию предложения.
В конце приведу один из моих любимых тестов: тест на инверсию предложения. Очень простая для человека задача ученики начальной школы обучаются за 10-15 примеров, но, порой, непреодолима для искусственных систем.
Для Google Colab скачаем обучающие последовательности.
В начале обучим сеть инверсии и посмотрим на результат.