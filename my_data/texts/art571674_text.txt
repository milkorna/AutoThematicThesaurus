
В этой статье я расскажу о том, как создавался проект HuggingArtists и что у него под капотом. Мне будет очень приятно, если вы поставите сразу звезду в репозитории:
Вступление.
В HuggingArtists , мы можем создавать тексты песен на основе конкретного исполнителя. Это было сделано путем fine-tune (точной настройки) предварительно обученного трансформера HuggingFace на собранных данных Genius . Кроме того, мы используем интеграцию Weights & Biases для автоматического учета производительности и прогнозов модели.
Запустить Демо →.
Полный отчет с красивыми и интерактивными графиками можно прочитать тут.
Все запуски сохраняются и визуализируются с помощью Weights & Biases и сохраняются по ссылке:
Disclaimer: Данный проект не предназначен для публикации какой-либо ложной информации или неприятных слов, а предназначен для проведения исследований по Natural Language Generation.
Примеры генераций.
Eminem:
Drake:
Собираем датасет.
Перед тренировкой нашей нейросети, мы должны собрать датасет и привести его в должный вид. Так как чистые данные — залог успеха. Поэтому убираем из него повторяющиеся пробелы и новые строки.
Парсим данные.
Все данные были собраны с Genius с помощью этого скрипта.
Здесь мы используем asyncio и aiohttp для парализации сбора данных.
Все наборы данных доступны здесь: ссылка.
Пример использования в Colab:
Как использовать датасеты.
Как загрузить набор данных непосредственно из библиотеки datasets (пример — Eminem):
Структура датасета.
Пример «train» выглядит следующим образом:
Поля датасета.
Поля данных одинаковы для всех разбиений.
text : строка.
Разделение данных.
Все данные сгруппированы в «train», но можно легко разделить на «train», «validation» и «test» с помощью нескольких строк кода:
Как использовать модели.
Проще всего использовать Colab за пару минут:
Также вы можете использовать любую модель непосредственно с помощью pipeline для генерации текста:
Или с библиотекой трансформеров:
Предварительная обработка данных.
Мы будем использовать tokenizer , чтобы наша нейросеть могла понимать текст. Tokenizer переводит текст в цифры. Например:
Результат:
Программа возвращает словарь. input_ids — это индексы, соответствующие каждому токену в нашем предложении. Про attention_mask и token_type_ids можно почитать в документации.
Токенизатор может декодировать список идентификаторов токенов в соответствующем предложении:
Результат:
Теперь нам нужно тожесамое сделать с нашими данными. Мы можем вызвать токенизатор для всех наших текстов. Это очень просто, используя метод map из библиотеки datasets . Сначала мы определяем функцию, которая вызывает токенизатор в наших текстах:
Затем мы применяем его ко всем разделениям в нашем объекте datasets , используя batched=True и 4 процесса для ускорения предварительной обработки. После этого нам не понадобится текстовая колонка, поэтому мы ее отбрасываем.
Если мы теперь посмотрим на элемент наших наборов данных, мы увидим, что текст был заменен на input_ids , которые понадобятся модели:
Результат:
Теперь самое сложное: нам нужно объединить все наши тексты вместе, а затем разделить результат на небольшие фрагменты определенного размера блока. Для этого мы снова будем использовать метод map с параметром batched=True . Эта опция фактически позволяет нам изменять количество примеров в наборах данных, возвращая другое количество примеров, чем мы получили. Таким образом, мы можем создавать наши новые образцы из серии примеров.
Во-первых, мы берем максимальную длину, с которой наша модель была предварительно обучена. Это может быть слишком большим, чтобы поместиться в оперативной памяти вашего графического процессора, поэтому здесь мы берем немного меньше-всего.
Затем мы пишем функцию предварительной обработки, которая будет группировать наши тексты:
Во-первых, обратите внимание, что мы дублируем входные данные для наших меток. Это связано с тем, что модель библиотеки Transformers применяет смещение вправо, поэтому нам не нужно делать это вручную.
Также обратите внимание, что по умолчанию метод map отправит пакет из 1000 примеров для обработки функцией предварительной обработки. Поэтому здесь мы отбросим оставшуюся часть, чтобы сделать объединенные маркированные тексты кратными block_size каждые 1000 примеров. Вы можете настроить это, передав больший размер пакета (который также будет обрабатываться медленнее). Вы также можете ускорить предварительную обработку с помощью многопроцессорной обработки:
Ура! Наши данные готовы!
Настраиваем тренера.
Тренер — это простой, но функциональный цикл обучения и оценки для PyTorch, оптимизированный для трансформеров.
Тренируем нейросеть.
Запускаем тренировку и ждем:
Ожидаем завершения и сохраняем модель куда удобно.
Анализируем результаты.
Полученые результаты оказались очень даже неплохими. В них присутствует хорошая рифма и даже бэки с аирбэками. Если в полученых результатах присутствуют многочисленные повторения, то модель недостаточно натренирована.
Все запуски сохраняются и визуализируются с помощью Weights & Biases и сохраняются по ссылке:
Полный отчет с красивыми и интерактивными графиками можно прочитать тут.
Запустить Демо →.
Заключение.
Автор: Алексей Коршук ﻿.
Если вы хотите внести свой вклад в этот проект ИЛИ создать что-то классное вместе — свяжитесь со мной: ссылка.
Поставте звезду репозиторию проекта: