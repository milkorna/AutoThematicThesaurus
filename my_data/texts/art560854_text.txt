Исследователи из Пекинской академии искусственного интеллекта объявили о выпуске собственной генеративной модели глубокого обучения, Wu Dao, которая способна конкурировать и даже превзойти GPT-3 от OpenAI.
Будучи обученной на 1,75 трлн параметров, Wu Dao 2.0 примерно в десять раз больше, чем GPT-3 (175 млрд). Она превзошла также Google Switch Transformer с 1,6 трлн параметров. Модель обучали на китайском и английском языках на 4,9 терабайт изображений и текстов.
Вторая версия Wu Dao 2.0 вышла всего через три месяца после выпуска первой в марте. Исследователи BAAI сначала разработали систему обучения с открытым исходным кодом FastMoE, похожую на Google Mixture of Experts. Она работает на PyTorch и позволяет обучать модель как на кластерах суперкомпьютеров, так и на обычных графических процессорах. Это дало FastMoE большую гибкость, чем системе Google, поскольку FastMoE не требует проприетарного оборудования, такого как TPU от Google.
В отличие от большинства моделей глубокого обучения, которые выполняют единственную задачу - либо генерируют текст, либо создают дипфейки, либо распознают лица  - Wu Dao является мультимодальной системой.
Исследователи продемонстрировали способности модели выполнять задачи по обработке естественного языка, генерации текста, распознаванию изображений и созданию изображений. Модель может не только писать эссе, стихи и двустишия на китайском языке, но и генерировать альтернативный текст на основе статического изображения и почти фотореалистичные изображения на основе описаний на естественном языке. Wu Dao также продемонстрировал свою способность предсказывать трехмерные структуры белков, таких как AlphaFold.
Разработчики отмечают, что модель требует лишь небольшого количества новых данных при использовании в новой задаче. Тан Цзе, заместитель директора BAAI по академическим вопросам, утверждает , что она позволит создавать «думающие» машины.
С моделью работают уже 22 партнера, в том числе Xiaomi, поставщик услуг доставки Meituan и соцсеть коротких видео Kuaishou.
GPT-3 для написания текстов на основе всего нескольких примеров обучали на 570 гигабайтах текста. Модель представили в мае 2020 года. GPT-3 может отвечать на вопросы по прочитанному тексту, а также писать стихи, разгадывать анаграммы и осуществлять перевод. Алгоритму достаточно от 10 до 100 примеров того, как выполнить действие.
Google представила свою модель Switch Transformer в январе. Исследователи применили метод «редко активируемого», который использует только подмножество весов модели или параметры, которые преобразуют входные данные. Таким образом, Switch Transformer включает несколько моделей, специализирующихся на различных задачах, и «стробирующую сеть», выбирающую, к какой из этих моделей обращаться в конкретном случае.