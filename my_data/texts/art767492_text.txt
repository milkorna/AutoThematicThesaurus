Обращаясь к мастерам научной фантастики, всё чаще удивляешься их проницательности. В рассказе Артура Кларка «Девять миллиардов имён Бога» компьютер воплотил пророчество тибетских монахов о наступлении конца света, а в повести Ника Горькавого «Астровитянка» ИИ был единственным другом маленькой девочки в течение десятка лет и помог ей выжить на чужой планете. Многие люди, включая специалистов, верят как в позитивный, так и в негативный путь развития искусственного интеллекта. К счастью, подобные системы являются лишь инструментом, который можно использовать себе во благо, или оставаться в стороне от этого. Уверен, что в течение нескольких лет ажиотаж вокруг нейросетей постепенно спадёт до такой степени, что мы будем относиться к ним как к ещё одному подарку технического прогресса.
А пока мы с вами находимся в настоящем и предвкушаем наступление новой технологической эры, предлагаю разобраться в основах машинного обучения и познакомиться с новой версией GigaChat'а. В ней нам удалось добиться качественного прорыва, обойти аналогичные по размеру языковые модели, а также расширить максимальную длину входного запроса модели и проделать множество других улучшений. Но обо всём по порядку.
Что такое языковая модель.
В прошлый раз мы рассказывали про GigaChat и его возможности, и даже выкладывали лежащую в его основе языковую модель ruGPT-3.5 13B в открытый доступ . Давайте немного освежим в памяти знания о том, что такое языковая модель, а заодно обсудим базовые понятия в машинном обучении и NLP (Natural Language Processing, обработка естественного языка).
Нашей первой задачей является научить машину понимать человеческий язык и выполнять наши просьбы, как если бы мы просили об этом человека. Мы не ставили себе цели выучить наш родной язык и оцениваем его знание как данность, поэтому очевидного пути для достижения поставленной задачи в полной мере не видится, и на протяжении последних десятилетий было разработано много различных подходов.
Модели на правилах.
Самое простое, что может прийти в голову, это набор правил, по которым алгоритм будет отвечать заготовленными шаблонами и отвечать некоторой заглушкой, когда подходящего правила нет.
— Привет — Добрый день! — Меня зовут Гриша — Приятно познакомиться — Сочини хокку про соловья — К сожалению, у меня нет правила для ответа на данный вопрос.
Можно написать тысячи таких правил, добавить для каких-то из них память (например, чтобы запомнить имя пользователя), добавить гибкости через предварительную классификацию пользовательского ввода, чтобы расширить «попадание» в наши правила.
— Хай — Добрый день! — Меня зовут Лиза — Приятно познакомиться, Лиза! — Сочини хокку про соловья — По этому запросу я могу выдать вам стихотворение Пушкина “Птичка”.
Выглядит довольно ограниченно, но такие системы сегодня активно используются, когда необходимо обработать небольшое количество сценариев диалога. Например, при обращении в техническую поддержку или при заказе пиццы по телефону. Одним из ярких примеров такой системы можно смело назвать программу ELIZA, разработанную в середине 1960-х годов Джозефом Вейценбаумом. По описанию автора, ELIZA является пародией на работу психотерапевта, так как в большинстве случаев переадресует вопрос собеседнику, а также вычленяет ключевые слова, наподобие «отец» и «мать» и отвечает глубокомысленными заготовками. При отсутствии шаблона, Элиза могла ответить «Я понимаю» и попросить продолжить.
Вот пример подобного диалога, смоделированного через GigaChat:
Prompt.
Здесь мы подали в GigaChat особенное начало нашего запроса (он называется «prompt» («промпт»), а также «затравка» или «подводка»), в котором попросили вести ассистента определённым образом. Есть ещё один вид промпта, системный, и он, как правило, не виден пользователю. В нём обычно находятся базовые инструкции по поведению модели, указывающие ей отвечать полно, вежливо и доступно.
Возвращаясь к моделям на правилах, можно сказать, что выглядят они довольно ограниченно, но такие системы используются и сегодня, когда необходимо обработать небольшое количество сценариев диалога. Например, при обращении в техническую поддержку.
Статистические модели.
Следующим подходом в развитии языкового моделирования являются статистические модели. Давайте попробуем взять все буквы алфавита и посчитать, сколько раз каждая из них идёт за остальными в каком-либо наборе текстов (корпусе). Для примера возьмём список из тысячи женских имён и составим по ним таблицу переходов (точка здесь означает начало или окончание слова):
Полученная модель будет называться n-граммной, а в нашем конкретном случае биграммной, так как мы посчитали комбинации из двух символов.
Давайте придумаем несколько новых имён с помощью нашей модели. Чтобы это сделать, разберёмся с вероятностями. Вероятность P(’.а’) = 0,12 означает, что при генерации N первых символов имени примерно 12% из них будут буквой «а». То же самое будет и с переходами: P(’ин’) = 0,21 будет означать, что в 21% случаев после буквы «и» будет идти буква «н». Таким образом, подставляя последнюю букву генерируемого слова в нашу модель, мы будем получать следующую, пока нам не попадётся точка. Проделаем так 10 раз:
гинэса. мизгуза. зувия. нама. алинцития. слибожанака. фаня. микаса. тальгеора.
Путём нехитрых подсчетов мы научились создавать новые слова, напоминающие обучающую выборку, состоящую из женских имён. Такой же трюк можно проделать с любым другим списком слов и нагенерировать, например, воображаемые названия городов или блюд неизвестной кухни.
Нейросетевые модели, про которые мы поговорим дальше, уже видели подобные корпуса и много чего ещё, поэтому также помогут вам сгенерировать что-то необычное без подсчёта необходимых статистик.
Как и модели на правилах n-граммные модели не являются достаточно гибкими для полноценной генерации текста. Если интересно, почему так, предлагаю обсудить это в комментариях, а мы тем временем продолжим наше знакомство с системами искусственного интеллекта. На очереди нейросетевые модели.
Нейросети.
Мы не зря рассмотрели вышеприведённые примеры языковых моделей, они помогут нам понять основной принцип работы нейросетей.
Если в n-граммной модели, где n = 11, мы рассчитаем статистику для одиннадцати слов, то сможем генерировать текст, получая вероятности очередного слова, опираясь на контекст лишь в десять слов назад, при этом мы не будем обращать внимание на какое-то конкретное слово в этой последовательности.
Нейросети, по сути, тоже работают на статистическом уровне, предсказывая кусочек текста за кусочком (такой кусочек называется токеном и обычно состоит из нескольких символов). Количество и состав токенов мы задаём сами на этапе построения токенизатора, от этого будет зависеть максимальная длина текста, с которым сможет работать модель. Например, в ChatGPT средняя длина токенов для русского и английского языков различается, так как русский не является в модели основным и был мало представлен на этапе обучения.
Контекст большинства современных языковых моделей начинается с 2 тысяч токенов, а это примерно 3–4 листа печатного текста формата A4. Помимо этого, внутри у сети не просто словарь с вероятностями, по которому расчитывается распределение токенов. Обученная нейросеть умеет обращать внимание на определённые части входного текста, которые она считает более важными в каждый момент генерации. Подробнее про этот механизм и архитектуру трансформерных моделей можно почитать в статье Джея Аламмара “The Illustrated Transformer”.
Но главная прелесть обучения нейросетевых моделей заключается в том, что у вас нет необходимости писать конкретные «правила» для его работы, модель сама научится обобщать свои знания на основе предоставленных данных. Данные эти должны представлять собой огромный массив текстов с самыми разнообразными знаниями. Десятки тысяч книг, миллионы документов, миллиарды интернет-страниц, научные статьи, песни, стихи и всё остальное.
Собрать и подготовить соответствующий набор данных, обучить модель с нуля на кластере из тысяч графических ускорителей, переделывать, когда что-то не получается, и постоянно придумывать пути для улучшения качества — очень сложная задача. Но оно того стоит.
GigaChat.
Про основные этапы обучения моделей, лежащих в основе GigaChat'а (мы назвали их NeONKA — NEural Omnimodal Network with Knowledge-Awareness) и больших языковых моделей (они же LLM, Large Language Models) вообще, вы можете прочитать в нашей статье . Если вкратце, то процесс обучения состоит из подготовки данных, обучения основной модели (pretrain) и её дополнительного выравнивания на инструктивных наборах данных (SFT, supervised fine-tuning и RLHF, reinforcement learning from human feedback).
Переосмыслив накопленные знания, мы обратили пристальное внимание на процесс подготовки данных, значительно расширив и улучшив наш обучающий набор. Это миллионы документов, книг и скачанных интернет-страниц. А так как любой эксперимент с LLM требует большого количества вычислительных ресурсов, то мы вложили много сил в оптимизацию обучения, увеличив его скорость в разы.
Всё это позволило нам прийти к новой линейке моделей, одну из которых мы с сегодняшнего дня начинаем использовать в GigaChat’е. Помимо очевидных вещей, вроде качества получаемых генераций, мы хотим поделиться и некоторыми метриками, полученными для первой модели из этой линейки, модели размером в 7 миллиардов параметров.
Метрики.
Одним из самых популярных тестов для оценки больших языковых моделей является MMLU. Он представляет собой набор закрытых вопросов (когда нужно выбрать один из предложенных вариантов) на 57 тем (математика, физика, экономика и другие). Подробнее про него можно почитать в статье “ Measuring Massive Multitask Language Understanding ”.
При случайном гадании этот тест покажет около 25% правильных ответов. Самая продвинутая на сегодняшний день модель GPT-4 от OpenAI показывает результат в 86,4%, Llama 2 7B — 45,3%, а люди, в зависимости от квалификации, по оценкам авторов теста получают в районе 34,5% для среднестатистических пользователей (замерялось на платформе MTurk) и до 89% для экспертов в своей области.
Этот показатель можно считать за условную «умность» модели, хотя и к нему и есть некоторые вопросы, связанные со способом замеров. Как бы то ни было, но в первой итерации мы показывали около 30% правильных ответов. Проверив другие гипотезы, нам удалось добиться того, чтобы этот показатель пробил и 30%, и 40%, а затем и 50%, составив 51,49%.
Чтобы понять, какие улучшения получила модель с точки зрения конкретных прикладных задач, мы провели сравнительный тест (side by side) с нашей предыдущей лучшей моделью (13 млрд параметров). Вот результат этого теста:
Значительно улучшились навыки суммаризации текстов, ответов на вопросы, генерации идей и перевода.
Кроме сравнения с предыдущей моделью мы провели SBS-сравнение  с моделью ChatGPT (gpt3.5-turbo). Здесь мы также зафиксировали рывок до 43/57 против 33/67 у предыдущей модели. Результат отличный, но есть куда стремиться.
Напоследок.
Само собой, мы любим наши модели и, как следствие, относимся к ним с бóльшим теплом, чем к конкурентам. Поэтому предлагаем вам самим оценить качество генераций, так как GigaChat сейчас доступен для всех бесплатно. Попробовать его можно на различных платформах.
Если хотите попробовать погенерировать через web-интерфейс (он хорош тем, что сохраняет историю ваших диалогов, позволяет добавлять запросы в избранное и т. д.), нужно перейти по ссылке https://developers.sber.ru/gigachat/ и зайти в свой аккаунт (при первом входе нужно будет зарегистрироваться).
В Telegram обитает @gigachat_bot , которому можно написать напрямую или добавить его в ваш чат с друзьями и коллегами.
Появился новый бот и в VK, для общения с ним переходите по адресу vk.me/gigachat . Его также можно добавить в групповой чат на этой платформе.
Нам очень интересна ваша обратная связь и идеи по развитию. Например, ничего не было сказано про мультимодальность, а ведь в GigaChat'е реализована генерация изображений на основе нейросети Kandinsky. Возможно, вы хотели бы добавить генерацию видео? В общем, ждём ваших идей.
Благодарности.
Как уже говорилось ранее, задача по осуществлению такого проекта требует титанических усилий. Участвуя в этом процессе, я не перестаю удивляться энтузиазму коллег, генерирующих идеи и готовых воплощать их в жизнь в любое время дня и ночи. Девушки и парни из SberDevices, Sber AI и Научно-исследовательского института искусственного интеллекта AIRI, вы лучшие.
На связи с вами.
Также приглашаю вас в наш Telegram-канал Salute AI , в котором мы с коллегами начали делиться наработками в области машинного обучения и другими рабочими моментами. А в соответствующем чатике Salute AI Community можно напрямую поспрашивать про всё это и просто пообщаться.
