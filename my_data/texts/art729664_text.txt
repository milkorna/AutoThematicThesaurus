Привет, Хабр!
Как не трудно догадаться из названия, сегодня пойдет речь о перспективном фреймворке для работы с языковыми моделями LangChain. Если вы хотите создать свой собственный ChatGPT, то этот инструмент поможет сильно ускорить процесс разработки вашего AI приложения. Langchain работает по принципу лего и позволяет собрать как заправку, так и Звезду Смерти – все детальки заботливо предоставлены разработчиками.
Основные компоненты.
Модели: универсальный интерфейс для работы с различными языковыми моделями. Можно использовать API OpenAI, Cohere, Hugging Face и других. Также есть возможность работать с локальными моделями.
Промпты: LangChain предоставляет ряд функции для работы с промптами – представление промпта согласно типу модели, формировани шаблона на основе внешних данных, форматирование вывода модели.
Индексы: Индексы структурируют документы для оптимального взаимодействия с языковыми моделями. Модуль включает функции для работы с документами, индексами и их использования в цепочках. В том числе поддерживает индексы, основанные на векторных базах данных.
Цепочки: С помощью цепочек можно объединять разные языковые модели и запросы в многоступенчатые конвееры. Цепочки могут быть применены для разговоров, ответов на вопросы, суммаризаций и других сценариев.
Агенты: С помощью агентов модель может получить доступ к различным источникам информации, таким как Google, Wikipedia итд.
Память: позволяет сохраняет состояния в цепочках. Например, для создания чат-бота можно сохранять предыдущие вопросы и ответы. Существует два типа памяти: краткосрочная и долгосрочная. Краткосрочная память передает данные в рамках одного разговора. Долгосрочная память отвечает за доступ и обновление информации между разговорами.
Готовим окружение и данные.
Посмотрим как этот инструмент работает на практике. В своей прошлой статье я рассказывал про плагин-ретривер к ChatGPT. Плагин позволяет подключить свою базу данных к LLM и использовать эти знания для формирования ответов. Почему бы не попытаться воспроизвести его самостоятельно на LangChain, тем более, что OpenAI так и не открыл мне доступ. Что нам понадобится:
Векторная база данных. За это будет отвечать модуль Indexes.
LLM для генерации ответа - блок Models. Здесь можно будет попробовать ChatGPT и какой-нибудь клон альпаки.
Передать знания из нашей базы в LLM. На помощь придет модуль Prompts.
Настраиваем окружение для проекта.
Создаем директорию mkdir langchain и переходим в нее cd langchain.
Создаем виртуальное окружение. Я буду использовать conda и python 3.11.
Также зарегистрируем наш кернел в jupyter.
Устанавливаем LangChain и сопутствующие библиотеки.
Готовим данные для нашего векторного хранилища.
Как и в прошлый раз, попробуем автоматизировать службу поддержки и научим бота отвечать на вопросы пользователей. Сгенерируем синтетические данные с помощью следующего запроса:
ChatGPT выдал нам следующие варианты:
Индексируем документы в FAISS.
Перед индексацией нужно будет подумать над моделью векторизации. Можно использовать разные опции: обучить собственный векторайзер(например на базе моделей из SentenceTransformers ), либо взять что-то из коробки. Я воспользуюсь решением от OpenAI(в этом случае нужно будет также добавить ключ доступа к API).
В качестве векторной БД буду использовать FAISS - сейчас это самое шустрое решение для поиска ближайших соседей. В LangChain также есть реализации для множества других хранилищ(ElasticSearch, Redis, Pinecone итд.).
Вроде все работает.
Добавляем силу ChatGPT.
Мы убедились, что наш векторный поиск вытаскивает релевантные документы. Теперь наша задача передать их на вход LLM для получения более развернутого и человекоподобного ответа. В LangChain это можно сделать несколькими способами:
Построить цепочку load_qa_chain из ответов нашего ретривера.
Обратиться напрямую к векторной БД RetrievalQA.
Получить конкретный контекст и передать в цепочке LLM LLMChain.
Попробуем пойти самым коротким путем:
ИИ тут явно переврал, тк нам нужно сначала перейти в раздел Управление командой. Как можно исправить этот косяк? Ну, наверное, надо как-то явно сообщить модели, чтобы она не трогала информацию в кавычках. Модифицировать промпт мы будем с помощью инструмента PromptTemplate.
А что Alpaca?
ChatGPT хорош, но как же новомодные легковесные архитектуры, которые позволят каждому школьнику создать свой скайнет. Для русского языка есть уже целое стадо таких моделек.
Такое. Явно нужно как-то иначе формулировать промпт и поиграться с шрифтами настройками модели.
Использовать модели по API может быть не очень оптимально. В LangChain вы можете припарковать их на собственный хост через функционал SelfHostedPipeline и SelfHostedHuggingFaceLLM.
Осталось протестировать возможности памяти, но я, конечно же, это делать не буду. Пусть это будет задачкой со звездочкой для вдумчивого читателя. Спасибо за внимание!
Пишу про AI и NLP в телеграм.