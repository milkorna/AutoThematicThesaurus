Автор статьи: Рустем Галиев.
IBM Senior DevOps Engineer & Integration Architect. Официальный DevOps ментор и коуч в IBM.
Одним из самых крутых и, возможно, самых неприятных приложений NLP является генерация текста. Способность генерировать убедительный текст с помощью ИИ имеет широкое применение, от чат-ботов до создания художественной литературы или фейковых новостей. Сегодня мы рассмотрим создание фейковой фантастики на основе романа Льюиса Кэрролла «Алиса в стране чудес». Из реквизитов на нужен Python и Tensorflow.
Мы загружаем некоторые импорты для обработки текста и загрузки:
Загружаем книгу из Project Gutenberg с помощью:
После того, как книга скачана, мы загружаем текст и очищаем его регулярным выражением, используя re.sub . Затем мы извлекаем только начало книги и выгружаем ее на консоль.
Токенизировать и кодировать.
Текст загружен, теперь можно переходить к токенизации и кодированию. Для генерации этого текста мы будем работать с токенами символов, а не с токенами слов. Токенизируя символы, мы сокращаем пространство для обучения с большого словарного запаса до чего-то более легкого для обучения.
Мы можем извлечь словарь токенов символов из текста с помощью:
Затем создадим кодировку для символов и функций отображения с помощью:
Функции отображения char2idx и idx2char отображают символы в индексы и обратно. Эти функции помогают нам кодировать, а затем декодировать символы.
Затем мы можем построить обучающие наборы с помощью:
Переменная example_per_epoch — это количество выборок или фрагментов текста, которые мы будем передавать модели. char_dataset — это преобразование кодировок text_as_int в тензоры.
После разметки текста и закодированных символов переходим к построению обучающих последовательностей.
Разобьем наш документ на обучающие последовательности. Помните, что слои RNN изучают последовательности токенов. Наша цель здесь состоит в том, чтобы передать последовательности и последовательности смещений, чтобы обучить модель генерации текста.
Создать их достаточно легко с помощью:
Затем мы берем и выгружаем 5 обучающих последовательностей в качестве примера.
Поскольку мы обучаем сеть последовательностям, создадим входную последовательность, а затем таргетовую или целевую. При использовании RNN целевой последовательностью будет входная последовательность, смещенная на один символ.
Создать входную и таргетовую последовательности можно с помощью простой функции карты:
После чего можем взглянуть на входную и таргетовую последовательность с помощью:
Таким образом, когда мы обучаем сеть, входной символ всегда должен сопоставляться с ожидаемым выходным символом. Пример того, как это выглядит, показан ниже:
На каждом шаге примера показаны входные данные, а затем ожидаемые выходные данные, к которым мы будем обучать сеть.
Построение и обучение модели.
Все данные готовы, поэтому мы можем перейти к построению модели и ее обучению.
Однако перед этим установим некоторые гиперпараметры с помощью:
Также в этом коде мы извлекаем данные в обучающие пакеты и выводим форму набора данных.
Далее можем создать модель с помощью:
В этой модели используются 2 уровня RNN типа GRU или вентилируемого рекуррентного блока. Уровни GRU проще, чем LSTM, и не требуют ввода состояния.
Для этого примера определим пользовательскую функцию потерь, а затем скомпилируем модель следующим образом:
Мы хотим повторно использовать модель для генерации текста позже. Таким образом, мы хотим сохранить копию модели во время ее обучения. Можно сохранить модель как контрольные точки, создав функцию обратного вызова контрольной точки следующим образом:
Сети RNN требовательны к производительности, и для их обучения на серверах CPU может потребоваться время. Поэтому ниже было определено несколько параметров для определения количества периодов обучения:
Построенна модель и установленна контрольная точка, мы можем перейти к обучению модели с помощью:
Обратите внимание, что обучение RNN дорого обходится и может занять много времени. Мы тренируем здесь только 5 эпох в демонстрационных целях. Чтобы получить хорошо настроенную модель, этот пример лучше всего запустить с 50 эпохами.
Обучив модель генерации текста, можем перейти к созданию нового интересного текста.
Для начала нам нужна пользовательская функция для запроса модели и генерации текста:
Эта функция принимает в качестве входных данных модель, начальную строку, температуру и количество символов для генерации. Температура используется для определения предсказуемости текста. Более низкая температура (0,25) создает интеллектуальный текст. В то время как более высокая температура (2.0) генерирует более уникальный текст. Более высокие температуры могут привести к бессмысленному тексту.
Теперь, перестроим нашу модель, используя только 1 вход или батч. После того, как модель построена, мы загрузим ранее обученные веса, а затем построим ее с входной формой из 1 элемента следующим образом:
С перестроенной моделью теперь мы можем сгенерировать некоторый текст:
На выходе будут 3 строки текста, сгенерированные с разными температурами. Обратите внимание на различия в генерации текста. Генерация текста может быть одной из самых интересных, но и сложных задач в NLP. Большая часть генерации текста в наши дни выполняется текстовыми преобразователями, такими как BERT или GPT. Использование преобразователей — это продвинутая процедура моделирования NLP, которая лучше подходит для всех текстовых задач.
В завершение хочу порекомендовать вам полезный вебинар посвященный задаче Question-Answering,  крайне востребованной задачи в области NLP сегодня. На вебинаре эксперты OTUS расскажут о том, какие типы вопросно-ответных систем существуют сегодня, на каких принципах и методах они основаны и как они применяются в чат-ботах.
Зарегистрироваться на бесплатный вебинар.
