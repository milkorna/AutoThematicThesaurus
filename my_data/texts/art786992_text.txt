Дообучение больших языковых моделей на кастомных датасетах делает модели гораздо сообразительнее. Есть история успеха датасета Alpaca. Он творит чудеса с моделями, которые сначала если и умели что-то делать, то делали это очень плохо. Мы решили понять, как это делается, а главное - какие проблемы есть на этом пути и могут ли новички вроде нас разобраться в этом. История взлетов и падений - под катом.
Предыстория: делаем образование лучше с помощью языковых моделей.
Действие происходило на физтеховском семестровом курсе по интеллектуальным правам, я его преподаватель. В течение семестра у слушателей было несколько контрольных работ. Первая из них на тему “Авторское право”  представляла из себя развернутые ответы на вопросы. Например, “я отсканировал учебник и поделился копией в локальной сети с сокурсниками, правомерны ли мои действия?” и так далее.
Заодно контрольную работу выполнило несколько больших языковых моделей (LLM): Gigachat от Сбера, YandexGPT2 от Яндекса и ChatGPT от OpenAI. Мы  - я и все заинтересованные слушатели, дальше мы, так как совместно все делали - скормили им вопросы и оценили их ответы так же, как если бы на вопросы отвечал обычный человек, прослушавший вводный курс по авторскому праву.
Модели отвечали неодинаково, а на некоторые вопросы - прямо противоположно. Из разбора их ответов возникло несколько идей.
Первая идея - своеобразный челлендж. Можно ли своими силами прокачать слабенькую LLM так, чтобы она отвечала не хуже, а может быть даже лучше сильных? Мы решили воспроизвести успех альпаки и других датасетов, на которых сейчас прокачивают LLM. Для пробы мы решили взять относительно небольшую доменную область, которой занимались - интеллектуальные права.
Вторая идея - можно ли доработать полученные на ответах LLM и нашей оценке данные до полноценного отраслевого бенчмарка и применять его для измерения качества моделей.
В общем, мы решили создать что-то вроде курсового пет-проекта с целью не рассчитывать на успешный результат, а в первую очередь научиться делать такие датасеты. У нас был месяц времени, за который можно было что-то успеть.
Немного об уровне продвинутости участников проекта. Конечно, в проекте было несколько людей, которые работали над похожими задачами в других областях ML, и благодаря ним в итоге и удалось провести самую сложную часть проекта - эксперимент. Но у большинства участников не было опыта в создании датасетов, ML, программировании, то есть во многих вещах мы разбирались с нуля. Нам помогло, что к началу нашей работы уже были опубликованы примеры качественных датасетов и подробнейшие мануалы для самых неподготовленных пользователей. Их мы и использовали как точку отсчета.
Шаг 1. Изучаем примеры датасетов для тонкой настройки, добываем и обрабатываем данные.
Для того, чтобы понять, с чего начинать в создании датасета для тонкой настройки модели, мы начали с самых успешных примеров - датасетов с инструкциями. На них LLM обучается выполнять нужные пользователю действия и в дальнейшем переносит этот опыт на задачи, которых нет в обучающем датасете. Так что мы принялись изучать примеры с Hugging face.
Структурированы датасеты с инструкциями похожим образом. Есть инструкция, то есть требование совершить определенное действие, и есть пример выполнения инструкции.
Мы нашли примеры неанглоязычных датасетов, например, на вьетнамском и испанском языках. Чаще всего попадались переводы альпаки с английского на другие языки. Мы нашли датасеты для доменных областей знания, например, для медицины и математики. Словом, нужное нам пересечение в датасетах присутствует, что нас приободрило. Вспомним, что в нашем случае мы хотели получить на выходе русскоязычный датасет для одного из разделов гражданского права.
Для добычи данных мы решили взять наиболее близкие к практике примеры, где встречается что-то похожее на инструкции и примеры их реализации. Сначала мы рассматривали вопросно-ответную структуру, которая встречается на юридических форумах, где люди приходят со своей проблемой, которую разбирают юристы. Проблема с ними в том, что эти данные нужно буквально вручную перебирать, так как ответы юристов часто не связаны с тематикой вопроса. Например, помимо ответов юристы могут общаться друг с другом, что конечно не связано с темой вопроса.
Второй вариант, на котором мы в итоге и остановились, это судебные решения. В нашем случае был идеально подходящий набор данных, так как подавляющее большинство вопросов в области интеллектуальных прав разбирает один и тот же суд, а законодательство в этой области мало менялось: четвертая часть гражданского кодекса РФ была принята в 2006 году, а суд по интеллектуальным правам работает с 2013 года. То есть данные более-менее консистентны и хорошо структурированы. Кроме того, на тексты судебных решений не распространяется копирайт, то есть при достаточном уровне анонимизации их можно свободно использовать.
В сумме мы нашли тексты 4163 судебных решений нужного нам суда.
Чтобы не нарушать законодательство о персональных данных, мы анонимизировали текст решений. Для анонимизации мы использовали модель ru_core_news_lg библиотеки Spacy, которая распознает именованные сущности (person, organization, location). Для надежности мы провели жесткую анонимизацию, удалив все упоминания людей, организаций и их адресов, хотя для самого необходимого уровеня соответствия закону о персональных данных можно было ограничиться именами людей, так как наименования организаций и их адреса строго говоря не являются персональными данными.
Мы решили разделить решения на исковую и резолютивную части. По задумке исковая часть - это наша инструкция,  так как в ней истец просит разобраться в ситуации. А резолютивная часть - это пример выполнения инструкции. Так как LLM должна понимать, в какой части датасета содержатся инструкции, для разметки нашего корпуса мы выбрали синтаксис <s> [INST] инструкция [/INST] пример выполнения инструкции </s>, который используется в датасете альпаки.
Проблема, которая возникла на этом этапе - это то, как впихнуть текст решения в контекстное окно модели, которое ограничено определенным количеством токенов.
Мы попробовали два варианта. Первый - очевидный - вариант - это суммаризация текста. Второй, менее очевидный - разрезание текста на чанки.
Суммаризацию мы попытались сделать несколькими способами. Первая попытка - попросить LLM вытащить из текста требования истцов и решения суда - сначала не удалась. Дело в том, что нам была нужна была специфика каждого дела, к примеру, “выкладывать учебник в сеть можно, если он под свободной лицензией”. Поначалу по причине неверной формулировки промпта вместо специфики дела модель выдавала нам краткий пересказ процессуальной части решения - какие документы подал истец, в какой срок суд их принял и так далее. Вот пример плохого запроса и ответа модели gpt-3.5-turbo-16k на основе текста этого судебного решения:
То есть знание о предмете судебного спора мы так и не извлекли.
Путем эмпирического подбора промптов более-менее сработал вариант со смешанным промптом: создание "личности" LLM, экстракция нужной нам части и суммаризация. Лучший результат дал англоязычный запрос. К другим условиям возможно относится то, что в этот раз на вход мы подавали текст с мягким вариантом суммаризации, то есть убирали только имена людей. Вот пример обработки этого решения.
Другое решение, который мы использовали, - это вариант суммаризации всего текста. Конечно, на выходе получается текст без инструкции, но зато в каждом случае мы получили более-менее хорошее описание всего документа.
Третий вариант - разрезать исковую и резолютивную часть на чанки и сделать комбинацию всех исковых частей со всеми резолютивными частями. Мы сделали несколько вариантов таких датасетов с длиной до 512, 1024 и 2048 символов, их можно найти здесь.
Шаг 2. Делаем проверочный датасет и автоматизируем оценку ответов.
Для того, чтобы создать проверочный датасет, мы начали изучать и сравнивать между собой различные примеры отраслевых датасетов. Очень помогла статья о создании датасета с инструкциями на основе медицинских карт пациентов. В его создании участвовали медики-добровольцы. Они написали самые нужные для их отрасли инструкции и дали примеры того, как их лучше выполнять. После этого инструкции подавали на вход различным LLM и сравнивали их ответы с эталонным примером.
Это очень похоже на то, что мы делали в самом начале, но нужно было сформулировать эталонные примеры.
Первая идея - взять вопросы-ответы из интернета. Но нам нужна гарантия, что этих примеров не было в обучающей выборке. Поэтому мы решили выдумать свои задачи и самим же дать на них обоснованные ответы со ссылками на законы.
Сначала мы взялись за создание проверочного датасета по схеме “вопрос - ответ модели - оценка ответа - комментарии проверяющего”, но, поразмыслив, поменяли комментарии на правильные ответы. Вот пример того, что у нас получилось.
Еще один важный момент, который нам пригодился, это использование автоматизированных методов оценки ответов моделей, которое упоминалось в статье о медицинском датасете. Идея была в том, чтобы ответ языковой модели проверяла другая языковая модель. В нашем случае это развернутая на сервере модель OpenChat, которая оценивает, верный или неверный ответ на вопрос (True / False) дала модель, а затем дает оценку  0 до 4 баллов за качество ответа и раскрытие темы. Результат того, что у нас получилось, есть на гитхабе.
Шаг 3. Проводим эксперимент и разбираем результаты.
В итоге мы добрались до эксперимента. Поскольку время курсового проекта истекало, мы успели провести только один эксперимент с моделью Llama2 по этой схеме.
С помощью графического процессора A100 мы дообучили модель на датасете с суммаризацией судебных решений на 1024 токена. Результат с дообученной моделью выложили здесь. Затем прогнали модель до и после дообучения на двух вручную собранных эталонных бенчмарках. Для оценки мы использовали модель OpenChat, которая сравнивала ответ модели с эталонным. Так как наш проверосный датасет к тому времени еще не был готов, пробный “золотой стандарт” пришлось взять из русскоязычных вопросов и ответов на сайтах всемирной организации интеллектуальной собственности и российского авторского общества. Мы выбрали эти источники, потому что и та, и другая организации обладают экспертностью в области интеллектуальных прав.
Первый эталонный датасет был с вопросами и закрытым перечнем ответов, один или несколько из которых были правильные. Примерно на таком же варианте датасета модель ChatGPT проходила экзамен для адвокатов. Здесь мы получили прирост правильных ответов у дообученной модели по сравнению с дефолтной: с 0,4383561644 до 0,5479452055 за правильный / неправильный ответ на основе 73 пар вопросов-ответов и с 2,821917808 до 2,849315068 за раскрытие темы на основе 31 пары вопросов-ответов. Пример:
Заметно, что модель научилась говорить на русском языке и давать в целом правильную аргументацию.
Второй проверочный датасет содержал открытые вопросы и ответы. Здесь модель показала снижение качества ответов по сравнению с дефолтной: с 0,2580645161 до 0,1935483871 за правильный / неправильный ответ и с 2,580645161 до 2,516129032 за раскрытие темы.  Например, модель в какой-то момент путалась и  давала в целом неверный ответ:
Мы пытались понять, почему развернутый ответ дообученной модели деградировал по сравнению с ответом модели из коробки.
Одна из гипотез - модель Llama2 на стадии претрейна вероятнее всего специально не обучалась на русскоязычном корпусе и выучила его стихийно. То есть мы пытались нагрузить модель новыми знаниями, которые скорее всего не содержались в претрейне. Поэтому дообучение не дало ни прироста новых знания, ни прогревания старых. Частично это подтверждает статья, в которой авторы пытались создать аналогичный датасет и для этого на претрейне дообучили модель Llama 2 на китайском корпусе законов и судебных решений. Выход: дообучать русскоязычную модель, а еще лучше - следовать примеру китайских исследователей и самостоятельно провести претрейн на нужном нам наборе текстов.
Другая гипотеза: дообученная модель хорошо справилась с ответами на  формализованные вопросы и плохо с неформализованными вопросами-ответами. Та же ChatGPT сдавала адвокатский экзамен на формализованных вопросах-ответах, точно так же YandexGPT2 сдавала ЕГЭ - тоже формализованный вариант оценки. Возможно, мы требовали от модели больше того, на что она способна, и получили ухудшение оценки на неформализованных примерах. В нашем случае модель из коробки также показала снижение качества ответов на  неформализованные вопросы по сравнению с формализованными.
Что дальше.
В силу обстоятельств - дефицита времени и ресурсов - мы успели попробовать немногое из того, что хотели сделать. Так что планируем развивать проект дальше. Для этого, конечно же, нужно как можно сильнее погрузиться в тему дообучения моделей. Например, для эксперимента мы использовали код с датакэмпа без каких-либо изменений. Поэтому продолжим с того, что разоберемся в настройках дообучения и поэкспериментируем с их различными вариантами.
А еще у нас осталась куча других вариантов датасетов, с которыми также можно поэкспериментировать. Так что основная задача - получить мотивацию, запастись 8x A100 :) и двигаться дальше.
Контрибьюторы (в алфавитном порядке):
Полезные ссылки:
Датасет Alpaca.
Другие датасеты с инструкциями на Hugging face.
Видеотуториал по загрузке датасетов на Hugging face.
Туториал и еще один по дообучению LLM.
Датасеты с судебными решениями: суммаризация и чанки.
Дообученная модель.
Проверочный датасет.
Код для проверки.