Для выявления ключевых слов, для начала будет решена задача кластеризации на тематики текстов с помощью метода LDA (Latent Dirichlet Allocation). После этого будет решаться задача, непосредственно, выявления ключевых словосочетаний с помощью предобученной модели Bert. И завершающим будет метод WordToVec, служащий для решения задачи поиска наиболее семантически похожих слов в тексте.
Обучающих набор данных состоит из 110 текстов на общую тему аудит. Посмотрим, можно ли эти тексты разбить на более подробные тематики. Метод LDA предназначен для тематического моделирования, чтобы выявить абстрактные темы, на которые написан пул текстов.
Предобработка текста.
Однако, сначала наши данные нужно предобработать. Итак, наш корпус – это список из списков python, где каждая отдельная статья заключена в свой список. Это необходимо, так как для реализации данной модели нужно знать количество документов. Переведем весь текст в нижний регистр.
Далее удаляем числа.
На третьем шаге удаляем пунктуацию и все символы. Для реализации данного шага можно было бы воспользоваться стандартным перечнем пунктуации, однако для более качественного результата символы лучше просмотреть самостоятельно.
LDA.
Теперь, когда текст готов к построению модели, импортируем необходимые библиотеки:
Токенизируем текст –разделяем слова между собой.
Удаляем стоп-слова, которые также лучше самостоятельно просмотреть среди текстов, а не просто удалять стандартные. Проводим лемматизацию, то есть приведение каждого токена в начальную форму.
Далее формируем словарь, содержащий количество раз, когда слово появляется в обучающем наборе. При этом словарь фильтруем, и выбираем токены, которые появляются не менее, чем в 1 документе, а также, которые появляются более, чем в 0,8 документах от общего объема корпуса.
Для каждого документа мы создаем словарь, в котором хранится информация, какое слово сколько раз встречается.
Строим модель LDA, и передаем в качестве аргумента словарь, который сделали на предыдущем шаге.
С помощью следующих команд можно вывести красивую визуализацию метода с ключевыми словами для каждой выделенной темы.
Bert.
Bert – это модель, с помощью которой становится возможно преобразовать фразы в векторы, при этом не теряя их значения. Здесь не будет описана технология работы модели Bert, а будет просто приведен пример решения задачи выделения ключевых слов с помощью предобученной модели.
Для реализации данного метода проводится предобработка, описанная выше. Далее все тексты склеиваются в один, так как этот метод решает задачу выделения ключевых слов.
Для преобразования слов в векторную форму будем использовать CountVectorizer. Также мы установили гиперпараметр n_gram_range(2, 2,), что обозначает, что мы будем выделять словосочетания, состоящие из двух слов.
Далее используем Bert для представления ключевых слов-фраз в числовые данные.
Используем косинусное расстояние между векторами, с помощью которого будут искаться наиболее похожие друг на друга слова. И для вывода итогового результата можно использовать приведенный код.
WordToVec.
После 2х методов выше, можно применить модель WordToVec, которая сохраняет семантическое значение различных слов в документе.
Сначала проводится предобработка текстов. Далее с помощью библиотеки python-nltk токенизруем текст на предложения, а далее на слова.
Применяем модель WordToVec, и ставим гиперпараметр, отвечающий за то, чтобы в корпус слов попали только те, что попадаются 2 и более раз и формируем словарь из слов:
В команду ниже можно вставлять слова, например, полученные с помощью модели LDA, и смотреть, какие слова наиболее часто употребляются вместе с этим словом.
Таким образом, все методы можно использовать для выделения ключевых слов и словосочетаний, с помощью которых можно первоначально оценить тематику текстов. Метод LDA, хоть и служит для решения задачи кластеризации на тематики, но, тем не менее, является самым подробным, так как выводит ключевые слова для каждой темы. Word-to-vec служит для других целей, но также является эффективным методом решения. Bert показывает качество при работе со словосочетаниями.