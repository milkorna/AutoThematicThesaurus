Процессы и продукты банка всё время совершенствуются, и в какой-то момент приходит понимание, что рутинные операции нужно автоматизировать. Так случилось и у нас: возникла необходимость в автоматизации обработки текстовой информации. Это не только банковская тенденция — во многих сферах бизнеса сейчас растёт спрос на подобные решения, поэтому мы подумали, что хабровчанам тоже могут быть интересны наши изыскания в этой сфере. Так что сегодня расскажем  о том, как работает наш прототип AutoML для распознавания именованных сущностей (named entity recognition, NER). Ну и о том, какие результаты в итоге показала обученная модель.
Предпосылки создания.
Итак, для начала — в каких задачах бизнес-подразделений нам нужна автоматизированная обработка текста? Вот несколько примеров:
Мониторинг новостей в целях: оценки кредитных рисков компаний,. рекомендаций покупки/продажи ценных бумаг,. выявления взаимосвязанных компаний / бенефициаров.
Обработка внутренних и входящих документов для выявления отправителей, получателей, определения дат, названий организаций, номеров телефонов, номеров банковских карт.
Разбор команд, произнесённых клиентом голосовому помощнику, для идентификации получателя платежа, номера карты и счёта, суммы и цели перевода.
Вышеперечисленные задачи могут быть декомпозированы на несколько подзадач, одной из которых как раз и является задача распознавания именованных сущностей (named entity recognition).
По определению NER — это задача поиска и классификации именованных сущностей в неструктурированном тексте. Таких как организации, локации, телефоны, имена людей, даты, денежные суммы и т. д.
Задача довольно хорошо изучена, и для её решения разработано несколько подходов:
Rule-based-системы, построенные на регулярных выражениях, грамматиках и словарях сущностей.
Статистические методы, основанные на выделении признаков сущностей, таких как части речи, морфология, количество цифр, количество прописных букв и т. п.
Supervised-learning-подходы, такие как conditional random fields и нейронные сети.
Rule-based-системы отлично справляются с извлечением определённых типов сущностей — таких как даты, номера телефонов и кредитных карт — довольно быстро работают, но требуют знаний о грамматиках и регулярных выражениях. При этом такие системы чаще ошибаются на более сложных сущностях, например адресах или именах. Этот недостаток можно устранить, используя статистические методы. Они не требуют от пользователей знания регулярных выражений, но для применения потребуется обучающая выборка и умение придумывать признаки. Но и статистические методы не дают наилучшего качества, потому что не учитывают контекст. В свою очередь, supervised-learning-подходы позволяют учитывать контекст, но также требуют обучающей выборки.
Сейчас количество заказов на NER в рамках различных процессов и продуктов всё возрастает, и это увеличивает нагрузку на data scientist’a: необходим инструмент, который бы реализовывал типовые модели с минимальным участием data scientist’ов, разработчиков и конечных пользователей. Для этого и была разработана библиотека NER AutoML.
Выбор модели.
В настоящее время практически во всех задачах natural language processing (NLP) используются языковые модели. Важное свойство языковых моделей — возможность существенного сокращения обучающей выборки (200–300 примеров) за счёт использования fine-tuning’a для дообучения модели при наличии весов предобученной нейронной сети на корпусе соответствующего языка. Архитектуры типа Transformer являются state-of-the-art (SOTA) для многих задач NLP. Например, question answering, text classification, named entity recognition и т. д. Актуальные бенчмарки трансформеров на различных наборах данных для задачи NER можно посмотреть здесь.
Для начала необходимо найти предобученную модель с SOTA-архитектурой, которая удовлетворяет следующим условиям:
модель обучена на корпусе русскоязычных текстов;
модель показывает хорошие результаты на открытых данных;
модель можно адаптировать для решения задачи NER.
NLP-моделей, обученных на корпусах русскоязычных текстов, не так много. Одна из наиболее известных библиотек для NLP, которая предоставляет веса моделей ( BERT , RNN и др.), обученных на русскоязычных корпусах, — это DeepPavlov.
В таблице, взятой с сайта DeepPavlov, приведено сравнение результатов применения этих моделей на корпусе Named_Entities_3 для решения задачи NER.
Для дальнейшей работы мы выбрали предобученную модель BERT, поскольку она показала наилучшее качество применительно к задаче NER на русскоязычном корпусе.
В комплекте с моделью идёт словарь для токенизатора WordPiece . Токенизатор преобразует исходный текст в токены и их идентификаторы, которые подаются на вход модели. Очень важно использовать для fine-tuning именно тот токенизатор и его словарь, которые были использованы для обучения модели. В противном случае на вход модели будут подаваться совершенно новые токены, которые модель никогда не видела, что, по сути, будет обучением с нуля.
Формат входных и выходных данных для модели.
Рассмотрим иллюстрацию fine-tuning для NER из оригинальной статьи про BERT.
На вход модели подаётся последовательность из 512 идентификаторов токенов, на выходе получается последовательность из 512 меток, каждая из которых принимает значение соответствующего класса именованной сущности. Модель хорошо согласуется с форматами разметки BIO , BIOE и BIOES, которые можно использовать для формирования выборки текстов для обучения.
В формате BIO метка первого слова именованной сущности имеет префикс «B-» (beginning), метки последующих слов имеют префикс «I-» (inside), а метки слов, не являющиеся сущностями, обозначаются префиксом «O» (outside). Формат позволяет идентифицировать первый и последний токен сущности в последовательности. Это свойство необходимо в случае, если в тексте встречаются несколько подряд идущих именованных сущностей одного класса. Вот так выглядит пример разметки:
Ограничения модели.
Пересекающиеся и вложенные сущности.
У формата BIO и BERT есть проблема — именованные сущности не должны пересекаться и не должны быть вложенными:
В этой статье предложены два способа устранения проблемы. Если вкратце, первый способ: для всех входных токенов, которым соответствует множество меток, формируется новая мультиметка (путём конкатенации всех меток из множества). Таким образом, если сущности «Иванова» соответствуют две метки «B-PER» и «I-LOC», формируется новая метка «B-PER|I-LOC»:
Второй способ: задача распознавания сводится к задаче seq2seq путём представления токенов в виде входной последовательности, а вложенных сущностей — в виде выходной последовательности. Модель состоит из энкодера, который обрабатывает входные последовательности, и декодера, который прогнозирует метки для каждого токена, пока не будет предсказана специальная метка <eow> (конец слова), после чего декодер переходит к обработке следующего токена.
В рамках своего решения мы выбрали первый способ. Его достоинство — простота реализации, но есть и существенный недостаток: в результате конкатенации меток их количество увеличивается, а частота встречаемости меток в датасете уменьшается. В худшем случае метка может встретиться в датасете один раз, и модель не сумеет её запомнить. Результаты применения этого способа смотрите в разделе «Результаты на открытых данных».
Удобство для пользователя.
Перед тем как обучить модель, пользователь должен сделать разметку своих текстов. Как правило, инструмент для разметки получает на вход исходный текст, а после ручной разметки возвращает список именованных сущностей. Каждый список содержит имена сущностей и соответствующие им позиции начала и окончания в тексте. В частности, так работает инструмент разметки, который используется у нас.
В этом случае, чтобы обучить модель, пользователю кроме разметки нужно сделать дополнительную обработку текста, разбив его на токены и сопоставив им метки именованных сущностей, учитывая их пересечения. Это лишняя работа для пользователя, так что имеет смысл автоматизировать преобразование выходного формата инструмента разметки в формат BIO.
Для удобства пользователя мы выбрали более универсальный формат входных данных. Пользователь предоставляет данные в формате jsonlines, где каждая строка — JSON, содержащий текст и список сущностей, которые содержат позиции начала и окончания сущности, а также имя сущности. Такой формат упрощает и представление пересекающихся именованных сущностей. Вот пример нормализованной строки:
Соответствие токенов и сущностей исходному тексту.
Предположим, что необходимо сделать подсветку именованных сущностей в тексте. Например, для текста с лишними разделителями:
…результат должен выглядеть так:
После препроцессинга и токенизации текст преобразуется в список токенов, а в результате инференса получится список меток:
Таким образом, NER AutoML должен находить соответствие между токенами и их позициями в тексте, чтобы сделать подсветку. В результате формируется список сущностей, аналогичный тому, который подаётся на вход модели при обучении:
4. Длина входной последовательности.
BERT не может обработать тексты произвольной длины, потому что длина входной последовательности ограничена 512 токенами, при этом первый токен всегда должен быть [CLS]. Есть способ устранить этот недостаток — токенизировать исходный текст, и если длина входной последовательности превышает установленный лимит, то она разбивается на подпоследовательности, длины которых не превышают этот лимит.
Обзор возможностей библиотеки transformers от HuggingFace.
Библиотека transformers от сообщества HuggingFace — это одна из самых популярных библиотек для работы с трансформерами. HuggingFace поддерживает репозиторий , где можно найти веса предобученных моделей практически для любого языка, а также словари для токенизаторов. Там же есть и ruBERT от DeepPavlov, который использовался для разработки прототипа AutoML. Дообучить модель можно с помощью pytorch . Каждая модель в репозитории содержит следующие файлы:
config.json — описание архитектуры трансформера,.
pytorch_model.bin — веса модели,.
vocab.txt — словарь для токенизатора.
Загрузить веса модели довольно просто. Импортируем класс для работы с конфигом и моделью для классификации токенов с выходным линейным слоем над эмбеддингами и указываем количество классов для меток:
На этапе обучения модель принимает на вход три тензора:
input_ids — входные последовательности идентификаторов токенов.
attention_mask — соответствующие идентификаторам токенов attention маски. Нуль ставится на те позиции, на которых во входной последовательности находятся [PAD]-токены, дополняющие её до нужной длины. На остальных позициях ставятся единицы.
labels — идентификаторы классов именованных сущностей.
На этапе инференса модель принимает на вход только тензор input_ids.
Пример батча для двух последовательностей токенов длины 11:
Чтобы получить последовательности идентификаторов токенов, нужно загрузить словарь для токенизатора, после чего передать в него исходный текст. Токенизатор возвращает объект encoding, у которого есть свойства tokens и ids, содержащие соответственно последовательность токенов и их идентификаторов.
Кроме того, в объекте encoding есть два замечательных метода char_to_token и token_to_chars.
Первый метод получает на вход позицию символа в тексте, например начальную позицию сущности, и возвращает номер токена в списке encoding.tokens (или encoding.ids). Этот метод позволяет преобразовать формат jsonline в формат BIO.
Второй метод получает на вход номер токена из списка encoding.tokens (или encoding.ids), например номер первого токена, который соответствует именованной сущности, и возвращает начальную и конечную позицию этого токена в исходном тексте. Метод позволяет сопоставить спрогнозированные сущности с их позициями в тексте.
Все вышеперечисленные возможности помогают обойти ограничения модели.
Реализация библиотеки NER AutoML.
Состав компонентов библиотеки приведён на диаграмме.
Каждый компонент удобно реализовать в виде класса. Рассмотрим особенности реализации каждого из классов подробнее.
1. TokenizedDataSet.
Класс можно реализовать, следуя документации pytorch. TokenizedDataSet наследуется от класса Dataset и переопределяет методы __init__, __len__ и __getitem__. Экземпляр этого класса хранит выборку в поле sequence в таком же формате, который подаётся на вход модели. Небольшое напоминание из раздела про обзор библиотеки transformers.
Реализация класса TokenizedDataset находится в модуле ner_automl.preprocessing.
2. Preprocessor.
Определим класс, который будет использоваться для обработки текстов. Экземпляр этого класса хранит токенизатор (tokenizer), словари отображения меток в их идентификаторы и обратно (label_to_id и id_to_label), имена входных тензоров для модели (tensor_names) и максимальную длину последовательности.
Словари отображения меток инициализируются одной парой значений 0: ' [PAD] ' ('[PAD] ':0), которые есть в свойствах токенизатора.
Исходная максимальная длина последовательности равна 511 токенов, а не 512. Это связано с тем, что первый токен последовательности всегда должен быть [CLS].
Реализуем метод, который преобразует текст jsonline в BIO-формат. Метод принимает на вход один аргумент — dictionary содержащий текст и список именованных сущностей, в котором каждый элемент представляет собой dictionary с именем сущности и позициями её начала и окончания в тексте.
На первом шаге применяем токенизатор к тексту, в результате получается объект encoding со списком токенов и списком их идентификаторов. Чтобы поставить в соответствие токенам их метки, инициализируется список label_names такой же длины, как и список токенов. Каждый элемент списка — это множество, которое при инициализации содержит только одну метку «O», а после обработки размеченных текстов будет содержать соответствующие токену метки. Если именованные сущности пересекаются, то одному токену будет соответствовать несколько меток.
Для каждой сущности из списка именованных сущностей её начальная позиция передаётся в метод char_to_token объекта encoding, чтобы определить start_token_pos — индекс первого токена сущности в списке encoding.ids. Та же операция повторяется для конечной позиции сущности в тексте, чтобы определить end_token_pos — индекс последнего токена сущности в списке encoding.ids. Затем для каждой позиции в диапазоне start_token_pos и end_token_pos метод добавляет имя сущности в множество на соответствующей позиции в списке labels_names, удаляя метку «O» из этого множества. Первая метка добавляется с префиксом «B-», а остальные с префиксом «I-».
Если множество в списке labels_names содержит несколько меток (в случае пересечения сущностей), метки объединяются в одну при помощи конкатенации.
На последнем шаге метод добавляет метки в словари отображения label_to_id и id_to_label и формирует список идентификаторов меток labels, соответствующий списку label_names, после чего возвращает объект encoding и список labels.
Следующий метод вызывается для обработки всех текстов, которые находятся в файле по указанному пути. Он возвращает список объектов encoding для каждого текста и соответствующие им последовательности меток.
Перед тем как реализовать метод разбиения последовательности, нужно создать вспомогательный метод для дополнения последовательности специальными токенами. Метод принимает на вход два параметра: последовательность идентификаторов и тип: input_ids, attention_mask или labels. В конец последовательности всегда добавляются идентификаторы [PAD] токена, дополняющие её до максимальный длины. В начало последовательности добавляется один идентификатор. Для типа input_ids добавляется идентификатор [CLS], для attention_mask — 1, для labels — идентификатор метки «O».
Теперь можно создать метод разбиения последовательностей, длина которых превышает установленную максимальную длину max_length.
Метод принимает на вход обязательный параметр — объект encoding, полученный в результате обработки одного текста, и один необязательный параметр labels — список идентификаторов меток именованных сущностей, который есть только на этапе обучения модели.
Если длина последовательности не превышает максимальную длину, то к ней просто добавляются специальные токены, иначе она разбивается на последовательности поменьше.
Рассмотрим случай разбиения последовательности, когда отсутствуют метки, то есть на этапе инференса. Естественный способ разбиения последовательности — по предложениям в тексте, чтобы учитывать контекст. В этом случае необходимо просмотреть все токены последовательности слева направо и найти индекс символа конца предложения, находящийся слева от позиции max_length, но максимально близкий к ней. Назовём такой индекс split_position и запомним его значение. Позицию max_length обозначим как border. Теперь необходимо передвинуть border на новую позицию, которая находится на расстоянии max_length от split_position. Процедура повторяется пока не закончится последовательность.
В качестве разделителей предложений используются символы «.», «!» и «?». Если предложение заканчивается на «…», «!!!» или другие сложные знаки препинания, то разбиение всё равно происходит по первому символу. В таком случае следующее предложение будет начинаться со знаков «..», «!!» и. т. п, но это не сильно влияет на качество распознавания именованных сущностей.
Может получиться, что длина предложения превышает максимальную длину или знаки препинания вовсе отсутствуют в тексте. В этом случае нужно искать альтернативные позиции для разбиения: split_position_alt — это позиция ближайшего к border токена, который не начинается на «##». Это сделано для того, чтобы не разбивать последовательность посередине слова. В случае если split_position не была найдена, используется split_position_alt.
Если метки сущностей передаются в метод в качестве параметра, то в алгоритме появляются дополнительные условия: именованная сущность должна полностью принадлежать подпоследовательности, получившейся в результате разбиения.
Для поиска split_position в алгоритм добавляется проверка метки, соответствующей символу конца предложения. Если метка принимает значение «O», значит, этот токен не принадлежит именованной сущности, а следующая позиция — это кандидат на split_position. В таком случае такие именованные сущности, как, например, «И. И. Иванов», «ул. Иванова», не будут разбиты по точке и отнесены к разным подпоследовательностям.
Для поиска split_position_alt в алгоритм добавляется проверка метки, которая соответствует позиции-кандидату на split_position_alt. Метка должна либо принимать значение «O», либо начинаться на «B».
После разбиения последовательности метод возвращает dictionary со списками подпоследовательностей.
Теперь нужно реализовать метод, который обрабатывает обучающую, валидационную и тестовую выборку. Метод принимает на вход пути до файлов, в которых эти выборки находятся. Далее он делает токенизацию текстов для каждой выборки. Определяется максимальная длина входной последовательности, которая равна длине самой длинной последовательности из обучающей выборки, но не более 511 токенов. Это делается для того, чтобы последовательности не дополнялись большим количеством [PAD]-токенов, например, в случае, если в выборках содержатся очень короткие фразы. При таком подходе можно сформировать батч большего размера. На последнем шаге последовательности из каждой выборки разбиваются и формируется dictionary с ключами train, valid, test и соответствующими им результатами работы метода cut_long_sequence.
На этапе инференса будет удобно иметь в распоряжении два очень простых метода, чтобы инициализировать препроцессор словарями отображения меток и значением максимальной длины последовательности, созданными на этапе обучения.
Реализация класса Preprocessor находится в модуле ner_automl.preprocessing.
3. Trainer.
Класс для обучения модели и формирования отчёта с метриками. Конструктор принимает на вход экземпляр класса Preprocessor, путь до каталога, в который нужно сохранить веса модели, путь до файла конфигурации BERT, путь до файла с весами модели и количество эпох для обучения. По умолчанию указано 10 эпох, поскольку, как показывает практика, BERT дообучается довольно быстро. При создании экземпляра класса Trainer загружаются веса модели и выбирается место, где будет обучаться модель — GPU или CPU.
Для валидации и тестирования удобно сделать метод, который будет применять модель к соответствующим датасетам. На вход он принимает два параметра — генератор батчей и необязательный параметр, в зависимости от которого возвращается либо значение функции потерь, либо оно же с метриками, precision, recall, f1-score для каждой сущности, а также micro и macro average для каждой из метрик. Для вычисления метрик используется библиотека seqeval, предоставляющая удобные методы для валидации моделей, работающих с последовательностями.
Ниже представлен код метода обучения модели. На вход он принимает два генератора батчей, первый из который выбирает батчи из обучающей выборки, а второй — из валидационной. Внутри реализован обычный цикл обучения модели. На каждой эпохе обучения веса модели сохраняются при условии, что loss на валидации уменьшился по сравнению с наилучшим. Кроме того, сохраняются словарь отображения идентификаторов меток в их значениях, максимальная длина последовательности и размер батча, поскольку эта информация требуется на этапе инференса, когда модель загружается. Экземпляр этого класса также хранит внутри себя массивы со значениями loss на обучении и валидации для каждой эпохи.
Класс Trainer реализован в модуле ner_automl.trainer.
4. Predictor.
Для инференса модели потребуется класс, который реализует загрузку весов модели и преобразование спрогнозированной последовательности меток из формата BIO в формат jsonline.
Конструктор принимает на вход экземпляр класса Preprocessor, путь до весов дообученной модели и путь до файла конфигурации, после чего загружает веса модели и сопутствующую информацию, сохранённую на этапе обучения. Поскольку экземпляр класса «препроцессор» не содержит информацию об обучающей выборке, в него загружается словарь отображения идентификаторов именованных сущностей в их метки, а также максимальная длина последовательности и размер батча. Далее модель загружается на GPU/CPU.
Единственный метод этого класса принимает на вход текст и возвращает список именованных сущностей, где каждая сущность — это dictionary с именем сущности и позициями её начала и окончания.
На первом шаге препроцессор токенизирует текст и возвращает encoding с последовательностью токенов.
Затем препроцессор разбивает полученную последовательность на подпоследовательности, не превышающие максимальную длину. Для удобства восприятия картинок идентификаторы токенов и меток заменили на токены и метки соответственно.
Если тексты очень длинные, список подпоследовательностей для них может не поместиться в видеопамяти, так что лучше разбить список подпоследовательностей на батчи. Поскольку на этапе инференса есть только один тип последовательностей — input_ids, генератор батчей создаётся за счёт стандартной связки TensorDataset и DataLoader из pytorch.
Для каждого батча модель делает инференс, возвращая тензор размерности (размер батча x количество токенов x количество меток). Каждый элемент тензора — это вероятность того, что токену в последовательности соответствует метка с некоторым названием (B-PER, I-LOC и т. д.). Выбирается самый вероятный класс для каждого токена, то есть получается тензор, состоящий из подпоследовательностей, элементами которых являются идентификаторы меток именованных сущностей.
Далее из каждой подпоследовательности исключаются идентификаторы, находящихся на тех позициях, которые соответствуют позициям [CLS]- и [PAD]-токенов во входных подпоследовательностях. После этого очищенные подпоследовательности объединяются. В результате получается последовательность идентификаторов меток labels такой же длины, как и исходная последовательность токенов в объекте encoding.
Мы помним, что модель может возвращать мультиметку, если сущности пересекаются. Поэтому для каждого входного токена определяется список меток — путём сопоставления ему идентификатора метки из последовательности labels и преобразования этого идентификатора в название метки с помощью словаря отображения препроцессора. Далее метка разбивается на список меток token_labels разбиением по символу «|».
Если метка из списка token_labels начинается с «B», то есть является первой меткой сущности, тогда по номеру соответствующего ей входного токена с помощью функции token_to_chars объекта encoding определяется начальная и конечная позиция токена, с которого начинается именованная сущность, а также создаётся новый элемент в списке именованных сущностей entities, состоящий из названия именованной сущности (label), её начальной (start) и конечной (end) позиции и номера первого входного токена (token_num), которому она соответствует. Пока конечная позиция представляет собой место в тексте, где заканчивается первый токен именованной сущности, поэтому её необходимо обновить.
Для каждой сущности из списка entities просматриваются токены, начиная от позиции, следующей за token_num, до конца последовательности. Аналогичным образом для каждого токена создаётся список соответствующих ему меток token_labels. Если название сущности есть в списке token_labels, то с помощью функции token_to_chars вычисляется конечная позиция данного токена и обновляется конечная позиция сущности. Просмотр токенов заканчивается, как только название сущности не будет обнаружено в списке token_labels.
Класс Predictor реализован в модуле ner_automl.predictor.
5. BatchSizeSelector.
Тут опишем только основную идею определения выбора размера батча.
В основе автоматического определения размера батча лежит метод проб и ошибок. Идея заключается в том, чтобы последовательно генерировать батчи, размер которых увеличивается с каждой итерацией:
На каждой итерации необходимо загрузить модель и батч в GPU, а потом применить модель к батчу, при этом после каждой итерации необходимо чистить видеопамять вызовом метода torch.cuda.empty_cache(). На некоторой итерации с номером k+1 GPU выбросит исключение «CUDA out of memory». Тогда в качества размера батча выбирается 2^k.
В случае работы с CPU всё несколько сложнее и пока задача автоматического определения батча не решена. Если используется CPU, то рекомендуется указывать размер батча вручную.
Класс BatchSizeSelector реализован в модуле ner_automl.batch_size_selector.
6. AutoNER.
Класс с двумя статическими методами реализует интерфейс для работы с AutoML.
Первый метод — fit, который выполняет препроцессинг выборок, обучает модель и формирует отчёт с метриками. На вход методу передаются следующие параметры:
checkpoint_path — путь до каталога, в который сохраняется модель;
report_path — путь до каталога, в который сохраняется отчёт;
bert_vocab_path — путь до словаря токенизатора;
bert_config_path — путь до файла конфигурации BERT;
bert_weights_path — путь до предобученных весов BERT;
train_path — путь до обучающей выборки;.
valid_path — путь до валидационной выборки;
test_path — путь до тестовой выборки;
batch_size — размер батча. Либо «auto», либо числовое значение;
epochs — количество эпох обучения. По умолчанию 10.
Метод загружает токенизатор и инициализирует препроцессор, который затем возвращает dictionary sequences с ключами train, valid, test. Значениями будут dictionary с входными тензорами для модели.
Опционально определяется размер батча.
Для генерации батчей используется DataLoader в связке с TokenizedDataSet. Для каждой выборки создаётся train_dataloader, valid_dataloader и test_dataloader.
Далее модель дообучается с помощью экземпляра класса Trainer, и рассчитываются метрики качества на всех трёх выборках.
Затем формируется отчёт с метриками качества в формате JSON.
Второй метод from_pretrained создаёт экземпляр класса predictor и возвращает его. На вход метод принимает checkpoint_path, bert_vocab_path и bert_config_path.
Класс AutoNER реализован в модуле ner_automl.auto.
Работа с библиотекой.
1. Дообучение модели.
На этапе обучения необходимо выполнить следующие шаги:
Импортировать класс AutoNER из модуля ner_automl.auto:
2. Задать следующие пути:
путь до словаря токенизатора.
путь до конфигурационного файла BERT.
путь до предобученных весов BERT.
путь до файла, в который будут сохраняться веса модели.
путь до файла, в который будут сохраняться метрики в формате JSON.
пути до обучающей, валидационной и тестовой выборки.
3. Вызвать статический метод fit класса AutoNER для обучения модели:
2. Инференс.
На этапе инференса нам необходимо выполнить следующие шаги:
Импортировать класс AutoNER из модуля ner_automl.auto:
2. Задать следующие пути:
путь до словаря токенизатора.
путь до конфигурационного файла BERT.
путь до файла, в который были сохранены веса модели на этапе обучения.
3. Вызвать статический метод from_pretrained класса AutoNER, возвращающий экземпляр класса Predictor:
4. Задать текст:
5. Сделать прогноз именованных сущностей:
Результаты на открытых данных.
Мы тестировали библиотеку на датасетах Named_Entities_3 , Named_Entities_5 и factRuEval . Во всех датасетах есть длинные тексты, но пересечение именованных сущностей встречается только в датасете factRuEval.
Метрики качества для датасетов Named_Entities_3, Named_Entities_5 приведены в таблицах:
В целом модель обучилась достаточно хорошо. Ниже приведён пример подсветки длинного текста, взятого с сайта , выданного гуглом по запросу «очень длинная статья». Даже при наличии лишних разделителей между словами именованные сущности корректно подсвечиваются.
На датасете factRuEval модель работает хуже. Во-первых, из-за того, что он небольшой, во-вторых, из-за пересекающихся сущностей. Результаты для непересекающихся сущностей приведены в таблице.
С пересечениями всё ещё хуже. Почти везде значения метрик упали до 0 в связи с низкой частотой встречаемости меток. Отсюда следует, что алгоритм работы с пересекающимися сущностями нужно изменить, а текущую реализацию применять только для датасетов, в которых сущности не пересекаются.
Пути улучшения библиотеки.
Что ж, нужно признать, что библиотека пока не идеальна и требует доработок. Но её уже можно использовать для автоматизации решения задачи NER. А доработки можно начать с рефакторинга кода.
Первая проблема, которую нужно решить, — это работа с пересекающимися и иерархическими именованными сущностями. Возможно, потребуется использование других моделей, которые лучше справляются с данной задачей.
Вторая немаловажная проблема, которая стала возникать чаще, — это отсутствие у пользователя выборок большого объёма. Обычно пользователь располагает выборкой в 10–15 примеров. Поэтому необходимо решать задачу Few-shot NER. Пока мы проводим эксперименты на основе этих статей: раз , два.
Третья проблема — это разбиение последовательностей на этапе инференса. Алгоритм может разбить последовательность таким образом, что часть именованной сущности попадёт в разные подпоследовательности. В таком случае модель вернёт две именованные сущности, идущие друг за другом. Один из путей решения этой проблемы — делать скользящее окно, которое передвигается по всей последовательности, а инференс осуществлять в рамках скользящего окна. В результате выбирать тот вариант инференса, в котором нет одинаковых подряд идущих именованных сущностей, а только одна.
Ещё одним выходом из ситуации может быть ∞-former . Авторы решили проблему длинных последовательностей.
Кроме того, есть планы добавить обучение других моделей, чтобы потом можно было выбрать наилучшую по некоторому критерию качества.
Если у вас есть какие-то вопросы или мысли по теме — пишите в комментариях. Может быть, у вас тоже был опыт работы с подобной задачей и вам есть что сказать.