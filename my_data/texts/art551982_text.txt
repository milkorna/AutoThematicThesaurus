Недавно Сбер в статье Всё, что нам нужно — это генерация предложил необычный подход для отсеивания некачественных текстов (технического мусора и шаблонного спама).
Мы дополнили такой подход ещё одной эвристикой: сделали сжатие текстов с помощью zlib и отбросили самые сильно и слабо сжимающиеся, а затем уже  применили классификацию. Эмпирически подобранный диапазон сжатия для нормального текста ×1.2—×8 (меньше 1.2 — случайные символы и технический мусор, больше 8 — шаблонный спам).
Подход безусловно интересный и стоит взять его на вооружение. Но разве коэффициент сжатия zlib на качественных текстах не имеет нелинейной зависимости от длины сжимаемого текста? Давайте проверим.
Возьмем текстовый корпус, состоящий из предложений, длина которых варьируется в диапазоне от 50 до 280 символов:
Посмотрим, как длина качественных предложений влияет на коэффициент сжатия.
Для этого:
1. Возьмем диапазон длин предложений с наибольшей частотой (25 - 75 перцентиль). В нашем случае это длины от 92 до 175 символов:
2. Разобьем эти предложения на примерно одинаковые по длине группы. Максимальное отклонение длин предложений в одной группе будет равняться отклонению между 25 и (25 + w) перцентилем или 75 и (75 - w) перцентилем (выбираем меньшее из двух дельт), где w - окно (принимаем 2.5 перцентиля).
считаем максимальное отклонение длин предложений в одной группе:
Максимальное отклонение длин предложений в одной группе на нашем примере составляет 3 символа.
разбиваем предложения на группы по длине +- 3 символа:
Получаем 20 групп.
3. Предполагаем, что пятидесятому перцентилю каждой группы соответствует качественное предложение и посмотрим на график зависимости коэффициента сжатия этих предложений от их длины:
наблюдаем степенную функцию вида:
где x - длина предложения, y - нормальный коэффициент сжатия для качественного предложения.
Аппроксимируем функцию с помощью МНК.
a = 0.17601951773514363, b = 0.3256903074228561, коэффициент корреляции: 0.9999489378452683.
для нашего корпуса текстов получаем зависимость:
Изобразим графически разницу между полученной степенной функцией (на всём диапазоне длин 50 - 280 символов) и постоянным коэффициентом сжатия, если бы он не зависел от длин. Для этого вводим переменную "c" для функции "y = с" (на пятидесятом перцентиле), которую принимал бы нормальный коэффициент сжатия, если бы он не зависел от длины:
Разница более чем существенная. Коэффициент сжатия для предложений короче ~130 символов будет заниженным, а для предложений длиннее ~130 символов - наоборот завышенным. И это видно невооруженным взглядом на практике. Если отсеивать предложения разной длины по коэффициенту сжатия без корректировки, то отсеются преимущественно более длинные предложения.
Таким образом, предложения разных длин некорректно отсеивать по одному распределению коэффициента сжатия. И чем больше разброс длин предложений в корпусе, тем более некорректный результат мы получим.
Делаем поправку коэффициента сжатия всех предложений в зависимости от их длины.
Напоследок посмотрим на примере, какие предложения отсеиваются после корректировки и не отсеялись бы без неё:
в нашем случае предложения уже очищены от технического мусора, поэтому в качестве примера отсеиваем только заспамленные предложения:
Как видим, это короткие предложения, для которых был занижен коэффициент сжатия. На практике довольно редко в корпусе встречаются предложения одинаковой длины. Как правило, разброс длин довольно существенный.
Полный код ноутбука выложен на Гитхабе.
Возможно, для кого-то будет полезно. Я для себя взял данный подход на вооружение, как относительно простой и эффективный способ избавиться от технического мусора и шаблонного спама.