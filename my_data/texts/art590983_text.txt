Иногда возникает необходимость провести анализ большого количества текстовых данных, не имея представления о содержании текстов. В таком случае можно попытаться разбить тексты на кластеры, и сгенерировать описание каждого кластера. Таким образом можно в первом приближении сделать выводы о содержании текстов.
Тестовые данные.
В качестве тестовых данных был взят фрагмент новостного датасета от РИА, из которого в обработке участвовали только заголовки новостей.
Получение эмбеддингов.
Для векторизации текста использовалась модель LaBSE от @cointegrated . Модель доступна на huggingface.
Кластеризация.
В качестве алгоритма для кластеризации был выбран алгоритм k-means. Выбран он для наглядности, часто приходится поиграться с данными и алгоритмами для получения адекватных кластеров.
Для нахождения оптимального количества кластеров будем использовать функцию, реализующую "правило локтя":
Выделение информации о полученных кластерах.
После кластеризации текстов берем для каждого кластера по несколько текстов, расположенных максимально близко от центра кластера.
Саммаризация центральных текстов.
Полученные центральные тексты можно попробовать слепить в общее описание кластера с помощью модели для саммаризации текста. Я использовал для этого модель ruT5 за авторством @cointegrated . Модель доступна на huggingface.
Заключение.
Представленный подход работает не на всех доменах - новости тут приятное исключение, и тексты такого типа разделяются достаточно хорошо и обычными методами. А вот с условным твиттером придется повозиться - обилие грамматических ошибок, жаргона, и отсутствие пунктуации могут стать кошмаром для любого аналитика. Замечания исправления и дополнения приветствуются!
Ссылки.
С ноутбуком можно поиграться в колабе, ссылка в репозитории на гитхаб.