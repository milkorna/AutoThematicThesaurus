Краткое описание метода машинного обучения, который представляет, как выглядит текстовое предложение, чтобы локализовать и обосновать его семантику в контексте реального мира, улучшая перевод подобно тому, как это делают люди.
В детстве мы подражаем звукам речи, чтобы выучить язык. Мы не начинаем с чтения текстов, которые требуют фундаментальных познаний, понимания окружающего мира и продвинутой способности интерпретировать, делать выводы и выстраивать взаимоотношения. Вместо этого человек начинает своё языковое путешествие не спеша, показывая на предметы пальцем и взаимодействуя с окружением, и "приземляя" слова в реальности, выводя их значения из контекста физического и социального мира. В конце концов, мы обучаемся построению законченных предложений для передачи сложных идей.
Подобным образом человек, при изучении иностранного языка или переводе на него, подключаются и другие источники сенсорной информации, например картинки и звуки, связанные с новыми или незнакомыми словами. Карточки с изображениями улучшают понимание и запоминаемость. Так, наработав определённый опыт, человек может точно переводить новые предложения в контексте даже при отсутствии других источников информации. Но изображение, основанное на исходном тексте, конечно, значительно облегчает задачу.
Эта идея и легла в основу новой модели машинного обучения под названием VALHALLA от исследователей из MIT, IBM и UCSD (Калифорнийского университета в Сан-Диего). В этой модели нейросеть читает предложение на исходном языке, затем "галлюцинирует" картинку на его основе, а затем использует и текст, и картинку для создания перевода. Команда разработчиков установила, что такой подход позволяет увеличить точность машинного перевода по сравнению с обычным текстовым подходом. Более того, улучшения стали особенно заметны в случаях с длинными предложениями, с мало проработанными языками, и в ситуациях, когда часть предложения не может быть получена машинным переводом.
Машинный перевод является основной задачей искуственного интеллекта в обработке естественных языков (NLP) и представляет собой "исключительно практичную технологию, которой ежедневно пользуются миллионы людей", - говорит соавтор исследования Юн Ким, доцент кафедры электротехники и компьютерных наук Массачусетского технологического института, работающий в Лаборатории компьютерных наук и искусственного интеллекта (CSAIL) и Лаборатории ИИ MIT-IBM Watson. Учитывая последние достижениями в области глубокого обучения "появилось интересное развитие в том, как можно использовать нетекстовую информацию - например, изображения, аудио или что-то ещё - для решения практических задач, связанных с языком", - говорит Ким, поскольку "когда люди сами выполняют задачи по обработке языка, они делают здесь, в реальном мире". Сочетание "галлюцинаторных" изображений и текста во время умозаключений, по предположению команды, имитирует этот процесс, обеспечивая движку контекст для улучшения производительности по сравнению с прочими методами, которые используют только текстовые данные.
Это исследование будет представлено на конференции IEEE / CVF по компьютерному зрению и распознаванию образов в июне 2022 г. Соавторами Кима являются аспирант UCSD по имени И Ли и профессор Нуно Васконселос, а также научные сотрудники Рамсвар Панда, Чун-фу Чен, Роджерио Ферис и директор IBM Дэвид Кокс из IBM Research и MIT-IBM Watson AI Lab.
Обучение галлюцинациям на основе изображений.
Когда мы изучаем новые языки и переводим, нам часто дают примеры и практику, прежде чем мы начнём работать самостоятельно. То же самое справедливо и для систем машинного перевода; однако, если при обучении используются изображения, эти методы ИИ также требуют визуальных средств для тестирования, что ограничивает их применимость, объясняет Рамсвар Панда.
"В реальных сценариях у вас может не быть картинки, соотносящейся с исходным предложением. Итак, наша мотивация заключалась в следующем: вместо того, чтобы использовать внешнее изображение во время вывода в качестве входных данных, можем ли мы использовать визуальную галлюцинацию — способность представлять визуальные сцены — для улучшения систем машинного перевода?" — объясняет он.
Для этого команда использовала архитектуру кодера-декодера с двумя преобразователями - это тип модели нейронной сети, которая подходит для данных, зависящих от последовательности, например, для естественного языка, и который может обращать внимание на ключевые слова и семантику предложения. Один преобразователь генерирует визуальную галлюцинацию, а другой выполняет мультимодальный перевод, используя выходы первого преобразователя.
Во время обучения есть два потока перевода: исходное предложение и репрезентативное изображение, которое с ним связано, и то же самое исходное предложение, которое визуально галлюцинируется для создания пары текст-изображение. Сначала репрезентативное изображение и предложение токенизируются в представления, которые могут быть обработаны преобразователями; в случае предложения каждое слово является токеном. Исходное предложение снова токенизируется, но на этот раз проходит через преобразователь зрительных галлюцинаций, в результате чего получается галлюцинация - дискретное образное представление предложения. Исследователи включили авторегрессию, которая сравнивает истинные и галлюцинаторные представления на предмет соответствия - например, омонимы: упоминание животного "bat" не галлюцинируется как бейсбольная бита. Затем преобразователь галлюцинаций использует разницу между ними, чтобы оптимизировать свои прогнозы и визуальный вывод, обеспечивая согласованность контекста.
Затем два набора токенов одновременно пропускаются через мультимодальный преобразователь перевода, каждый из которых содержит представление предложения и либо галлюцинацию, либо репрезентативное изображение. Результаты перевода токенизированного текста сравниваются на предмет сходства друг с другом и с целевым предложением на другом языке. Любые различия затем передаются обратно в преобразователь перевода для дальнейшей оптимизации.
Для тестирования поток репрезентативных изображений уменьшается, поскольку в повседневных сценариях они, скорее всего, будут недоступны.
"Нам неизвестны другие работы, в которых бы преобразователь галлюцинаций использовался совместно с системой мультимодального перевода для повышения эффективности машинного перевода", — говорит Панда.
Визуализация целевого текста.
Чтобы проверить свой метод, команда сравнила VALHALLA с другими современными методами мультимодального перевода и перевода только текста. Они использовали публичные эталонные базы данных, содержащие достоверные изображения с исходными предложениями, и базу данных для перевода только текстовых новостных статей. Исследователи измерили качество при выполнении 13 задач, начиная от перевода на хорошо проработанные языки (например, английский, немецкий и французский), языки с недостаточным количеством ресурсов (например, с английского на румынский) и на парах без английского (например, с испанского на французский). Они также проверили различные размеры моделей преобразователей, то, как точность меняется в зависимости от длины предложения, и перевод в ограниченном текстовом контексте, когда части текста были скрыты от движков перевода.
Исследователи заметили значительные улучшения по сравнению с методами перевода только текста, повышение эффективности данных и то, что меньшие модели работают лучше, чем большая базовая модель. По мере увеличения длины предложений качество перевода VALHALLA по сравнению с другими методами росло, что исследователи объясняют добавлением более неоднозначных слов. В случаях, когда часть предложения была замаскирована, VALHALLA могла восстановить и перевести оригинальный текст, что показалось команде удивительным.
Были и другие неожиданные находки: "Там, где было не так много обучающих пар [изображений и] текстов, [например, для языков с ограниченными ресурсами], улучшения были более значительными, что указывает на то, что опора на изображения помогает в режимах с низким объемом данных", — говорит Ким. "Еще одна вещь, которая меня удивила, - это улучшенная производительность даже на тех типах текста, которые не всегда очевидно коррелируют с изображениями. Например, не удивительно, что это поможет в переводе визуально значимых предложений, таких как «перед домом стоит красная машина». [Однако], даже в доменах, содержащих только текст [новостные статьи], этот подход смог улучшить работу систем, использующих только текст".
Хотя VALHALLA показывает хорошие результаты, исследователи отмечают, что у метода есть ограничения: он требует, чтобы пары предложений были аннотированы изображением, что может сделать его получение более дорогим. Он также лучше работает в своей основной области, а не в чисто текстовых новостных статьях. Более того, отмечают Ким и Панда, такая техника, как VALHALLA, все еще является "черным ящиком", предполагая, что галлюцинаторные изображения предоставляют полезную информацию, и команда планирует исследовать, чему и как учится модель, чтобы подтвердить свои методы.
В будущем команда планирует изучить и другие способы улучшения перевода. "Здесь мы фокусируемся только на изображениях, но существуют и другие виды мультимодальной информации — например, речь, видео, тактильная отдача или другие сенсорные модальности", — говорит Панда. "Мы считаем, что такое мультимодальное добавление контекста может привести к созданию еще более эффективных моделей машинного перевода, что потенциально может принести пользу переводу на многих языках с низким уровнем ресурсов, распространённых в мире".
Это исследование было частично поддержано Лабораторией искусственного интеллекта MIT-IBM Watson AI и Национальным научным фондом.
