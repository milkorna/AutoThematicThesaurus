5 - Convolutional Sequence to Sequence Learning.
В этом разделе мы будем реализовывать модель из статьи Convolutional Sequence to Sequence Learning.
Введение.
Эта модель кардинально отличается от предыдущих моделей, реализованных нами. Здесь вообще не используются рекуррентные компоненты. Вместо этого применяются свёрточные слои CNN, обычно используемые для обработки изображений. В качестве введения в особенности использования свёрточных слоёв для обработки текста см. руководство здесь.
Если коротко, то свёрточный слой использует фильтры . Эти фильтры имеют ширину (а также высоту в изображениях, но обычно не для текстов). Если фильтр имеет ширину 3, он может видеть 3 последовательных токена. Каждый свёрточный слой имеет множество таких фильтров 1024вэтомразделе. Каждый фильтр будет скользить по последовательности от начала до конца, просматривая все 3 последовательных токена одновременно. Идея состоит в том, что каждый из этих 1024 фильтров научится извлекать из текста разные признаки. Результат этого извлечения признаков будет затем использоваться моделью — потенциально в качестве входных данных для другого свёрточного слоя. Далее всё это можно использовать для извлечения особенностей из исходного предложения для перевода его на целевой язык.
Как и ранее, если визуальный формат поста вас не удовлетворяет, то ниже ссылки на английскую и русскую версию jupyter notebook:
Исходная версия Open jupyter notebook In Colab.
Русская версия Open jupyter notebook In Colab.
Замечание : русская версия jupyter notebook отличается от исходной добавленным в конце тестом на инверсию предложения.
Подготовка данных.
Во-первых, давайте импортируем все необходимые модули и зададим случайные начальные числа для воспроизводимости.
Для загрузки в Google Colab используем следующие команды (После загрузки обязательно перезапустите colab runtime! Наибыстрейший способ через короткую комаду： Ctrl + M + . ):
Затем мы загрузим модели spaCy и определим токенизаторы для исходного и целевого языков.
Далее мы настроим Field которые решают, как будут обрабатываться данные. По умолчанию модели RNN в PyTorch требуют, чтобы последовательность была тензором формы sequencelength,batchsize поэтому TorchText по умолчанию возвращает пакеты тензоров той же формы. Однако в этой части мы используем CNN, которая ожидают, что размер пакета будет первым. Мы говорим TorchText, что партии должны быть batchsize,sequencelength установив batch_first = True.
Мы также добавляем токены начала и конца последовательности, и переводим весь текст в нижний регистр.
Затем мы загружаем наш набор данных.
Мы, как и раньше, пополняем наш словарь, конвертируя любые токены, которые встречаются менее двух раз, в <unk> токены.
Последний этап подготовки данных — определение устройства, а затем создание итератора.
Построение модели.
Следующим шагом будет построение модели. Как и прежде, модель состоит из кодировщика и декодера. Кодировщик кодирует вводное предложение, на исходном языке, в вектор контекста . Декодер декодирует вектор контекста для создания выходного предложения на целевом языке.
Кодировщик.
Предыдущие модели в этих уроках имели кодировщик, который сжимает всё входное предложение в один вектор контекста . Свёрточная модель sequence-to-sequence немного отличается — она получает два вектора контекста для каждого токена во входном предложении. Итак, если бы в нашем входном предложении было 6 токенов, мы получили бы 12 контекстных векторов, по два на каждый токен.
Два вектора контекста на токен - это преобразованный вектор и комбинированный вектор. Преобразованный вектор - это результат прохождения каждого токена через несколько слоев, работу которых мы вскоре объясним. Комбинированный вектор получается в результате суммирования вектора, прошедшего свёрточные слои, и эмбеддинга этого токена. Оба они возвращаются кодером для использования декодером.
На изображении ниже показан результат ввода предложения - zwei menschen fechten. - которое проходит через кодировщик.
Сначала токен проходит через слой эмбеддинга токенов - что является стандартом для нейронных сетей обработки естественного языка. Однако, поскольку в этой модели нет рекуррентой части, то она не имеет представления о порядке токенов в последовательности. Чтобы исправить это, у нас есть второй слой эмбеддинга, позиционный слой эмбеддинга . Это стандартный уровень эмбеддинга, где входом является не сам токен, а позиция токена в последовательности - начиная с первого токена, токена <sos> началопоследовательности, в позиции 0.
Затем токен и позиционные эмбеддинги поэлементно суммируются, чтобы получить вектор, который содержит информацию о токене, а также его позицию в последовательности, которую мы будем называть вектор эмбеддинга . Затем следует линейный слой, который преобразует вектор эмбеддинга в вектор с требуемым размером скрытого слоя.
Следующий шаг - передача этого скрытого вектора в свёрточных блоков . Именно здесь в этой модели происходит вся «магия», и вскоре мы подробно рассмотрим содержимое свёрточных блоков. После прохождения свёрточных блоков вектор затем проходит через другой линейный слой, чтобы преобразовать его обратно из размера скрытого слоя к размеру выхода эмбеддинга. Это наш преобразованный вектор - по одному для каждого токена во входной последовательности.
Наконец, преобразованный вектор поэлементно суммируется с вектором эмбеддинга через остаточное соединение, чтобы получить комбинированный вектор для каждого токена. Опять же, имеется комбинированный вектор для каждого токена во входной последовательности.
Свёрточные блоки.
Итак, как работают эти свёрточные блоки? На изображении ниже показаны 2 свёрточных блока с одним фильтром синий, который скользит по токенам в последовательности. В реальной реализации у нас будет 10 свёрточных блоков с 1024 фильтрами в каждом блоке.
Сначала вводимое предложение необходимо дополнить. Это связано с тем, что свёрточные слои уменьшают длину входного предложения, и мы хотим, чтобы длина предложения, входящего в свёрточные блоки, равнялась его длине по выходу из свёрточных блоков. Без заполнения длина последовательности, выходящей из свёрточного слоя, будет filter_size - 1 элементов короче, чем последовательность, входящая в свёрточный слой. Например, если бы у нас был размер фильтра 3, последовательность будет на 2 элемента короче. Таким образом, мы дополняем предложение одним элементом заполнения с каждой стороны. Мы можем рассчитать количество отступов с каждой стороны, просто выполнив (filter_size - 1)/2 для фильтров нечетного размера - в этой части мы не будем рассматривать фильтры четного размера.
Эти фильтры разработаны таким образом, чтобы их скрытый размер вывода был вдвое больше, чем скрытый размер ввода. В терминологии компьютерного зрения эти скрытые измерения называются каналами - но мы будем называть их скрытыми измерениями. Почему мы удваиваем размер скрытого измерения, покидая свёрточный фильтр? Это потому, что мы используем специальную функцию активации, которая называется закрытые линейные блоки gatedlinearunits−GLU. У GLU есть затворные механизмы похожиенаLSTMиGRU, содержащиеся в функции активации и, фактически, вдвое меньшие размера скрытого измерения, тогда как обычно функции активации сохраняют скрытые измерения того же размера.
После прохождения активации GLU размер скрытого измерения для каждого токена такой же, как и при входе в свёрточные блоки. Теперь он поэлементно суммируется со своим собственным вектором, прежде чем он будет пропущен через следующий свёрточный слой.
На этом завершается одиночный свёрточный блок. Последующие блоки берут вывод предыдущего блока и выполняют те же шаги. У каждого блока есть свои параметры, они не разделяются между блоками. Вывод последнего блока возвращается в основной кодировщик — где он проходит через линейный слой для получения преобразованного вывода, а затем поэлементно суммируется с токеном эмбеддига для получения комбинированного вывода.
Реализация кодировщика.
Чтобы упростить реализацию, мы допускаем только ядра нечётного размера. Это позволяет добавлять одинаковые отступы к обеим сторонам исходной последовательности.
Переменная величина scale используется авторами, чтобы «гарантировать, что дисперсия во всей сети не изменится кардинально». Производительность модели может сильно различаться, если не использовать данное ухищрение.
Позиционный эмдеддинг инициализируется так, чтобы иметь "словарь" 100. Это означает, что он может обрабатывать последовательности длиной до 100 элементов с индексами от 0 до 99. Его можно увеличить, если использовать в наборе данные с большей длиной последовательностей.
Декодер.
Декодер принимает фактическое целевое предложение и пытается его предсказать. Эта модель отличается от моделей рекуррентных нейронных сетей, описанных ранее, тем, что она прогнозирует все токены в целевом предложении параллельно. Нет последовательной обработки, то есть цикла декодирования. Это будет подробно описано позже.
Декодер похож на кодировщик, с некоторыми изменениями как в основной модели, так и в свёрточных блоках внутри модели.
Во-первых, эмбеддинги не имеют остаточной связи, которая соединяется после свёрточных блоков и преобразования. Вместо этого эмбеддинги подаются в свёрточные блоки, которые будут использоваться там в качестве остаточных соединений.
Во-вторых, для подачи информации декодеру от кодера используются преобразованные и комбинированные выходные данные кодера — опять же, в свёрточных блоках.
Наконец, выход декодера представляет собой линейный слой от размера эмбеддинга до размера вывода. Это используется для предсказания того, каким должно быть следующее слово в переводе.
Свёрточные блоки декодера.
Они похожи на свёрточные блоки в кодировщике с некоторыми изменениями.
Во-первых, дополнения. Вместо того, чтобы заполнять одинаково с каждой стороны, чтобы длина предложения оставалась неизменной на всем протяжении, мы выполняем заполнение только в начале предложения. Поскольку мы обрабатываем все целевые токены одновременно параллельно, а не последовательно, нам нужен метод, позволяющий фильтрам, переводящим токен , смотреть только на токены перед словом . Если бы им было разрешено смотреть на -ый токен токен,которыйонидолжнывыводить, то модель просто научилась бы предоставлять следующее слово в последовательности напрямую, копируя его, фактически не обучаясь переводить.
Давайте посмотрим, что произойдет, если мы неправильно сделали добавления с каждой стороны, как в кодировщике.
Фильтр в первой позиции, который пытается использовать первое слово в последовательности <sos> , чтобы предсказать второе слово, two , теперь может напрямую видеть слово two . Это слово для каждой позиции, которое модель пытается предсказать, является вторым элементом, охваченным фильтром. Таким образом, фильтры могут научиться просто копировать второе слово в каждой позиции, обеспечивая идеальный перевод, фактически не изучая, как переводить.
Во-вторых, после активации GLU и до остаточного соединения блок вычисляет и применяет внимание - используя закодированные представления и эмбеддинг текущего слова. Замечание : мы показываем соединения только с крайним правым токеном, но на самом деле они охватывают все токены примерприведёнтолькодляясностипроблемы. Каждый входной токен использует свои собственные и только свои эмбеддинги для расчёта собственного внимания.
Внимание рассчитывается путем использования сначала линейного слоя, для изменения скрытого размера на тот же размер, что и размер эмбеддинга. Затем эмбеддинг суммируется с помощью остаточного соединения. Затем к этой комбинации применяется стандартный расчет внимания путем нахождения того, насколько оно «совпадает» с закодированным преобразованным вектором , а затем это применяется путем получения взвешенной суммы по закодированному объединенному вектору . Затем эта спроецированная копия до размера скрытого измерения и применяется остаточная связь с начальным входом к слою внимания.
Почему они сначала вычисляют внимание с закодированным преобразованным вектором, а затем используют его для вычисления взвешенной суммы по закодированному комбинированному векторы? В статье утверждается, что закодированный преобразованный код хорош для получения более широкого контекста по сравнению с закодированной последовательностью, тогда как закодированный комбинированный вектор содержит больше информации о конкретном токене и, следовательно, более полезен для создания прогноза.
Реализация декодера.
Поскольку мы дополняем только одну сторону, декодеру разрешено использовать заполнение как нечетного, так и четного размера. Опять же, scale используется для уменьшения дисперсии по всей модели, а встраивание позиции инициализируется, чтобы иметь «словарь» равный 100.
Эта модель принимает представления кодировщика в свой метод forward , результат и аргумент - оба передаются в метод calculate_attention , который рассчитывает и применяет механизм внимания. Он также возвращает фактические значения внимания, но в настоящее время мы их не используем.
Seq2Seq.
Инкапсулирующий модуль Seq2Seq сильно отличается от методов рекуррентной нейронной сети, используемых в предыдущих разделах, особенно при декодировании.
У нашего trg элемент <eos> отрезан от конца последовательности потому, что мы не вводим токен <eos> в декодер.
Кодирование аналогично предыдущим разделам, вставляем исходную последовательность и получаем «вектор контекста». Однако здесь у нас есть два вектора контекста на одно слово в исходной последовательности, encoder_conved и encoder_combined.
Поскольку декодирование выполняется параллельно, нам не нужен цикл декодирования. Вся целевая последовательность сразу вводится в декодер, а заполнение используется для обеспечения того, чтобы каждый свёрточный фильтр в декодере мог видеть только текущий и предыдущий токены в последовательности, когда он скользит по предложению.
Однако это также означает, что мы не можем применять обучение с принуждением, используя эту модель. У нас нет цикла, в котором мы можем выбрать, вводить ли предсказанный токен или фактический токен в последовательности, поскольку все предсказывается параллельно.
Обучение модели Seq2Seq.
Остальная часть раздела аналогична всем предыдущим. Мы определяем все гиперпараметры, инициализируем кодер и декодер, а также инициализируем общую модель - размещая ее на графическом процессоре, если он у нас есть.
В статье авторы считают, что более выгодно использовать небольшой фильтр размерядра3 и большое количество слоев 5+.
Мы также можем видеть, что модель имеет почти вдвое больше параметров, чем модель, основанная на внимании от20до37миллионов.
Далее мы определяем оптимизатор и функцию потерь критерий. Как и раньше, мы игнорируем потери, когда целевая последовательность является маркером заполнения.
Затем мы определяем цикл обучения модели.
Мы обрабатываем последовательности немного иначе, чем в предыдущих уроках. Для всех моделей мы никогда не помещаем <eos> в декодер. В моделях RNN это объяснялось тем, что цикл декодера останавливается при достихении значения <eos> в качестве входа для декодера. В этой модели мы просто отрезаем токен <eos> от конца последовательности. Таким образом:
обозначает фактический элемент целевой последовательности. Затем мы вводим это в модель, чтобы получить предсказанную последовательность, которая, как мы надеемся, должна предсказывать токен <eos>:
обозначает предсказанный элемент целевой последовательности. Затем мы вычисляем наши потери, используя исходный тензор trg с токеном <sos> , отрезанным спереди, оставляя токен <eos>:
Затем мы рассчитываем наши потери и обновляем наши параметры, как это обычно делается.
Опять же, у нас есть функция, которая сообщает нам, сколько времени занимает каждая эпоха.
Наконец, мы обучаем нашу модель. Обратите внимание, что мы уменьшили значение CLIP с 1 до 0.1, чтобы обучить эту модель более точно. При более высоких значениях «CLIP» градиент иногда резко увеличивается.
Хотя у нас почти в два раза больше параметров, чем у модели RNN, основанной на внимании, на самом деле она обучение занимает примерно половину времени от стандартной версии, и примерно столько же, как в версии упакованных дополненных последовательностей. Это связано с тем, что все вычисления выполняются параллельно с использованием свёрточных фильтров вместо последовательного использования RNN.
Замечание : эта модель всегда имеет коэффициент для обучения с принуждением равный 1, то есть всегда будет использоваться истинное значение для следующего токена из целевой последовательности. Это означает, что мы не можем сравнивать значения точности с предыдущими моделями, в которых использовался коэффициент обучения с принуждением отличный от 1. Смотрите здесь для результатов RNN, основанной на внимании, с использованием коэффициента обучения с принуждением равным 1.
Затем мы загружаем параметры, которые имеют наименьшие потери при проверке, и вычисляем потери по набору тестов.
Вывод.
Теперь мы можем переводить с помощью нашей модели использую функцию translate_sentence ниже.
Выполняются следующие шаги при переводе:
токенизируем исходное предложение, если оно не было токенизировано являетсястрокой.
добавляем токены <sos> и <eos>.
нумеруем исходное предложение.
преобразовываем его в тензор и добавляем размер батча.
передаём исходное предложение в кодировщик.
создаём список для хранения выходного предложения, инициализированного токеном <sos>.
пока мы не достигли максимальной длиныпреобразуем текущий прогноз выходного предложения в тензор с размерностью батчапоместим текущий выход и два выхода кодировщика в декодерполучим предсказание следующего выходного токена от декодерадобавим предсказание к предсказанию текущего выходного предложенияпрерываем, если предсказание было токеном <eos>.
преобразоваем выходное предложение из индексов в токены.
возвращаем выходное предложение (с удаленным токеном <sos>) и вектор внимания с последнего слоя.
Затем у нас есть функция, которая будет отображать, насколько модель обращает внимание на каждый входной токен на каждом этапе декодирования.
Затем мы, наконец, начнем переводить несколько предложений.
Сначала возьмем пример из обучающей выборки:
Затем мы передаем его в нашу функцию translate_sentence , которая дает нам предсказанные токены перевода, а также вектор внимания.
BLEU.
Наконец, мы рассчитываем оценку BLEU для модели.
Мы получили оценку BLEU ~ 34 по сравнению с моделью RNN, основанной на внимании, которая дала нам ~ 28. Это улучшение показателя BLEU на ~ 17%.
Обучение сети инвертированию предложения.
В конце приведу один из моих любимых тестов: тест на инверсию предложения. Очень простая для человека задача ученики начальной школы обучаются за 10-15 примеров, но, порой, непреодолима для искусственных систем.
Для Google Colab скачаем обучающие последовательности.
В начале обучим сеть инверсии и посмотрим на результат.
Мы рассмотрели первую из наших моделей, не использующих RNN! Далее идет модель «Transformer», в которой даже не используются свёрточные слои — только линейные слои и множество механизмов внимания.