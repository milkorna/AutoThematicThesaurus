Представте себе, как было бы удобно, написать предложение и найти похожее к нему по смыслу. Для этого нужно уметь векторизовать всё предложение, что может быть очень не тривиальной задачей.
По специфике своей работы, я должен искать похожие запросы в службу поддержки и даже имея достаточно большую разметку, бывает тяжело собрать необходимое количество сообщений подходящих по тематике, но написанных другими словами.
Ниже обзорное исследование на способы векторизации всего предложения и не просто векторизации, а попытка векторизовать предложение с учётом его смысла.
Например две фразы 'эпл лучше самсунг' от 'самсунг лучше эпл', должны быть на противоположном конце по одному из значений вектора, но при этом совпадать по другим.
Можно привести аналогию с картинкой ниже. По шкале от кекса до собаки они находятся на разных концах, а по количеству чёрных точек и цвету объекта на одном.
Вот тут сборник статей по векторизации предложений.
Методы в статьях очень не тривиальные и интересны для изучения, но минусы в том, что:
они испытывались на английском языке.
в каждой статье написано, что они превзошли предшественников, но сравнения проводились на разных датасетах и нет возможности сделать рейтинг.
Поэтому ниже обзорно — сравнительный анализ 7 разных методов векторизации предложений на одном датасете.
Содержание.
Подготовка Функция оценки..
Методы BOW.
1.1. простой BOW.
1.2. BOW c леммами слов.
1.3. BOW с леммами и очисткой стопслов
1.4. LDA..
Методы, использующие эмбединги токенов.
2.1 Среднее по эмбедингу всех слов.
2.2 Среднее по эмбедингу с очисткой стоп слов
2.3 Среднее по эмбедингу с весами tf-idf..
Languade Models.
3.1 Language Model on embedings
3.2 Language Model on index..
BERT.
4.1 rubert_cased_L-12_H-768_A-12_pt.
4.2 ru_conversational_cased_L-12_H-768_A-12_pt.
4.3 sentence_ru_cased_L-12_H-768_A-12_pt.
4.4 elmo_ru-news_wmt11-16_1.5M_steps.
4.5 elmo_ru-wiki_600k_steps
4.6 elmo_ru-twitter_2013-01_2018-04_600k_steps..
Автоэнкодеры.
5.1 Автоэнкодер embedings -> embedings.
5.2 Автоэнкодер embedings -> indexes.
5.3 Автоэнкодер архитектура LSTM -> LSTM
5.4 Автоэнкодер архитектура LSTM -> LSTM -> indexes..
Эмбединги на Transfer Learning.
6.1 Эмбединги на BOW.
6.2 Эмбединг на LSTM + MaxPooling.
6.3 Эмбединг на LSTM + Conv1D + AveragePooling
6.4 Эмбединг на LSTM + Inception + Attention..
Triplet loss.
7.1 Triplet loss на BOW
7.2 Triplet loss на embedings.
Подготовка.
База знаний.
Источник знаний и разметки по тематикам Обяснение на Вики.
Выбираем из базы знаний три темы с примерно равным количество сообщений в них, которые будут использоваться для оценки методов и удалим их из базы знаний.
Примеры выводов.
Для оценки нужна только одна функция от каждого метода get_similarity_values, которая будет принимать на вход сообщения и возвращать расстояния от каждого к каждому. Сообщения сортируются по увеличению расстояния и рассчитывается сколько баллов набрал этот метод.
Баллы будут считаться так: всего 3 темы с 70, 72 и 72 предложениями в каждом. Чем ближе сообщение распологается к тому, которое мы оцениваем, тем больше баллов за него начисляется. Убываение ценности предложения идёт пропорционально индексу. Т.е. если самое ближайшее сообщение из той же темы, то +214 баллов, если второе сообщение из другой темы, то -213 и т.д. до последнего.
Теоретически максимально возможное значение = sum(214… 214 — 72) — sum(214-72… 0) + 7626 = 10395.
Теоретически минимально возможное значение = -sum(214… 72) + sum(72… 0) + 7626 = -10053.
'random arrange: 0.0'.
1. Методы BOW.
1.1 BOW.
Будем определять расстояни между предложеними по словам, которые находятся в предложении без предварительной обработки (сохраняя все знаки препинания).
'BOW: 693.1'.
1.2 BOW с леммами слов.
Тот же алгоритм, но теперь будут использоваться леммы слов.
'BOW с леммами слов: 1645.8'.
1.3 BOW с леммами с очисткой стоп слов и знаков препинания.
'BOW с леммами и без стоп слов: 1917.6'.
1.4 LDA.
LDA: 344.7 LDA с леммами: 1092.1 LDA с леммами и без стоп слов: 1077.2.
2. Методы, использующие эмбединги токенов.
Будем использовать предобученные эмбединги и добавим возможность выбирать метод векторизации и фунцию расстояния.
Здесь можно скачать fasttext.
А вот тут ссылка для скачивания и инструкция использования gensim модели word2vec.
2.1 Среднее по эмбедингу слов.
среднее по embedings с euclidean_distances с word2vec + fast_text: 1833.6 среднее по embedings с euclidean_distances с fast_text: 913.5 среднее по embedings с euclidean_distances с word2vec: 1941.6 среднее по embedings с cosine_distance с word2vec + fast_text: 2278.1 cреднее по embedings с cosine_distance с fast_text: 829.2 среднее по embedings с cosine_distance с word2vec: 2437.7.
2.2 Среднее по эмбедингу с предварительной очисткой стоп слов.
среднее по embedings без стопслов с euclidean_distances с word2vec + fast_text: 2116.9 среднее по embedings без стопслов с euclidean_distances с fast_text: 1314.5 среднее по embedings без стопслов с euclidean_distances с word2vec: 2159.1 среднее по embedings без стопслов с cosine_distance с word2vec + fast_text: 2779.7 среднее по embedings без стопслов с cosine_distance с fast_text: 2199.0 среднее по embedings без стопслов с cosine_distance с word2vec: 2814.4.
2.3 Среднее по эмбедингу с весами tf-idf.
среднее по embedings с tf-idf с cosine_distance с word2vec + fast_text: -133.6 среднее по embedings с tf-idf с cosine_distance с fast_text: 9.0 среднее по embedings с tf-idf с cosine_distance с word2vec: -133.6 среднее по embedings с tf-idf с euclidian_distance с word2vec + fast_text: 6.4 среднее по embedings с tf-idf с euclidian_distance с fast_text: -133.6 среднее по embedings с tf-idf с euclidian_distance с word2vec: -133.6.
Методы без учителя.
Следующие несколько моделей потребуют потоковой генерации данных, поэтому сделаем универсальные генераторы.
Если кажтся, что потоковая генерация слишком дорогая, то оцените время, которое занимает генерация 100 батчей с размером батча 32, получается:
448 ms ± 65.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each).
5.77 s ± 115 ms per loop (mean ± std. dev. of 7 runs, 1 loop each).
3. Languade Models.
Нейронная сеть угадывает следующее слово в преложении. Предсказание по всей длине текста, является эмбедингом предложения. Угадывать будем на основании предыдущих слов: максимум 20 и минимум 5.
3.1 Language Model on embedings.
0 1644.6 3 148.7 6 274.8 9 72.3 12 186.8 15 183.7 18 415.8 21 138.9.
3.2 Language Model on token index.
0 1700.6 3 404.7 6 255.3 9 379.8 12 195.2 15 160.1 18 530.7 21 701.9 24 536.9.
Конец первой части.
ссылка на Часть2.