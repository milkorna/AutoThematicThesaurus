Немного пафоса вместо предисловия.
В качестве иллюстрации к этой статье я выбрал картину Жана-Мишеля Баскии, написаннаю в 1984 году и купленную на аукционе в 2017 году за 110.5 миллионов долларов. Она привлекает сочностью цвета и отталкивает бесчеловечностью образа человеческого лица. Кажется, что это рисунок машины, появившийся за 30 лет до нейронных сетей, способных создавать подобные изображения (например работы от ruDALL-E Malevich от Сбера, которые можно генерить, например, в Телеграме ).
Современные большие нейросетевые модели привлекают и завораживают своей сложностью и мощью и пугают своей похожестью на нас и одновременно своей чуждостью нам, своим потенциалом превзойти нас.
Человеку удалось создать алгоритм, логику работы которого он уже не может объяснить ( GPT-4 от OpenAI исследуется не как математическая модель, а как человеческий разум ). Десятки и сотни миллиардов элементарных математических функций, соединенные в один вычислительный комплекс, обрели способности, которые до недавнего времени были доступны только одному существу в известной нам части Вселенной, нам самим.
Историческое введение.
Отсчет истории "искусственного разума" можно начать с первых больших вычислительных машин, а можно с арифмометра Лейбница, но я начну с конца последней зимы искусственного интеллекта, примерно с 2010 года.
В 2011 году в книге "Физика будущего" Митио Каку описывал эксперимент в котором моделировался мозг крысы, имеющий 100 млн. нейронов. Для эксперимента понадобился суперкомпьютер Пентагона.

Суперкомпьютер был в состоянии удерживать работающую модель несколько секунд.
По оценкам 2011 года для создания компьютерной модели, содержащей количество параметров, соизмеримых с человеческим мозгом (порядка 100 миллиардов нейронов и связей между ними), понадобился бы суперкомпьютер, чье энергопотребление составило бы 1ГВт, что соответствует мощности одного энергоблока атомной электростанции. Для охлаждения этого суперкомпьютера потребовалось бы отвести целую реку и провести ее через этот компьютер ("Физика будущего", Митио Каку, М. Альпина нон-фикшн, 2014, стр. 142).
"...на полное понимание механизмов мышления или на создание машины, способной дублировать функции человеческого мозга, не приходится рассчитывать раньше конца XXI века." (там же, стр. 146).
Однако, уже в следующем после издания книги, 2012 году произошел прорыв в области машинного обучения. Использование сверточной многослойной нейронной сети сети дало резкий скачек в качестве распознавания образов (задача, которая не поддавалась исследователям и инженерам на протяжении десятков лет). А дальше "понеслось" и вот уже в  2014 году Рэймонд Курцвейл обещает, что Гугл к 2020 году создаст систему, которая сможет понимать естественный язык и общаться с человеком.
К 2020 году Гуглу не удалось создать систему, понимающую естественный язык. Однако, в 2017 разработана удачная архитектура нейронных сетей, названная «Трансформером», благодаря которой был совершен прорыв в компьютерной лингвистике. Это позволило резко поднять качество компьютерного перевода а также появились новые инструменты для классификации, кластеризации, генерации текстов, создания вопросно ответных систем и т. п. (Трансформеры активно применяются и в других областях – компьютерное зрение, распознавание речи, предсказательные модели и т. п. но мы тут про язык). Совершилась маленькая революция, значение которой могли оценить только специалисты в области машинного обучения.
Революция ChatGPT.
И вот в 2022 году, по прошествии 11 лет после экспериментов с вычислительной моделью мозга крысы и предсказаниями Митио Каку, что между человечеством и "искусственным разумом" многие десятилетия и через 2 года после срока, предсказанного Курцвейлом, компания OpenAI на основе Трансформерной архитектуры разработала и обучила модель GPT-3.5, ставшую известной, как ChatGPT.
ChatGPT это нейронная сеть, созданная на базе GPT-3 (GPT расшифровывается как  Generative Pre-trained Transformer, или «трансформер, обученный для генерации текста») относящаяся к классу «больших языковых моделей» (LLM или Large Language Model) имеющая 175 млрд. параметров (что в 2000 раз больше, чем нейронов у мозга крысы и соизмеримо с количеством нейронов в мозгу человека). Компьютерная инфраструктура OpenAI выдерживает  запросы от десятков миллионов пользователей.  Слово Generative (генеративная) в названии означает, что ее ответы полностью сгенерированы нейросетевым "мозгом" машины (не берутся из базы знаний или базы текстов, как это делает Интернет поисковик).
ChatGPT умеет отвечать на вопросы без ограничения по тематикам. Сочиняет истории и даже стихи на заданные темы. Может создавать программный код на разных языках программирования (часто рабочий). Генерирует документы, по моей просьбе написала вполне сносное "Техническое задание" на разработку лингвистического сервиса, так же умеет рисовать (создавать рисунки по запросу пользователя). И все это получая задание от человека на естественном языке. Попробовать модель можно через Телеграм-бота (вдруг кто-то еще не...).
Если не критически относиться к истинности высказываемых суждений и приводимых фактов, то все утверждения сети и ее ответы выглядят убедительными. Формулировки не уступают ответу образованного человека с хорошо подвешенным языком, легко жонглирующего цифрами и специальными терминами.
Наиболее революционным в работе этой модели является качество понимания запросов пользователей (интентов, если пользоваться терминологией чат-ботов). Она практически всегда понимает, что от нее хотят, умение удерживать контекст длительного диалога (еще пару лет назад это считалось не решаемой задачей для чат-бота). Качество генерируемых ответов позволяет утверждать, что «тест Тьюринга» в его первоначальной формулировке пройден с большим запасом.
Отдельно нужно отметить, что OpenAI уже запустила и предоставляет платный доступ к следующему поколению своего виртуального андроида GPT-4 (доступ из РФ закрыт, впрочем, как и к ChatGPT). GPT-4 модель по своим возможностям существенно превосходит ChatGPT . Говорят, что в недрах OpenAI уже тестируется GPT-5 и будет опубликована в этом году...
Будет ли монополия на искусственный разум.
На сегодняшний день можно обозначить два полюса развития высокотехнологичных систем. На одном находятся поисковики, непрерывно индексирующие Интренет (Яндекса, Гугл и им подобные), занимающие монопольное положение на рынке поиска и скрывающие используемые алгоритмы ранжирования выдачи. На другом продукты, создаваемые на основе свободно распространяемого программного обеспечения, к которым относятся большинство библиотек машинного обучения.
Например «глубокие нейросетевые архитектуры» (те самые Трансформеры типа BERT, GPT-2, GPT-3 и т. п имеющие сотни миллионов параметров), которые являются очень сложными вычислительными комплексами, были созданы и предобучены айти гигантами типа Гугла, Фейсбука, Майкрософт, Байду и т.п., способными вкладывать неограниченные ресурсы в исследования и разработки, обладающие безграничными вычислительными мощностями.
Однако, через некоторое время эта архитектура, задавшая новый стандарт качества в решение интеллектуальных задач, стала доступной для широкой публики. Одновременно подтянулись вычислительные возможности. Появились облачные сервисы и достаточно мощные видеокарты, позволяющие за разумные деньги использовать эти модели.
На сегодняшний день, через 4 года после создание BERT есть большое количество библиотек, которые может использовать любой желающий (самая популярная https://huggingface.co/ предоставляющая доступ к огромному количеству нейросетевых моделей). Появились методы, позволяющие дообучать эти модели под свои задачи на видеокарте домашнего компьютера. Трансформерная архитектура стала стандартом в приложениях по переводу, классификации, кластеризации текстов, прочих лингвистических задач и в разработке чат-ботов.
Будут ли модели аналогичные ChatGPT, GPT-4, GPT-5 и их потомки и аналоги монопольно принадлежать нескольким крупным вендорам или относительно доступными (при наличии достаточных вычислительных мощностей) сказать сложно.
С одной стороны после медийного и коммерческого успеха ChatGPT, OpenAI, поступившись собственной философией, закрепленной в названии компании, закрыла доступ к технической информации о своих моделях (и к методологиям их обучения). Задав тренд на закрытие информации о новых разработках в области машинного обучения.
Отличный ролик на данную тему от Татьяны Шавриной из Сбера, известной в узких кругах, как Кали Новская:
С другой есть открытые библиотеки больших языковых моделей. Например, Яндекс выложил в открытый доступ модель, имеющую 100 миллиардов параметров или вот тут целый список больших моделей, которые относительно свободно распространяются (пока свободно).
Если технология ChatGPT, ее потомки (и аналоги) повторит путь Трансформеров, то через 2-5 лет дообучение и использование этих моделей станет доступным небольшим командам и отдельным разработчикам, которые смогут приспосабливать этот программный комплекс под частные задачи. Возможно, появятся компании, предоставляющие виртуальных андроидов – секретарей, бухгалтеров, юристов, программистов, продавцов, других офисных работников, разговаривающих с заказчиком на естественном языке.
Если же окажется, что такие модели будут только у технологических монстров типа Майкрософт, Гугл, OpenAI, Baidu и т. п. (после успеха ChatGPT все они в спешном порядке клепают свои LLM), то эти компании сконцентрируются на развитии своих моделей. На превращении их в «супер интеллект», владеющий всем доступным человечеству знанием. Возможно будут созданы API для «дообучения» пользователями этих моделей.
Ограничения Искусственного разума.
Пожалуй самый важный вопрос на сегодняшний день (возможно более важный, чем экология, свобода выбора пола и сексуальной ориентации, неравенство и локальные войны) – какие у этих моделей есть принципиальные ограничения (и есть ли они?). Порассуждаем об этом.
Ограничения, связанные с обучением.
Информация о том, как обучалась ChatGPT очень скудная. Неизвестно, есть ли возможность дообучать (переобучать) модель без доступа к ее коду. Можно ли, например, объяснить ей, что килограмм железа и килограмм моркови весят одинаково (некоторое время назад ChatGPT была уверена, что килограмм железа тяжелее  килограмма моркови, сейчас она отвечает правильно) и закрепить это «знание», посредством диалога с моделью.
Другой вопрос - можно ли дообучить «свой кусочек» сети, не повлияв на всю сеть. Для классических архитектур нейронных сетей это представляет отдельную проблему. От наличия (или отсутствия) такой возможности напрямую зависят дальнейшие подходы к созданию и развитию систем на основании LLM. А так же изменения ожидающие рынок экспертных систем и поиска в Интернете в ближайшие 5 - 10 лет.
Отдельный вопрос, как будет выглядеть процесс «дообучение» больших моделей. Совсем не обязательно для этого понадобятся программисты (за исключением обеспечения инфраструктуры для запуска и функционирования моделей) или «датасаентисты» в том виде, в котором мы их знаем (вот уж воистину, "посеявшие ветер...").
Фундаментальные ограничения.
Вероятностная природа генерации ответов.
Генеративные модели являются вероятностными моделями. Они выбирают наиболее вероятную последовательность выдаваемых слов в ответ на входящий к ним запрос. При этом, как бы хорошо не была обучена модель, всегда есть не нулевая вероятность, что она выдаст «безумный» ответ. У большой модели никто не сможет предсказать момент и условия генерации такого ответа (все, что можно предсказать, будет исправлено в процессе обучения).
На сегодняшний день нет гарантированного решения по устранению этого недостатка. Вполне возможно, что % ошибок таких систем будет существенно ниже, чем у среднего (а может быть даже у выдающегося) человека, но сами ошибки могут шокировать. Например, виртуальный андроид только что абсолютно верно рассказал вам Теорию относительности Эйнштейна и правильно ответил на самые каверзные вопросы по квантовой механике и тут же уверенно сообщил, что 2х2 без сомнения равно 7.
Это накладывает определенные ограничения на использование таких систем (особенно в критичных областях, таких как медицина, судопроизводство и т. п.).
Данные для обучения.
Одной из предпосылок, обусловивших текущий триумф нейронных сетей связан с доступностью для обучения больших объемов данных. В частности, языковые модели обучаются на сотнях гигабайт и терабайтах данных, доступных в Интернете. Парадоксальным образом этот факт может стать вторым принципиальным ограничением нейросетевых моделей. Эти модели способны усваивать гигантские объемы информации, трансформировать и комбинировать эту информацию. Но смогут ли они «генерировать новое» (знания, литературные сюжеты, музыку, направления в живописи)? Ответ на этот вопрос не очевиден.
Данное ограничение ИИ не является критическим (в отличие от предыдущего) для использования моделей. Однако, если оно принципиальное и непреодолимое, то это дает человечеству (по крайне мере отдельным его представителям) шанс остаться востребованными в мире, в котором «вкалывают роботы» (а счастлив ли человек?).
Отсутствие связи с реальным миром.
Фразы «вода жидкая», «камень твердый» кодируют чувственный опыт человека. Через ощущения увязывается человеческий язык или символьная система, моделирующая реальный мир с этим самым миром. У нейронной сети нет ощущений, через которые она может верифицировать собранные при обучении в Интернете данные.
Грубо говоря, выбор между «вода жидкая» или «вода твердая» такие модели осуществляют на основании частоты упоминания той или иной фразы в текстах, на которых она обучалась (возможно с последующей корректировкой, проводимой специальными асессорами, которые, очевидно, не могут охватить все вопросы и все темы).
Система не имеет встроенной возможности проверить истинность того или иного утверждения. Специалисты, обучающие такие системы, пытаются научить их выбирать правильный (или похожий на правильный) ход рассуждений. В результате, генерируемые рассуждения выглядят, как доказательства средневековых схоластов, убедительные с точки зрения формы подачи, но без какой бы то ни было гарантии (гарантии со стороны модели) их соответствия материальному миру.
Это ограничение не ставит крест на использовании моделей, но требует дополнительных усилий и подходов для их обучения и верификации ответов.
Вместо заключения.
В 2016 году в распоряжении компьютерной лингвистики были довольно простенькие модели, такие как Doc2Vec, LSI, LDA, LSTM, TF-IDF, BM25, CNN, fasttext и им подобные, в 2018 к ним присоединились "Трансформеры" (BERT, RoBERT, Sbert, GPT-2, GPT-3 и прочие). При всей мощи трансформеров и существенном приросте качества, эти модели одного уровня - инструменты для решения узких технических задач.
Появление в 2022 году ChatGPT ознаменовало переход количества (моделей, прилагаемых усилий, вложенных денег, выделенных вычислительных мощностей и т. п.) в новое качество. ChatGPT (и ее "потомки") система нового уровня. Ее работа действительно похожа на "сильный" (общий) искусственный интеллект или человеческий интеллект.
Угадать, как такие системы будут развиваться и как повлияют на нашу жизнь, скорее сможет нелинейное мышление фантаста, чем специалисты в области того же машинного обучения. Я специально начал с исторического экскурса в совсем недавнее прошлое, чтобы показать, что динамика в области искусственного интеллекта опережает ожидания, построенные на базе аппроксимации прошлого опыта.
Я не возьмусь предсказывать, как будет развиваться эта технология и ее использование. Давайте лучше спросим, что думает сама ChatGPT.
Вопрос ("затравка"): Напиши завершение текста про ChatGPT. Обобщи те возможности, которые несут новые технологии, в частности ChatGPT и те риски, которые связаны с их использованием.
ChatGPT:
Новые технологии, такие как ChatGPT, предоставляют широкие возможности для автоматической обработки естественного языка и создания интеллектуальных помощников, способных общаться с людьми. Это имеет множество практических применений, от автоматизации работы в сфере клиентского обслуживания до создания умных домов, где устройства могут общаться с людьми на естественном языке.
Однако, с развитием новых технологий, возникают и новые риски. Одним из них является угроза конфиденциальности данных. Поскольку обработка больших объемов личной информации становится все более распространенной, возникает риск утечки данных, а также использования этой информации в неправомерных целях.
Другой риск, связанный с использованием ChatGPT и других новых технологий, заключается в возможности создания поддельных сообщений и манипуляции информацией. Это может привести к распространению неправдивой информации и повлиять на принятие решений людьми.
В целом, новые технологии, такие как ChatGPT, предоставляют огромный потенциал для улучшения нашей жизни, но необходимо быть осторожным и внимательным к рискам, связанным с их использованием. Важно, чтобы производители этих технологий принимали меры для обеспечения безопасности и конфиденциальности данных, а также чтобы пользователи понимали, как использовать эти технологии безопасно и ответственно.
