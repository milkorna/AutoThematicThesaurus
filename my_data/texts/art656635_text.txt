Представляем разбор применения алгоритмов машинного обучения с использованием технологий LSTM для создания текстов.
В итоге должен получиться генератор более-менее осмысленного текста. Способы создания текстов на специальную, определенную пользователем, тему затронуты не будут – но в целом, текст будет создан в том стиле, в котором написана «обучающая выборка».
Кстати об обучающей выборке: в качестве оной будут использованы народные сказки братьев Гримм. Эти тексты будут обработаны, разбиты на биграммы уровня символов, из которых будет составлен словарь из уникальных биграмм.
Набор необходимых данных лежит на https://www.cs.cmu.edu/~spok/grimmtmp/ . Из текстовых файлов на этом сайте будут загружены и поделены на биграммы тексты первых 100 сказок. Получится что-то вроде: [‘th’, ‘e’, ‘ki’, ‘ng’, ‘w’, ‘as’…] . Для этого есть скрипт:
Использование символьных биграмм значительно уменьшает размер словаря по сравнению с использованием отдельных слов. И еще, для более быстрой обработки, все биграммы, встречающиеся в словаре менее 10 раз, будут заменены токеном ( ‘UNK’ ).
Хотя в TensorFlow и есть подбиблиотеки, где реализованы функции для работы с LSTM, они будут написаны самостоятельно, ведь важно не просто запустить код, но постараться разобраться в том, как это работает.
Для обучения с LSTM есть множество параметров и гиперпараметров. Все они имеют разный эффект, рассмотрим некоторые из них. Затем обсудим, как эти параметры используются для записи операций, выполняемых в LSTM. За этим последует понимание того, как будем последовательно передавать данные в LSTM. Обсудим, какими способами можно реализовать оптимизацию параметров с помощью отсечения градиента. И, наконец, рассмотрим возможности использования модели для вывода прогнозов, которые по сути являются биграммами, но в конечном итоге составят осмысленный текст.
Гиперпараметры:
• num_nodes: обозначает количество нейронов в памяти ячейки. Когда данных много, увеличение сложности памяти ячейки может дать лучшую производительность; однако в то же время это замедляет вычисления;
• batch_size: объем данных, обрабатываемых за один шаг. Увеличение этого параметра повышает производительность, но предъявляет более высокие требования к памяти;
• num_unrollings: количество временных шагов. Чем больше шагов, тем выше производительность, но при этом увеличиваются как требования к памяти, так и время вычислений.
• dropout: иначе — отсев (метод регуляризации), применяется чтобы уменьшить переобучение модели и получить лучшие результаты; этот параметр показывает, какое количество информации будет случайным образом удалено из входов/выходов/переменных перед передачей их для обработки. Это может приводить к повышению производительности.
Параметры:
• ix: это веса для входов функции;
• im: это веса, соединяющие скрытые слои с входами;
• ib: это смещение (bias).
Примерно так же будут обозначены веса для скрытых слоев нейронной сети и на выходе.
Для получения фактических прогнозов, определим слой softmax:
Операции в ячейке LSTM:
• Вычисление выходных данных;
• Расчет состояния ячейки;
• Расчет внешнего скрытого состояния;
Входные (тренировочные) данные и лейблы:
Входные данные для обучения представляют собой список с последовательными пакетами данных, где каждый пакет данных имеет размер [batch_size, word_size]:
Данные для тестирования и валидации:
Последовательные вычисления, для обработки данных:
Здесь рекурсивно рассчитываются выходы ячеек и логиты (модели логистических функций) для скрытых выходных данных из тренировочного сета.
Далее, перед расчетом потерь, нужно убедиться, что значения выводов обновлены до самого последнего вычисленного значения. Для этого, добавим условие tf.control_dependencies:
Также определяем логику прямого распространения ошибки для валидационных данных. Dropout на валидационных данных использован не будет:
Оптимайзер:
Определим процесс оптимизации. Будем использовать Adam-оптимайзер, который на сегодняшний день является одним из лучших на основе стохастического градиента. Переменная gstep в коде используется для уменьшения скорости обучения с течением времени. Подробности будут в следующем разделе. Кроме того, будем использовать отсечение градиента, чтобы избежать так называемых «взрывов» градиента:
Убывающая скорость обучения вместо постоянной — распространенный метод, используемый в глубоком обучении для повышения производительности и уменьшения переобучения. Ключевая идея в том, чтобы уменьшить скорость обучения (например, в 2 раза), если сложность проверки не уменьшается в течение заданного количества эпох.
Вот как это реализовано. Сначала определяем gstep и операцию увеличения gstep (inc_gstep), следующим образом:
Затем, всякий раз, когда потери на валидационной выборке не уменьшаются, вызываем inc_gstep следующим образом:
Прогнозирование:
Теперь можно делать прогнозы, просто применяя softmax к логитам, которые рассчитали ранее. Определим операцию прогнозирования для логитов на валидационных данных:
Расчет потерь:
Чем выше значение потерь, тем хуже работает модель. Если потери большие следует попробовать изменить гиперпараметры. Есть и другие метрики для проверки качества модели, но лучше перейдем к следующему, финальному этапу.
Генерация текста:
Наконец, определим переменные и операции, необходимые для создания нового текста. Они определяются аналогично тому, что я сделал для обучающих данных. Только вместо проверки будет вывод результата:
Пример сгенерированного текста:
Текст на английском поскольку для обучения были использованы английские версии сказок братьев Гримм. Его можно перевести на русский и вы увидите, что содержание окажется вполне осмысленным. Так же можно использовать другие датасеты, или создать свой из интересных книг. Истории, созданные алгоритмами машинного обучения, гарантированно вас удивят.
Так же советую к прочтению книгу: Natural Language Processing with TensorFlow от автора Thushan Ganegedara.