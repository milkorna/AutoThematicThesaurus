С тех пор как в 2018 году был представлен BERT, исследования в области обработки естественного языка охвачены новой парадигмой: использованием больших объемов существующего текста для предварительного обучения параметров модели на основе самообучения (self-supervision), не требующего разметки данных. Таким образом, вместо того, чтобы обучать модель для обработки естественного языка (NLP) с нуля, можно взять предобученную модель, уже имеющую некоторое знание о языке. Однако, для успешного применения этого нового подхода в NLP исследователю необходимо иметь некоторое представление о том, что же именно способствует языковому обучению модели: высота нейронной сети (т.е. количество слоев), ее ширина (размер представлений скрытых слоев), критерий обученности для самообучения или что-то совсем иное?
В статье « ALBERT: облегченный BERT для самообучения языковым представлениям », принятой на ICLR 2020, была представлена обновленная версия BERT’а, которая показывает более высокие результаты в 12 задачах обработки языка, включая соревнование Stanford Question Answering Dataset (SQuAD v2.0) и бенчмарк RACE для понимания текстов из экзаменов SAT. ALBERT выпускается в качестве открытого решения поверх TensorFlow и включает в себя несколько готовых к использованию предобученных языковых моделей.
Что повышает качество NLP моделей?
Выделить главный фактор улучшения качества в NLP сложно: да, одни настройки важнее других, но, как показывает данное исследование, простой перебор этих настроек независимо друг от друга не даёт правильных ответов.
Ключом к оптимизации модели, заложенным в архитектуру ALBERT, является более эффективное распределение ее ресурсов. Входные эмбеддинги (слова, суб-токены и т.д.) выучивают векторные представления, независимые от контекста: например, представление слова «bank». Эмбеддинги скрытых слоев, напротив, должны уточнять значения этих векторных представлений в зависимости от конкретного контекста: например, вектор «bank» как финансового учреждения или как берега реки.
Это достигается путем факторизации параметризации эмбеддингов: матрица эмбеддингов разделяется между векторами входного слоя с относительно небольшой размерностью (например, 128), в то время как вектора скрытого слоя используют бОльшие размерности (768, как в случае с BERT'ом, и больше). Только с помощью этого шага, при прочих равных, ALBERT на 80% снижает количество параметров проекционного блока ценой лишь незначительного падения производительности – 80.3 балла вместо 80.4 для SQuAD2.0 и 67.9 вместо 68.2 для RACE.
Другое важное изменение в архитектуре ALBERT связано с исследованием избыточности. Архитектуры нейронных сетей на основе Трансформера (такие как BERT, XLNet и RoBERTa ) полагаются на независимость слоев, расположенных друг над другом. Однако было замечено, что зачастую нейросеть выучивается выполнять схожие операции на разных уровнях, используя различные параметры сети. Эта возможная избыточность устраняется в ALBERT с помощью обмена параметрами между слоями, и, таким образом, один и тот же слой применяется друг к другу. И хотя такой подход немного снижает точность (accuracy), более компактный размер самой модели оправдывает потерю в качестве. Подобный обмен обеспечивает снижение параметров для блока с механизмом внимания внимания на 90% (общее снижение на 70%), что при применении в дополнение к факторизации параметризации эмбеддингов приводит к небольшому снижению показателей: до 80.0 (-0.3) для SQuAD2.0 и до 64.0 (-3.9 балла) для RACE.
Внедрение двух представленных изменений рождает модель ALBERT-base, которая имеет всего 12 миллионов параметров, что на 89% меньше базовой модели BERT, но при этом обеспечивает достойное качество в рассмотренных бенчмарках. Вместе с тем подобное уменьшение количества параметров дает возможность и дальнейшего масштабирования. Если объем памяти позволяет, можно увеличить размер эмбеддингов скрытого слоя в 10-20 раз. При размере скрытого слоя 4096 конфигурация ALBERT-xxlarge обеспечивает как общее снижение параметров на 30% по сравнению с моделью BERT-large, так и, что более важно, значительный прирост качества: +4.2 для SQuAD2.0 (88.1 по сравнению с 83.9) и +8.5 для RACE (82.3 по сравнению с 73.8).
Полученные результаты свидетельствуют о том, что понимание языка зависит от разработки надежных и высокоёмких контекстных представлений. Контекст, смоделированный в эмбеддингах скрытого слоя, улавливает значение слов, что, в свою очередь, способствует общему пониманию, которое напрямую измеряется показателями модели в стандартных бенчмарках.
Показатели оптимизированной модели на наборе данных RACE.
Чтобы оценить способность модели понимать язык, можно провести тест на понимание прочитанного (например, схожий с SAT Reading Test ). Крупнейший общедоступный ресурс для этой цели — набор данных RACE (2017 г.). То, как компьютер справляется с этим испытанием, хорошо отражает достижения в области языкового моделирования последних лет: модель, предварительно обученная только на контекстно-независимых представлениях слов, получает низкие баллы в этом тесте (45.9; крайний левый столбик), в то время как BERT, получивший контекстно-зависимое знание языка, справляется достаточно хорошо — 72.0 балла. Усовершенствованные модели BERT, такие как XLNet и RoBERTa, установили планку еще выше — в диапазоне 82–83 баллов. Конфигурация ALBERT-xxlarge, упомянутая выше, дает оценку RACE в том же диапазоне (82.3) при обучении на наборе данных базового BERT'a (Википедия и Книги). Однако при обучении на том же расширенном наборе данных, что и XLNet и RoBERTa, она значительно превосходит все существующие на сегодняшний день подходы и устанавливает новую оценку в 89.4.
Показатели моделей в бенчмарке RACE (задача на понимание текстов из экзаменов SAT). Оценка для случайного предсказания составляет 25.0. Максимально возможный балл – 95.0.
Успех ALBERT демонстрирует важность выявления тех аспектов модели, которые помогают создать мощные контекстные представления. Сосредоточив свои усилия по улучшению на этих аспектах архитектуры, можно значительно повысить как эффективность модели, так и ее показатели в самых различных задачах обработки естественного языка. Для того, чтобы способствовать дальнейшему развитию в области NLP, авторы открыли исходный код ALBERT и приглашают сообщество исследователей к сотрудничеству.
Авторы.
Авторы оригинала — Radu Soricut, Zhenzhong Lan.
Перевод — Смирнова Екатерина.
Редактирование и вёрстка — Шкарин Сергей.