Предисловие.
На просторах интернета имеется множество туториалов объясняющих принцип работы LDA(Latent Dirichlet Allocation — Латентное размещение Дирихле) и то, как применять его на практике. Примеры обучения LDA часто демонстрируются на "образцовых" датасетах, например "20 newsgroups dataset", который есть в sklearn.
Особенностью обучения на примере "образцовых" датасетов является то, что данные там всегда в порядке и удобно сложены в одном месте. При обучении продакшн моделей, на данных, полученных прямиком из реальных источников все обычно наоборот:
Много выбросов.
Неправильная разметка(если она есть).
Очень сильные дисбалансы классов и 'некрасивые' распределения каких-либо параметров датасета.
Для текстов, это: грамматические ошибки, огромное кол-во редких и уникальных слов, многоязычность.
Неудобный способ харнения данных(разные или редкие форматы, необходимость парсинга).
Исторически, я стараюсь учиться на примерах, максимально приближенных к реалиям продакшн-действительности потому, что именно таким образом можно наиболее полно прочувстовать проблемные места конкретного типа задач. Так было и с LDA и в этой статье я хочу поделиться своим опытом — как запускать LDA с нуля, на совершенно сырых данных. Некоторая часть статьи будет посвящена получению этих самых данных, для того, чтобы пример обрел вид полноценного 'инженерного кейса'.
Topic modeling и LDA.
Для начала, рассмотрим, что вообще делает LDA и в каких задачах используется. Наиболее часто LDA применяется для Topic Modeling(Тематическое моделирование) задач. Под такими задачами подразумеваются задачи кластеризации или классификации текстов — таким образом, что каждый класс или кластер содержит в себе тексты со схожими темами.
Для того, чтобы применять к датасету текстов(далее корпус текстов) LDA, необходимо преобразовать корпус в term-document matrix(Терм-документная матрица).
Терм-документная матрица — это матрица которая имеер размер.
LDA строит, для данной Терм-документной матрицы и T заранее заданого числа тем — два распределения:
Распределение тем по текстам.(на практике задается матрицей размера ).
Распределение слов по темам.(матрица размера ).
Значения ячеек данных матриц — это соответственно вероятности того, что данная тема содержится в данном документе(или доля темы в документе, если рассматривать документ как смесь разных тем) для матрицы 'Распределение тем по текстам'.
Для матрицы 'Распределение слов по темам' значения — это соотв-но вероятность встретить в тексте с темой i слово j, качествено, можно рассматривать эти числа как коэффициенты характеризующие, то насколько данное слово характерно для данной темы.
Следует сказать, что под словом тема понимается не 'житейское' определение этого слова. LDA выделяет T тем, но что это за темы и соответствуют ли они каким-либо известным темам текстов, как например: 'Спорт', 'Наука', 'Политика' — неизвестно. В данном случае, уместно скорее говорить о теме, как о некой абстрактной сущности, которая задается строкой в матрице распределения слов по темам и с некоторой вероятностью соответствует данному тексту, если угодно можно представить ее, как семейство характерных наборов слов встречающихся вместе, с соответствующими вероятностями(из таблицы) в некотором определенном множестве текстов.
Если вам интересно более подробно и 'в формулах' изучить как именно обучается и работает LDA, то вот некоторые материалы(которые использовались автором):
Оригинальная статья.
На английском, с наглядными примерами.
Подробно на русском.
О реализации на Python.
Добываем дикие данные.
Для нашей 'лабораторной работы', нам понадобится кастомный датасет со своими недостатками и особенностями. Добыть его можно в разных местах: выкачать отзывы с Кинопоиска, статьи из Википедии, новости с какого-нибудь новостного портала, мы возьмем чуть более экстремальный вариант — посты из сообществ ВКонтакте.
Делать это мы будем так:
Выбираем некоторого пользователя ВК.
Получаем список всех его друзей.
Для каждого друга берем все его сообщества.
Для каждого сообщества каждого друга выкачиваем первые n(n=100)постов сообщества и объединяем в один текст-контент сообщества.
Инструменты и статьи.
Для выкачивания постов будем использовать модуль vk для работы с API ВКонтакте, для Python. Один из наиболее замысловатых моментов при написании приложения с использованием API ВКонтакте — это авторизация, к счастью, код выполняющий эту работу уже написан и есть в открытом доступе, кроме vk я использовал небольшой модуль для авторизации — vkauth.
Ссылки на используемые модули и статьи для изучения API ВКонтакте:
vkauth.
vkauth туториал.
vk туториал.
vk туториал №2.
Официальная документация API ВКонтакте.
Пишем код.
И так, с помощью vkauth, авторизируемся:
В процессе, был написан небольшой модуль содержащий все необходимые для выгрузки контента в соответствующем формате функции, ниже они преведены, давайте пройдемся по ним:
Итоговый пайплайн имеет следующий вид:
Fails.
В целом, сама по себе процедура выкачивания данных не представляет собой ничего трудного, обратить внимание следует лишь на два момента:
Иногда, в силу приватности некоторых сообществ вы будете получать ошибки доступа, иногда другие ошибки — решается установкой try,except в правильном месте.
У ВК есть ограничение на количество запросов в секунду.
При совершении большого количества запросов, например в цикле, мы так же будем ловить ошибки. Эту проблему можно решить несколькими способами:
Тупо и прямо: Воткнуть sleep(some) каждые 3 запроса. Делается в одну строчку и сильно замедляет выгрузку, в ситуациях, когда объемы данных не велики, а на более изощренные методы нет времени — вполне приемлимо.(Реализовано в данной статье).
Разобраться в работе Long Poll запросов https://vk.com/dev/using_longpoll.
В данной работе был выбран простой и медленный способ, в дальнейшем, я возможно напишу микростатью про способы обхода или ослабления ограничений на количество запросов в секунду.
Итог.
С затравочным 'некоторым' пользователем имеющим ~150 друзей, удалось добыть 4679 текстов — каждый характеризует некоторое сообщество ВК. Тексты сильно варьируются по размеру и написаны на многих языках — часть из них не пригодна для наших целей, но об этом мы поговорим чуть дальше.
Основная часть.

Пройдемся по всем блокам нашего пайплайна — сначала, по обязательным(Идеально), затем по остальным — они, как раз и представляют наибольший интерес.
CountVectorizer.
Перед тем, как учить LDA, нам необходимо представить наши документы в виде Терм-документной матрицы. Это обычно включает в себя такие операции как:
Удаление путктуации/чисел/ненужных лексем.
Токенизация(представление в виде списка слов).
Подсчет слов, составление терм-документной матрицы.
Все эти действия в sklearn удобно реализованы в рамках одной программной сущности — sklearn.feature_extraction.text.CountVectorizer.
Ссылка на документацию.
Все, что нужно сделать это:
LDA.
Аналогично с CountVectorizer`ом, LDA, прекрасно реализовано в Sklearn и других фреймворках, поэтмому уделять непосредственно их реализациям много места, в нашей, сугубо практической статье нет особого смысла.
Ссылка на документацию.
Все, что нужно, чтобы запустить LDA это:
Preprocessing.
Если мы просто возьмем наши тексты сразу после того как скачали их и конвертируем в Терм-документную матрицу с помощью CountVectorizer, со встроеным дефолтным токенайзером, мы получим матрицу размера 4679x769801(на используемых мной данных).
Размер нашего словаря будет составлять 769801. Даже если допустить, что большая часть слов информативны, то мы все равно вряд ли получим хороший LDA, нас ждет что-то вроде 'Проклятия размерностей', не говоря уже о том, что практически для любого компьютера, мы просто забьем всю оперативную память. На деле, большя часть этих слов совершенно не информативны. Огромная часть из них это:
Смайлы, символы, числа.
Уникальные или очень редкие слова(например польские слова из группы с польскими мемами, слова написаные с ошибками или на 'олбанском').
Очень частые части речи(например, предлоги и местоимения).
Кроме того, многие группы в ВК специализируются исключительно на изображениях — там почти нет текстовых постов — тексты соответствующие им вырождены, в Терм-документной матрице они будут давать нам практически полностью нулевые строки.
И так, давайте же отсортируем это все! Токенизируем все тексты, уберем из них пунктуацию и числа, посмотрим на гистограмму распределения текстов по количеству слов:
Уберем все тексты размером меньше 100 слов(их 525).
Теперь словарь: Удаление всех лексем(слова) состоящих не из букв, в рамках нашей задачи — это вполне допустимо. CountVectorizer делает это сам, даже если нет, то думаю здесь не нужно приводить примеров(они есть в полной версии кода к статье).
Одной из наиболее распространенных процедур по уменьшению размера словаря является удаление так называемых stopwords(стопворды) — слов не несущих смысловой нагрузки или/и не имеющих тематической окрашенности(в нашем случае — Topic Modeling же). Такими словами в нашем случае являются, например:
Местоимения и предлоги.
Артикли — the,a.
Общеупотребительные слова: 'быть', 'хорошо', 'наверное' и.т.д...
В модуле nltk есть сформированные списки стопвордов на русском и на английском, но они слабоваты. В интернете можно найти еще списки стопвордов для любого языка и добавить их к тем, что есть в nltk. Так мы и сделаем. Возьмем дополнительно стопворды отсюда:
https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.json.
https://gist.github.com/menzenski/7047705.
На практике, при решении конкретных задач списки стопвордов постепенно корректируются и дополняются по мере обучения моделей, так как для каждого конкретного датасета и задачи существуют свои конкретные 'несодержательные' слова. Мы тоже подберем себе кастомных стопвордов после обучения нашего LDA 'первого поколения'.
Сама по себе процедура удаления стопвордов встроена в CountVectorizer — нам только нужен их список.
Достаточно ли того, что мы сделали?

Большинство слов которые находятся в нашем словаре по-прежнему не слишком информативны для обучения на них LDA и не находятся в списке стопвордов. Поэтому применим к нашим данным еще один способ фильтрации.


, где t — слово из словаря. D — корпус(множество текстов) d — один из текстов корпуса. Посчитаем IDF всех наших слов, и отсечем слова с самым большим idf(очень редкие) и с самым маленьким(широкораспространенные слова).
Полученный после вышеописанных процедур уже вполне пригоден для обучения LDA, но произведем еще стемминг — в нашем датасете часто встречаются одни и те же слова, но в разных падежах. Для стемминга использовался pymystem3.
После применения вышеописанных фильтраций размер словаря уменьшился с 769801 до 13611 и уже с такими данными, можно получить LDA модель приемлимого качества.
Тестирование, применение и тюнинг LDA.
Теперь, когда у нас есть датасет, препроцессинг и модели которые мы обучили на обработаном датасете, хорошо было бы проверить адекватность наших моделей, а так же соорудить для них какие-нибудь приложения.
В качестве приложения, для начала рассмотрим задачу генерации ключевых слов для данного текста. Сделать это в достаточно простом варианте можно следующим образом:
Получаем из LDA распределение тем для данного текста.
Выбираем n(например n=2) наиболее выраженных темы.
Для каждой из тем, выбираем m(например m=3) наиболее характерных слова.
У нас есть набор из n*m слов характеризующих данный текст.
Напишем простой класс-интерфейс который и будет реализовывать данный способ генерации ключевых слов:
Применим наш метод к нескольким текстам и посмотрим что получается: Cообщество: Агентство путешествий "Краски Мира" Ключевые слова: ['photo', 'social', 'travel', 'сообщество', 'путешествие', 'евро', 'проживание', 'цена', 'польша', 'вылет'] Cообщество: Food Gifs Ключевые слова: ['масло', 'ст', 'соль', 'шт', 'тесто', 'приготовление', 'лук', 'перец', 'сахар', 'гр'].
Результаты выше не 'cherry pick' и выглядят вполне адекватно. На деле, это результаты из уже настроенной модели. Первые LDA, которые были обучены в рамках этой статьи выдавали существенно более плохие результаты, среди ключевых слов можно было часто увидеть, например:
Составные компоненты веб адресов: www, http, ru, com...
Общеупотребительные слова.
единицы измерения: cm, метр, км...
Настройка(тюнинг) модели производился следующим образом:
Для каждой темы, выбираем n(n=5) наиболее характерных слов.
Считаем их idf, по тренеровочному корпусу.
Вносим в ключевые слова 5-10% наиболее широкораспространенных.
Подобную 'чистку', следует проводить аккуратно, предварительно просматривая, те самые 10% слов. Скорее, так следует выбирать кандидатов на удаление, а после уже в ручную отбирать из них слова которые следует удалить.
Где-то на 2-3 поколении моделей, с подобным способом отбора стопвордов, для топ-5% широкораспространенных топ-слов распределений мы получаем: ['любой', 'полностью', 'правильно', 'легко', 'следующий', 'интернет', 'небольшой', 'способ', 'сложно', 'настроение', 'столько', 'набор', 'вариант', 'название', 'речь', 'программа', 'конкурс', 'музыка', 'цель', 'фильм', 'цена', 'игра', 'система', 'играть', 'компания', 'приятно'].
Еще приложения.
Первое, что приходит в голову конкретно мне — это использовать распределения тем в тексте как 'эмбеддинги' текстов, в такой интерпретации можно применять к ним алгоритмы визуализации или кластеризации, и искать уже итоговые 'эффективные ' тематические кластеры таким образом.
Проделаем это:
На выходе, мы получим следующего вида картинку:
Крестики — это центры тяжести(cenroids) кластеров.
На изображении tSNE ембеддингов, видно, что кластеры выделенные с помощью KMeans, образуют достаточно связные и чаще всего пространственно разделимые между собой множества.
Все остальное, up to you.
Ссылка на весь код: https://gitlab.com/Mozes/VK_LDA.