В этой статье, используя технику Retrieval-Augmented Generation ("Поисковая расширенная генерация"), мы настроим русскоязычного бота, который будет отвечать на вопросы потенциальных работников для выдуманного свечного завода в городе Градск.
Что такое RAG?
RAG - это техника, повышающая производительность языковых моделей путём предоставления модели контекста вместе с вопросом.
Существуют разные подходы к использованию RAG. В некоторых случаях используют две LLM, в некоторые встраивают классификаторы или проводят поиск контекста по базе документов. Зависит от целей.
Мы возьмём самый простой:
Последовательность действий:
Передадим модели информацию о нашем заводе без дополнительного тюнинга;
Создадим базу векторов, где будут храниться ембеддинги ранее заданных вопросов (кэш);
При обращении к модели, будем проверять, задавались ли ранее похожие вопросы. Если да, то отдаём ранее сгенерированные ответы.
Зачем использовать кэш?
Чтобы увеличить скорость ответов для вопросов, которые задавались ранее.
Снизить затраты при использовании платных API (GTP-3.5, GPT-4) для ответов на однотипные и повторяющиеся вопросы.
В качестве модели мы будем использовать адаптивные веса русскоязычной " saiga_mistral_7b_lora ", которые натренировал Илья Гусев @Takagi рядом с оригинальной моделью - " Mistral-7B-OpenOrca ". Saiga_mistral_7b_lora распространяется по лицензии CC BY 4.0.
При наличии достаточно мощной GPU адаптивные веса, и саму модель можно скачать, запустить у себя локально и пользоваться offline.
Импортируем зависимости и загружаем модель с Hugging Face:
Загружаем "sentence-transformers/all-MiniLM-L6-v2" для получения ембеддингов.
Ниже - функция для получения эмбеддингов. На вход подаём строку, на выходе получаем torch.tensor размерностью (1, 384):
Создаём базу для эмбеддингов. В промышленных версиях приложений используются vector database, мы же для примера будем использовать обычный torch.tensor.
Подготовим краткое описание завода, которое будем подавать модели вместе с вопросами, чтобы модель была в курсе контекста.
Обернём промт в PromptTemplate из библиотеки langchain. Это аналог f-строки, только с возможностью передавать строку, как зависимую переменную, с последующей передачей ей аргумента.
Вот так выглядит пример из документации langchain:
Напишем функцию для генерации ответа моделью и парсинг ответа:
Пайплайн:
1. Создаём ембеддинг вопроса:
2. Считаем косинусный коэффициент ( Коэффициент Отиаи ) между эмбеддингом вопроса и эмбеддингами, которые лежат в базе:
Пока что в базе у нас нету эмбеддингов, поэтому вектор с коэффициентами Отиаи пуст:
3. Получаем ответ от модели, добавляем ембеддинг вопроса в базу ембеддингов, а ответ - в массив answers.
Как видим модель отвечает согласно переданному контексту. Вот так выглядит кусок в промте: "Открытая вакансия: Производственный работник - зарплата 150 тысяч рублей в месяц.".
Зададим ещё один вопрос и посмотрим коэффициент Отиаи:
Коэффициент Отиаи говорит, что новый вопрос не схож с предыдущими вопросами (для двух одинаковых предложений коэффициент Отиаи равен 1).
Сгенерируем ответ:
Ну и последний пример перед тем, как запустить цикл. Зададим вопрос, похожий на предыдущий и посчитаем коэффициент Отиаи:
Здесь мы уже видим, что один из ранее задаваемых вопрос похож на новый вопрос.
Остаётся только выбрать подходящий порог коэффициента Отиаи и запустить цикл, имитирующий запросы к нашему боту.
Из вывода выше мы видим, насколько часто DATABASE отдаёт ответы вместо модели. Сейчас там встречаются ошибки, но это можно поправить, увеличив порог коэффициента Отиаи и подправив промт-контекст.
С увеличением количества вопросов база будет пополняться и отвечать на все однотипные вопросы без привлечения модели, а значит, экономить деньги или вычислительные ресурсы.
Репозиторий с кодом на GitHub.
Google Colab, где можно запустить код на бесплатной Т4 - RAG_LLM_with_cache.
