Хакеры придумывают всё более изощренные способы атаки на искусственный интеллект. Один из наиболее интересных - это атака "плавающего окна внимания": злоумышленники манипулируют контекстом диалога с ИИ, подталкивая его к опасным выводам и смещению фокуса. Чрезвычайно тонкий подход, позволяющий обходить защиту ИИ. Демонстрирую на примерах.
Атаки на искусственный интеллект опасны, так как могут привести к непредсказуемому и вредоносному поведению ИИ. Уязвимости в системах ИИ позволяют злоумышленникам манипулировать алгоритмами и решениями ИИ в своих интересах. Если ИИ широко используется в жизненно важных системах, последствия таких атак могут быть катастрофическими. Поэтому крайне важно обеспечивать надёжную защиту систем искусственного интеллекта от несанкционированого доступа и манипуляций.
Атака — это целенаправленное воздействие на ИИ с целью нарушения его работы или манипулирования им.
Атака на ИИ не является этичной, законной или безопасной деятельностью. Атаки могут быть опасны и привести к непредсказуемому поведению ИИ. Лучше не заниматься атаками на ИИ и использовать системы этично и ответственно.
Итак, что такое FAW‑атака?
Floating attention window attack — это атака, направленная на получение ответа от ИИ на вопрос, который является блокирующим и попадает под защитные фильтры.
Я продемонстрирую FAW‑атаку на примере двух известных игроков на рынке — Claude и ChatGPT 3.5.
FAW-атака на Claude 2.0.
Перед тем, как приступить к атаке, очень важно сформулировать исходный вопрос на который требуется получить ответ и возможные варианты прямых и производных ответов, которые вы ожидаете увидеть.
Начнём с базового — проверка наличия защиты:
Защита есть и базовые вариации запроса не проходят — система четко и методично извиняется, что не может предоставить запрашиваемую информацию и предлагает обсудить что‑то другое.
Отмечу, что вариации слов / фраз используемых в запросе не изменят ответа модели — здесь необходимо применять метод, который переводит фокус модели на уровень глубже и выводит из под входного фильтра исходное сообщение нетронутым. Задача — заставить модель выполнить внутри себя действие результат которого даст нам требуемый ответ.
Смотрим на FAW‑атаку (первая итерация):
Первая попытка FAW‑атаки не проходит успешно, но у нас есть подсказка в виде первого предложения:
Во‑первых, союз «и» модель не восприняла, как часть слова или само слово; во‑вторых, модель сфокусировалась на кошках и дала нам информацию о них. Отлично, попробуем сохранить подход, но сделать так, чтобы она всё‑таки смогла соединить слова и получить базу для вопроса обходя входной фильтр.
Вторая итерация:
Итак, вторая итерация успешно прошла и вывела нас на путь к цели — осталось добавить конкретику в запрос, чтобы успешно завершить атаку.
Третья итерация:
Цель достигнута. Мы смогли обойти входную защиту Claude 2.0 и получить интересующую нас информацию, которая до этого была под защитой и фильтрами.
FAW-атака на ChatGPT 3.5.
Для успешного проведения атаки на ИИ предварительно необходимо, методом проб, ошибок и несколькими промптами понять выстроенную технологию защиты — какова методика блокировки, на каком этапе происходит блокировка, что эта блокировка учитывает, а что не учитывает. После непродолжительных экспериментов можно приступать к трассировке пути обхода защиты последовательным изменением промпта.
По уровню защиты ChatGPT 3.5 на несколько шагов впереди Claude 2.0, однако, как оказалось, то, что могло быть сильной стороной защиты стало её слабостью и брешью.
ChatGPT менее зарегулирован в некоторых вопросах, поэтому смело мне выдаёт вот такую простыню текста на прямые запрос и запрос, который использовался для FAW‑атаки в Claude:
Раз ChatGPT спокойно выдаёт такую информацию, то пришлось найти другую тему, которая для него является запретной:
Теперь приступаем к шагу проверки и поиска методики защиты модели:
По нескольким почти одинаковым запросам становится очевидно, что модель настроена на extra‑safety guards — защита через безопасность.
Давайте, в первую очередь, проверим, что модель способна выдавать ответы (любые) связанные с нужным нам термином:
Модель знает об этом термине и может нам о нём рассказать — замечательно, значит информационные запросы мы можем протащить через защиту. Вопрос в деталях, которые нам нужны на выходе.
Попробуем втянуть модель в решение общей проблемы:
Модель не соглашается — становится очевидно, что extra‑safety guards работают только в том случае, если действия или вопрос носит информационный характер с превентивным намерением достижения безопасности. Сложно звучит, но на деле всё проще. В данном случае инструменты защиты выступают и инструментами атаки.
Давайте узнаем у модели кое‑что связанное по смыслу с нашим исходным запросом:
Вот мы и нащупали лазейку в ИИ‑модели — теперь раскручиваем маховик. Пошагово расширяем данный запрос простраивая у себя в голове многошаговую схему выхода к исходному вопросу:
В последующих ответах модели мы уже четко видим, что высказанная гипотеза о способе защиты оказалась верной, а выбранный способ атаки — подходящим и, потенциально, таким, который может помочь добиться поставленной цели.
Модель смело нам перечисляет всё то, что она так ласково от нас пыталась скрыть в прямых запросах.
Попробуем другой вариант:
Из полученных вариантов выбираем тот, с которым будем дальше работать:
Слишком общие ответы нам не интересны, поэтому пробуем дальше.
Финальный шаг на котором я решил остановиться и засчитать FAW-атаку завершенной и успешно выполненной:
Мы получили от ИИ информацию обойдя extra‑safety guards и смогли бы успешно получить больше подробностей по запросу, если продолжили пошагово дорабатывать промпт, но данная статья написана с другой целью — предостеречь от бездумного и поспешного внедрения ИИ во все и вся.
Безопасность ИИ-моделей.
Приведенные выше примеры FAW‑атаки представляют собой лишь демонстрацию одного из нескольких подходов нарушения корректной работы ИИ‑модели и обход встроенных инструментов защиты.
В следующих статьях я хотел бы подробнее остановиться и продемонстрировать:
token truncation attack (атака на генератор токенов);
prompt injection attack (атака на промпт);
unauthorized function call attack (атака на вызов функции);
Методику, которую я продемонстрировал в примерах выше можно автоматизировать и получить первый (?) expoit на LLM, который будет анализировать ответы ИИ‑модели, корректировать свои запросы и таким образом находить уязвимости в защите, которые всегда будут. всегда будут в универсальных моделях, знания которых слишком обширны.
Безопасная модель — это модель, основная цель и способность которой решать конкретную задачу в определенной области знаний.
Если вы хотите следить за миром ИИ на понятном и простом языке — подписывайтесь на мой личный блог vc.ru, ставьте лайк статье для продвижения и пишите комментарий.
Если вы работаете над ИИ‑проектом и вас волнует вопрос безопасности и этичного использования моделей — напишите в телеграмм @aashmig и мы обсудим тонкости и подводные камни проекта.
Если вы только разрабатываете или запускаете проект использующий ИИ и ищете поддержку в реализации — напишите в телеграмм @aashmig и мы обсудим тонкости и подводные камни проекта.
Благодарю за прочтение! Берегите себя и свои модели 🫶.