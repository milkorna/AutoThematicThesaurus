Привет, это команда ответов на вопросы Маруси. Мы все привыкли к тому, что голосовые помощники отвечают на любые вопросы. Не всегда правильно, но обычно вполне толково и с пользой. А вы когда‑нибудь задумывались, как это устроено? Сейчас расскажем на примере нашей Маруси.
Материал состоит из двух частей, это первая часть. В ней мы дадим поверхностный обзор того как устроена Маруся, локализуем место навыка «ответов на вопросы» и расскажем на концептуальном уровне, как можно решать эту задачу.
Почему вода мокрая?
Многие пользователи любознательны и ожидают, что Маруся сможет ответить на любые информационные вопросы. Например, «Сколько лет было Пушкину, когда он умер?», «Сколько кораблей было в экспедиции Магеллана?», «Почему унитаз так называется?» и множество других.
Очевидно, что писать ответы на все вопросы вручную бессмысленно, ведь количество вопросов бесконечно. Поэтому на помощь приходит машинное обучение в сочетании с поисковыми технологиями, сегодня это безальтернативный подход в любых голосовых помощниках. Благодаря ему Маруся может поддержать вас полезным ответом в любой жизненной ситуации, например: «Что такое стаксель?» или «Кто такой эндермен в майнкрафте?».
Что такое вопрос?
Прежде чем рассказывать про ответы, вкратце объясним, а как Маруся вообще понимает, что её спрашивают . Когда вы обращаетесь к ней, она это определяет, записывает вашу речь и отправляет на сервер. Там голос распознаётся и преобразуется в текст (ASR — Automatic Speech Recognition). Теперь нужно классифицировать обращение: пользователь захотел просто поболтать с Марусей, дал ей команду или о чём‑то спросил? Этим занимается компонент «матчинг». Он выбирает подсистему, ответ из которой наилучшим образом соответствует запросу пользователя. Некоторые вопросы сильно похожи на фразы для других скиллов Маруси и от качества работы матчинга сильно зависит конечный результат (см. таблицу с примерами).
Как видите, даже небольшие различия в формулировке могут приводить к изменению скилла, а значит и другим результатам. В скилл ответов на вопросы отправляются самые разные запросы, как сформулированные в виде вопросов («В каком году построили Великую Китайскую стену?»), так и выглядящие как поисковый запрос («Китайская стена, год постройки»), и в обоих случаях Маруся должна понять, что это информационный запрос и ей нужно найти и сообщить некую объективную информацию. А вот фраза «Маруся, как дела?» — тоже вопрос, но он является не информационным запросом, а приглашением к общению, это не информационная потребность. Такие фразы обрабатываются «болталкой», как и фразы о субъективном отношении Маруси к каким‑либо явлениям. В некоторых случаях, таких как погода, информационный запрос передаётся отдельной подсистеме, предоставляющей более удобные и актуальные данные для удовлетворения запросов пользователя.
Кроме того, вопросы можно поделить на хорошие и плохие . Хорошие — это ясно озвученные запросы, из текста которых полностью понятен смысл спрашиваемого. А плохие — это нечётко сказанные или обрывочные, которые для хорошего ответа требуется уточнить. Например, дети часто не могут внятно и чётко сформулировать вопрос к Марусе. Или человек начинает о чём‑то спрашивать, но задумывается, как бы это сказать, ну, это… и, вот, короче… и алгоритмы Маруси уже решили, что человек закончил говорить, и пытаются как‑то обработать услышанные слова. Хорошие вопросы идут по стандартному процессу формирования ответов, а плохие — по отдельному, их мы тоже обрабатываем, но это уже другая тема.
Отвечаем по-порядку.
Итак, система определила, что фраза пользователя — это информационный запрос. Если смотреть на проблему высокоуровнево, то есть несколько способов ответить на него:
Выбрать из отдельной базы готовых ответов на запросы . Такой подход оправдан по отношению к самым чувствительным темам, когда минимальные ошибки могут приводить к сильному негодованию пользователей. Эту базу заполняют наши редакторы.
Найти ответ в графе знаний . Граф знаний — это хранилище, в котором хранятся знания об объектах окружающего мира и связях между ними. Объектами могут быть различные достопримечательности, важные исторические события и личности. Связями выступают различные отношения в духе «находится в», «автор произведения», «место рождения» и т. д. Если запрос пользователя не касается чувствительных тем, то система вычленяет из него объекты и их свойства и ищет их в графе знаний. Такая подсиcтема позволяет достаточно быстро и точно отвечать на простые запросы вида «Кто убил Пушкина?».
Если же запрос не относится к первым двум категориям, то включается механизм поиска информации в сети и компилирования ответа . О нём подробно пойдёт речь во второй части этой статьи.
Наконец, можно делегировать задачу генеративной модели и надеяться, что в ответе не будет галлюцинаций. До выхода в свет моделей типа ChatGPT ответы генеративных моделей оставляли желать лучшего из‑за маленьких неточностей, ломающих корректность ответа ( хороший разбор ChatGPT здесь ). Но, скорее всего, в ближайшем будущем мы можем ожидать позитивных сдвигов в этом направлении.
Важный нюанс: если пользователь спрашивает Марусю в фирменном приложении, то там она может ответить целым абзацем текста. А когда спрашивают о чём‑нибудь умную колонку, то не ожидают, что она в ответ разразится речью минуты на полторы, то есть при работе через колонку ответ должен быть короче, но с сохранением информативности. Именно о таких ответах мы и будем дальше рассказывать.
В самом начале Маруся могла отвечать только заранее заготовленными ответами или с помощью графа знаний. Это сильно ограничивало её возможности, ведь фактоиды — относительно простые вопросы, которые легко укладываются в граф знаний — составляют лишь небольшую долю от всего потока запросов к Марусе. Нам очень нужно было решение, позволяющее отвечать на остальную, львиную долю запросов людей.
Следующим шагом было использование результатов собственного поиска Mail.ru для нужд Маруси и демонстрация сниппета выдачи — текстового фрагмента с одной из найденных поисковиком страниц, идущего после заголовка. По идее, сниппет должен вам показать, что на этой странице есть то, что вы ищете.
Но это решение не помогло нам значительно увеличить количество хороших ответов из-за того, что система сниппетов изначально создавалась под сценарий веб-поиска, который значительно отличается от взаимодействия в режиме голосового помощника.
Генеративные модели.
Логично, что в какой‑то момент мы обратились к модным генеративным моделям, таким как GPT и T5. Такие модели получают на вход некоторый текст (например, текст вопроса) и на его основе генерируют какой‑то ответ. Эта методика оказалась лучше сниппетов, но тоже не без недостатков.
Во‑первых, генеративные модели моложе 2023 года про окружающий мир знают только из текстов, которые они видели на этапе предварительного обучения, и, следовательно, пытаются писать похожий текст. Но мир меняется, и информация, которую «запомнила» нейросеть, устаревает. Например, где‑то выбрали другого президента, обновился мировой рекорд, Месси сменил клуб — и всё, модель отстала от жизни. То есть вам в любом случае нужно обеспечить себя достаточным количеством актуальных фактов о внешнем мире.
Во‑вторых, такие нейросети не всегда генерируют идеальный текст: даже при верно понятой теме они могут «уделять внимание» не тем деталям или давать несодержательный ответ.
Примеры проблемных ответов генеративных моделей середины 2022 года:
С этими недостатками можно бороться, увеличивая размер нейросети, количество и актуальность обучающих данных, а также усложняя процедуру обучения. Но вам в любом случае нужны примеры хороших ответов на задаваемые вопросы.
Поэтому главным локомотивом нашего решения являются дискриминативные модели, извлекающие ответы из текстов в интернете. Именно такие модели сейчас дают большую часть ответов на вопросы в Марусе.
Дискриминативные модели.
Как же мы находим ответы в интернете? После первичного препроцессинга запроса система обращается к Поиску и получает от него десять первых найденных документов в виде заголовка и тела страницы, затем делит полученные тексты на отдельные предложения и решает, является ли каждое из них ответом на вопрос. Ранжирование выдачи для ассистента и выделение ответа из текста происходит с использованием трансформеров, таких как Roberta и Alberta.
Более подробно о технической стороне вопроса мы поговорим во второй части. Здесь же вкратце ограничимся перечислением некоторых требований, удовлетворение которым представляет определённые вызовы:
Ответ должен быть получен достаточно быстро. Но последовательный вызов поиска и извлечение ответа с использованием больших трансформеров может занимать значительное время.
Пользователю нужно показывать с одной стороны достаточно информативный кусок текста, а с другой стороны излишне длинные и/или избыточные ответы могут вызывать дискомфорт у слушателя колонки.
Оценка качества и разметка данных. Повышение качества.
Каждый день мы отслеживаем качество всего, что говорит Маруся пользователям, а не только качество ответов на информационные запросы. Конечно, это слишком большой объём информации, и для анализа качества всех ответов потребовался бы огромный штат людей, что слишком дорого даже для большой корпорации. Поэтому мы проверяем лишь контрольные срезы в разных категориях, в том числе и в ответах.
Но чтобы понять, что ответ был хорошим, нужно сначала обучить асессоров тому, что такое «хороший ответ». Для этого мы написали и развиваем инструкцию для службы оценки качества. Если коротко, то самая‑самая первая инструкция сводилась к следующему: асессоры классифицировали пары вопрос+ответ на 3 класса:
0 — если они не видели ответа на вопрос в предлагаемом тексте;
2 — если они видели, что ответ есть;
1 — если они чувствовали, что этот текст вроде бы, в тему, но как будто бы чего‑то в нём не хватает.
Промежуточный класс крайне важен, потому что при разметке он позволяет избегать ошибок, в которых не особо качественный ответ признаётся хорошим и получает метку 2. При обучении и подсчёте метрик качества такие пограничные ответы считались плохими.
Примеры разметки для вопроса «До скольки можно слушать громко музыку?»:
В процессе развития моделей и роста качества ответов возникало очень много особых случаев, и под каждый особый случай мы начали детализировать инструкцию, чтобы можно было учитывать в наборах данных всё более узкие сценарии в информационных запросах.
Сегодня при проверке качества ответов Маруси мы учитываем всевозможные признаки, в зависимости от которых асессоры ежедневно размечают данные для дальнейшего улучшения моделей. А чтобы уменьшить влияние человеческого фактора при разметке, мы кроме качества ответов контролируем ещё и качество разметки. Если мы видим, что кто‑то из асессоров систематически проставляет некорректные метки, если его мнение постоянно расходится с мнением нейросетей, то мы ему на это указываем.
Кроме того, мы ведём статистику, как оценивают Марусю сами пользователи: человек покупает умную колонку и спустя какое‑то время мы просим оценить, порекомендует ли он её кому‑нибудь, спрашиваем, что понравилось, а что нет. И по мере обновления наших нейросетей оценка пользователей растёт. Люди замечают, что колонка стала умнее, пишут: «Клёво. Она отвечает на вопросы».
Ближайшее будущее нашей системы ответов на запросы.
Текущее решение не лишено недостатков. Например, сейчас наши модели используют от одного до трёх предложений для формирования ответа. Иногда этого объёма недостаточно. Иногда пользователи не дослушивают ответ из‑за того, что он слишком длинный.
То есть на уровне интуиции всем понятно, что на вопрос «Кто такой Пушкин?» достаточно сказать, что он великий российский поэт XIX века, а всё остальное — ненужные подробности. А вот на фразу «Расскажи про Пушкина» отличным будет ответ подлиннее. Другой пример, на вопрос: «Сколько кораблей было в экспедиции Магеллана?» хорошим ответом будет: «В экспедиции Магеллана было пять кораблей»; мы же рассказываем про саму экспедицию, и в том же ответе еще называем число кораблей. Поэтому в будущем Маруся может начать отвечать короче, а если пользователю понадобятся подробности, он сможет её расспросить. Для этого какие‑то дополнительные факты мы будем находить контекстно.
Иногда пользователи просят Марусю «рассказать что‑нибудь интересное». В таких случаях Маруся сама предлагает темы, которые могут заинтересовать человека — разумеется, такие, на которые смогут поддерживать разговор наши модели. Мы считаем, что эту функцию нужно глубже интегрировать с другими подсистемами Маруси. Например, для проработки неявных сценариев удовлетворения информационных запросов: вроде как человек хочет развлечься, но делать это как‑то интеллектуально.
Пока что это все планы, о которых мы готовы рассказать, следите за релизами и увидимся во второй части, в которой мы расскажем о том, как устроен процесс поиска ответов на вопросы, и о том какие есть вызовы с данными для обучения и производительностью.
