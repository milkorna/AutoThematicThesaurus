Привет, Хабр!
Меня зовут Николай Шукан, я Data Scientist и участник профессионального сообщества NTA . Сегодня речь пойдет о методах снижения размерности эмбеддингов для задач определения семантического сходства предложений.
Для чего это необходимо? С каждым годом растет сложность моделей, решающих вопросы семантически‑ и контекстно‑ориентированной обработки естественного языка ( NLP ). Также нельзя забывать и про проблемы мультиязычности моделей. Все это сильно сказывается на увеличении их размеров и системных требований к железу для их обучения, дообучения, да и просто запуска. Задачи NLP сегодня — это прикладные задачи, их хочется решать на доступном оборудовании и за разумное время.
А если поконкретней? Перед мной стояла задача найти и обобщить текстовые данные, представляющие собой массив предложений. Я точно знал, что среди них есть семантически схожие фразы. Однако прямой подход для определения семантического сходства наборов фраз требовал много памяти и времени. Чтобы решить эту проблему, я попытался уменьшить размерность векторов признаков предложений, но как понять, когда остановиться и что это даст?
Ну и как понять? В рамках данного поста посмотрим, как меняется оценка семантического сходства от изменения размерностей эмбеддингов разными классическими методами их уменьшения.
Введение.
Мера степени семантического сходства между парами предложений — любопытный аспект NLP. Отлично справляется с группировкой схожих по смыслу предложений и предикатов, выискивает повторы, убрав которые можно снизить нагруженность модели, которая будет обрабатывать наши текстовые данные.
Для решения задачи семантического сходства между предложениями нужно преобразовать их в вектор. Для этого воспользуемся эмбеддингом: на вход подается набор предложений (в нашем случае — два для попарного сравнения), а на выходе преобразуется в числовые векторы этих предложений. Собственно, под эмбеддингом будем понимать результат преобразования текстовой сущности в числовой вектор. Модель преобразования выглядит так:
На сегодня самый простой и эффективный способ создания контекстуализированных эмбеддингов — использовать трансформер , модель машинного обучения, отлично зарекомендовавшую себя в сфере NLP.
Архитектурно трансформеры схожи с RNN (система кодировщик‑декодировщик). Кроме того, они также предназначены для обработки последовательностей. Архитектура трансформера выглядит следующим образом:
В отличие от RNN трансформеры не требуют обработки последовательностей по порядку: если входные данные это текст, то трансформеру не требуется обрабатывать конец текста после обработки его начала. Благодаря этому трансформеры распараллеливаются легче чем RNN и оттого выигрывают в скорости обучения. Также важной особенностью данной архитектуры является механизм внимания — он фокусируется на важных с точки зрения контекста словах и отдает их напрямую в обработку. Благодаря этому трансформеры обладают хорошей долгосрочной памятью и лучшим умением учитывать контекст.
Все это делает трансформеры выбором номер один для решения нашей задачи семантического сходства.
Таким образом, получим следующую модель определения семантического сходства:
Функция метрики будет заключаться в определении близости полученных векторов. Для этого можно использовать разные инструменты, мы остановимся на косинусном сходстве:
где A и B — n‑мерные вектора, θ — угол между ними, A∙B — скалярное произведение векторов A и B, ||A|| и ||B|| — длины векторов в евклидовом пространстве, A i и B i — i‑ые компоненты векторов A и B соответственно.
Для уменьшения количества признаков предложений, воспользуемся классическими методами снижения размерностей. Цель — уменьшить количество слабо информативных признаков для облегчения модели, но не потерять в качестве информации, т. е. получить значение семантического сходства как минимум не значительно хуже, чем до уменьшения размерности.
Схема снижения размерности будет выглядеть следующим образом:
Полученные эмбеддинги пониженной размерности также отправляются на вход в функцию метрики, где уже вычисляется семантическое сходство.
Приступим к реализации описанной схемы.
В объятиях Hugging Face.
Для подбора датасета и создания модели семантического сходства я обратился к замечательной платформе Hugging Face.
STSb Multi MT — это набор мультиязычных переводов и англоязычный оригинал классического STSbenchmark. Датасет состоит из трех колонок: первое предложение, второе предложение и метрика их схожести от 0 до 5 (далее — эталонная оценка). Датасет разбит на 3 части — train, test и dev. Так как в рамках поста вопросы точной донастройки рассматриваться не будет, то ограничимся dev сплитом в 1,5 тыс. строк русскоязычной части датасета.
Первые пять строк датасета:
Среди моделей мой выбор пал на distiluse‑base‑multilingual‑cased‑v1 из семейства sentence‑transformers.
Архитектура модели выглядит следующим образом:
На вход в модель подается предложение. Оно проходит через слой трансформера (DistilBertModel) и преобразуется в эмбеддинг, который через слой пулинга попадает на полносвязный слой Dense с вектором смещения (bias) и тангенсальной активационной функцией. На выходе получаем эмбеддинг с размерностью 512.
Данная модель отображает предложения в 512-мерное векторное пространство.
Разберёмся с датасетом.
Обработаем наш датасет. Для чего нормализуем эталонную оценку, приведя ее к значениям от 0 до 1. Из модели вытащим эмбеддинги и рассчитаем косинусное сходство, которое и послужит основой для сравнения с целевой метрикой датасета и измененной метрикой после снижения размерности.
Начинаем снижение.
Теперь можем приступить к применению методов снижения размерности.
Я выбрал четыре метода доступных в модуле scikit‑learn.decomposition — Матричная декомпозиция:
PCA — Метод главных компонент.
FastICA — Быстрый алгоритм для Анализа независимых компонент.
Factor Analysis (FA) — Факторный анализ.
TruncatedSVD — Усеченное сингулярное разложение.
Для сравнения методов между собой и с эталонным значением воспользуемся евклидовым расстоянием:
где x — вектор эталонных оценок, y m — вектор приближений при сокращении размерности до m, n — число пар предложений, для которых надо рассчитать косинусное сходство, x i и y i — i‑ые элементы соответствующих векторов.
Таким образом, получим вектор эталонных оценок, основной вектор приближений (семантическое сходство), вектора приближений n‑ой размерности и i‑го метода (например, размерность 50 и метод ICA).
Приведем пример кода для метода ICA:
Итого, получим функцию зависимости евклидового расстояния и числа размерностей. Найдя локальный минимум евклидового расстояния до эталонной оценки на интересующем нас интервале [50; 450] размерностей, получим оптимальное количество размерностей, где нет существенных потерь информации.
Визуализируем рассчитанные данные:
Оранжевая пунктирная линия на графике (target) — это значение евклидового расстояния между вектором эталонных оценок и вектором семантического сходства (т. е. основным вектором приближения без уменьшения размерностей). С этим значением мы и будем сравнивать получившиеся функции методов снижения размерностей.
Из графика видно, что:
Алгоритмы ICA и FA отработали лучше всего и приблизились к эталонной оценке даже больше, чем target, с локальным минимум около 200 размерностей (что в 2.5 раза меньше начальных 512).
Алгоритм PCA показал себя чуть хуже, но при этом при 200 размерностях уже совпал с target.
Алгоритм TSVC в чистом виде не позволяет эффективно снизить количество размерностей.
Итог.
Рассмотренный алгоритм позволяет упростить модель за счет уменьшения числа избыточных, слабо информативных признаков, и это позволяет:
Снизить объем обрабатываемых многомерных эмбеддингов . Это также уменьшает объем задействуемой памяти и увеличивает скорость работы дальнейшей обработки этих данных. На конкретном примере сокращение объема данных составило около 60%.
Повысить эффективность модели . После подбора оптимального сочетания метода снижения размерности и целевого количества измерений удалось добиться сохранения качества данных и даже получить результат лучше, чем при исходных параметрах.
Посмотреть весь код можно под спойлером или на GitHub.
