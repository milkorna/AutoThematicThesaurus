Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа чат-бота.
В этом посте я рассказываю про первую многозадачную модель T5 для русского языка и показываю, как её можно обучить на новой задаче.
Зачем нужна русскоязычная T5.
T5 – нейросетевая модель для понимания и генерации текста. Изобрели её в работе от Google два года назад, и расшифровывается это название как text-to-text transfer transformer . Трансформер – это архитектура нейросетей, позволяющая извлекать из текста довольно объёмную информацию. Благодаря этой архитектуре модели типа BERT круто понимают тексты, а модели типа GPT весьма правдоподобно их генерируют. Text-to-text означает, что модель T5 принимает на вход тексты и "читает" их энкодером (как BERT), а потом "пишет" декодером новые тексты и отдаёт на выход. Слово transfer говорит о цели этой модели: она предобучалась восстанавливать пропущенные фрагменты текста, но при желании её можно дообучить на новые, более полезные задачи: перевод, перефразирование, суммаризация текстов, генерация диалоговых ответов, и т.п.
Гугл выпустил две версии T5: первая понимает только английский язык, зато дообучалась на 24 разных задачах, а вторая понимает 101 язык (включая русский), но умеет только заполнять пропуски в тексте. Поэтому я решил сначала ужать мультиязычную модель T5 (mT5) до двух языков: русского и английского, выкинув ненужные токены из её словаря и соответствующие строки из матриц входных и выходных эмбеддингов. Процесс подробно описан в этом посте , а в результате модель "похудела" с 2.2 до 0.9 ГБ, а значит, стала более удобной для применения. Эту уменьшенная модель я выложил под именем cointegrated/rut5-base . А дальше я пошёл по пути Google и дообучил свою русскую T5 решать одновременно несколько разных русских и англоязычных задач.
Многозадачная модель.
Для каждой задачи, как и в гугловской статье, я использовал свой префикс, который надо писать, отделив символом | , перед входным текстом. Сами задачи такие:
Перевод (префикс translate ru-en или translate en-ru ). Обучал на датасете opus_wikipedia.
Перефразирование ( paraphrase ). Обучал на датасете tapaco.
Заполнение пропусков в тексте ( fill ). Пропуски можно обозначать как ___ или _3_ , где 3 – примерное количество слов, которые надо вставить. Обучал на корпусе ru_web-public_2019_1M из Лейпцигской коллекции.
Восстановление текста из зашумлённого мешка слов ( assemble ). Обучал также на Лейпцигском корпусе.
Упрощение текстов ( simplify ). Обучал на данных дорожки RuSimpleSentEval.
Диалоговый ответ ( reply для ответов в стиле художественной литературы, обученных на корпусе Козиева , и answer для ответов, обученных на otvet.mail.ru).
Ответ на вопросы по тексту ( comprehend ). Обучал на датасете SberQUAD.
Задавание вопросов по тексту ( ask ), обучал также на SberQUAD.
Генерация заголовка к новостной статье ( headline ). Обучал на датасете Ильи Гусева по мотивам соревнования Телеграма.
Как же работает эта магическая функция generate ? Стандартный python и код из transformers , и дальше вы можете запускать любой из примеров выше. Попробовать это вы можете в демо-блокноте.
Весь код, которым я дообучал модель, есть в этом блокноте ; он не очень чистый, ибо я только экспериментировал, и содержит несколько задач, не вошедших в опубликованную версию модели. А сама модель выложена в каталог Huggingface: cointegrated/rut5-base-multitask.
Как обучать T5 на собственных данных.
Текущая версия модели задумывалась как демонстрация возможностей предобученной seq2seq модели и как болванка для последующего дообучения. Поэтому я особо не возился ни с подбором гиперпараметров, ни с качеством датасетов. Следовательно, если более внимательно поработать с датасетом и гиперпараметрами и дообучить модель на какую-то одну задачу, работать она будет ещё лучше. Например, так я уже дообучал её на задаче перефразирования . А здесь я покажу, как обучить модель отвечать на вопросы из кроссвордов, используя фреймворки transformers и pytorch (при необходимости, модель можно потом сконвертировать в tensorflow или другой формат). Полный код примера есть всё в том же демо блокноте.
Инициализировать модель можно многозадачной версией. Тут, как и везде в библиотеке transformers , model – это сама нейросеть, а tokenizer – это часть модели, ответственная за сопоставление текстов со словарём: разбиение текстов на числовые токены, и сбор текстов из токенов обратно. optimizer – это объект, отвечающий за градиентный спуск; он нужен только на время обучения.
Ниже  – полный код обучения модели.  Предполагается, что переменная pairs – это список из пар, состоящих из вопроса и ответа. На GPU одна эпоха обучения на 75 тысячах примеров занимает около 15 минут.
Три эпохи я выбрал от балды. Размер батча я выбрал опытным путём: максимальный, при котором хватает памяти на GPU. learning_rate , равный 1e-5 , я выставил, исходя из опыта: обычно при нём модель обучается не очень быстро, но качественно.
Код для генерации ответа обученной моделью весьма прост:
Посмотрим, насколько хорошо модель выучила свою тренировочную выборку.
Что ж, модель пытается, но часто "мажет". Возможно, стоит поучить её в течение ещё нескольких эпох. А теперь посмотрим, насколько хорошо модель справляется с ответами на вопросы, которые она не видела.  Кажется, довольно пристойно.
Обученную модель можно сохранить на диск, а потом, при желании, даже выложить на хаб Huggingface. Я выложил её под названием cointegrated/rut5-base-quiz.
Вместо заключения.
Предобученные seq2seq модели – это здорово. Сейчас ими можно решать много разных задач NLP, а Google считает, что вообще чуть ли не все. Моя моделька показывает, что отчасти это уже верно и для русского языка.