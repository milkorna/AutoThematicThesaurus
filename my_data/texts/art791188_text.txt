Всем привет. Хочу поделиться с сообществом небольшим опытом и наработками для исследования и развития языков, в особенности малых. Для большинства таких языков нет ни систем машинного перевода, ни виртуальных ассистентов, ни других языковых моделей. Основная проблема тут в недостатке данных и отсутствии большого интереса у крупных компаний в их развитии. Однако есть достаточно большое число людей, которым дорог их язык, и которые прикладывают усилия по его сохранению и развитию. Предлагаю обсудить это и познакомиться с инструментами, которые помогут не только собирать данные, но и делать на их основе полезные вещи, типа параллельных книг для изучения языка и систем машинного перевода.
Мы научимся:
Дообучать мультиязычные языковые модели, переводящие текст в векторное представление (эмбеддинги).
Использовать их для выравнивания текстов библиотекой lingtrain-aligner, извлекая из текстов параллельные корпуса..
Загружать датасеты и модели на HuggingFace, чтобы это было доступно всем.
Создавать из выравнивания параллельные книги для изучения языков.
Начнем собирать датасет инструкций на малых языках, чтобы языковые модели и виртуальные смогли понимать и общаться на чувашском, якутском, башкирском и других языках.
Все это в делается в виде открытых проектов и сообществ, поэтому приглашаю всех интересующихся изучением и поддержкой языков подключаться к нам , будет интересно.
Глава 1. Обучаем LaBSE марийскому языку.
Модель LaBSE — это языковая модель, которая переведет фразы типа «как дела?» и «how are you?» в векторы чисел типа (0.43.., 2.71..., 1.56.., ...) — эмбеддинги, причем чем ближе фразы будут по смыслу , тем меньше будет расстояние между векторами. Языков эта модель поддерживает более ста, передавать их в качестве параметров не нужно. Вот их список:
Есть ряд других подобных моделей, но LaBSE в целом является наиболее универсальной и качественной. Для оценки encoder-моделей есть тест MTEB и его рейтинг , оценку мультиязычных моделей можно посмотреть на вкладке Bitext Mining.
Все это дает нам следующие возможности. Если у нас есть два текста на разных языках, то мы можем посчитать для них такие эмбеддинги и выудить предложения, наиболее близкие по смыслу, получая кандидаты для параллельного корпуса. В случае же текстов с переводом, например, оригинал книги и её редакция на другом языке, мы можем полностью выровнять тексты и дополнительно получить параллельную книгу.
Список языков LaBSE довольно широк, особенно радует наличие разных языковых семей, это нам очень поможет. Однако, если мы хотим начать использовать её для малых языков, например, марийского, мокшанского или ингушского, то получим довольно низкое качество. Это качество будет варьироваться в зависимости от наличия родственного языка и объема данных при обучении модели. Башкирский язык будет кодироваться более-менее качественно благодаря наличию татарского, но их обоих можно улучшить при дообучении.
Для начала, как ни странно, нам понадобится параллельный корпус. Откуда его взять, если мы только начинаем работу с новым языком? Тут не обойтись без помощи языковых энтузиастов. На старте придется потратить некоторые усилия для сбора такого корпуса вручную. Это не так страшно как может показаться на первый взгляд. Существует большое количество уже сформированных языковых сообществ, в которых люди собирают параллельные и моно-корпуса чувашского, марийских, башкирского и других языков. Есть и просто отдельные люди, которые вручную составили параллельный корпус на несколько тысяч пар предложений, например, на ингушском. Призываю вас поделиться в комментариями такими примерами, это позволит объединить наши усилия по поддержке малых языков.
Для того, чтобы вам и другим было удобно пользоваться собранным корпусом, вы можете загрузить его на HuggingFace. Делается это просто. Если у вас, например, есть два файла, в одном N предложений на одном языке, например, на якутском, а в другом файле соответствующее количество строк на русском, то вот код для подготовки и загрузки датасета:
Как вы, наверное, заметили из заголовка, мы возьмем русско-марийский параллельный корпус. Отмечу здесь Андрея Чемышева, усилиями которого этот корпус прирастает новыми единицами, также Андрей очень помог с поиском редакций «Маленького принца» на малых языках России, об этом ниже.
Корпус уже приличного размера, больше 380 тысяч пар. Сразу скажу, что обучать можно и на меньшем количестве, это в любом случае позволит получать нам больше правильных совпадений и ускорять сбор корпуса.
Загрузим датасет:
В подготовленном датасете будут содержаться вот такие примеры:
На этом датасете будет происходить обучение LaBSE, но проверять качество мы должны на паре текстов, которых в корпусе нет . Иначе у нас будет утечка данных, то есть хорошее качество на тестах, но неизвестное при реальном использовании.
Подготовим такие тексты (artamonov_mhr.txt и artamonov_ru.txt) и создадим специальный объект для проверки качества ChainScoreEvaluator:
Полный код обучения будет по ссылке ниже, проверка качества идет через библиотеку lingtrain_aligner, о ней будет рассказано подробней.
Наши тексты являются переводами и мы ожидаем, что при идеальном выравнивании они должны были бы выстроиться в цепочку из последовательных предложений с учетом того, что по предложениям тексты разбиты по разному. Где-то переводчик склеил их, где-то разделил, как посчитал нужным (так и случается в реальных случаях). ChainScoreEvaluator оценит нам такую связность.
Дообучать проще всего через обертку от авторов библиотеки sentence-transformers. В ней есть все необходимые компоненты в виде целевых функций и кода для дообучения.
Вызов выглядит примерно так:
Параметры надо попробовать разные, главное понимать, что мы тут делаем. А делаем мы следующее — подаем модели на вход пары предложений и заставляем её приближать эмбеддинги для соответствующих пар и отдалять от всех остальных. Поэтому батч тут должен содержать по несколько примеров. Причем не факт, что большой батч будет увеличивать качество. Все зависит от ваших данных, попробуйте размер от 6 до 10.
На каждом шаге проверки (evaluation_steps) будет запускаться наш ChainScoreEvaluator, который создаст базу данных для выравнивания, разобьет тексты по предложениям, посчитает текущей моделью эмбеддинги, алгоритмически их сопоставит и разрешит простейшие конфликты выравнивания (один из трех этапов). Подробнее про алгоритм и библиотеку можно почитать здесь . Под конец будет нарисована визуализация выравнивания и история подсчета chain_score метрики.
Выглядит это примерно так:
Видим, что метрика отражает наличие связных кусков в нашей тестовой паре, то есть модель научилась лучше сопоставлять предложения по смыслу на марийском и русском языках.
Вот то же самое выравнивание в начале обучения:
Лучшая по метрике модель сохраняется в папке ./output/best_model_{lang1}_{lang2}. Можно загрузить её на HuggingFace:
После этого модель станет доступна нам на следующих шагах.
Код.
1_prepare_dataset.ipynb.
2_finetune_labse.ipynb.
Глава 2. Извлекаем параллельный корпус новой моделью.
По итогам первой главы мы научились оформлять корпус, обучать модель и загружать их на HF, чтобы все могли этим воспользоваться. Теперь мы попробуем максимально автоматически извлечь из двух текстов соответствующие пары предложений.
Наши тексты должны представлять из себя два одинаковых по смыслу документа, например, какой-нибудь роман или рассказ на одном языке и его перевод на другой. Это позволит нам извлечь много соответствующих пар, так как мы будем использовать априорную информацию о том, что в целом «поток смысла» развивается одинаково, хотя разбиение по предложениям может значительно отличаться. Где-то переводчик опустит предложение, где-то склеит три в одно, а где-то добавит от себя.
Поможет в этом мой пет-проект в виде python библиотеки lingtrain-aligner. Подробнее о нем можно почитать тут и тут . А сейчас я расскажу о там как им пользоваться. Библиотека умеет сохранять информацию об авторе и названии текста, а также о главах, если проставить для них специальные метки. Кроме того, на основе таких меток мы можем выравнивать тексты по сегментам, давая дополнительные подсказки алгоритму.
Вот наш исходный текст на марийском:
Проставим метки для глав и проследим, что в тексте нет номеров страниц, сносок и другой посторонней информации. Должно получиться примерно так:
Проделаем то же самое с текстом на русском. Количество меток должно совпадать. Можно расставить эти метки в дополнительных местах, где смысл текста совпадает. Тогда сегментов станет еще больше и качество дополнительно возрастет, однако это требует знания языка, с которым вы работаете.
Прочитаем тексты, разобьем их по предложениям и создадим объект выравнивания (SQLite база данных):
Выравнивание идет по батчам — отрезкам текста по N предложений с нахлестом, чтобы искать соответствия только поблизости а не в отдаленных кусках текста.
Из нового. Сегментация текстов, дополнительно повышающая качество. Мы можем посмотреть на структуру сегментов, которая будет использоваться при выравнивании:
В нашем тексте 7 глав, каждая будет выравниваться независимо от другой, в каждой главе от 1 до 3 батчей в зависимости от длины главы.
Запустим выравнивание:
Важными параметрами тут являются shift и window. Про них можно почитать в этой статье. Их удобно использовать в приложении, о нем ниже.
После первичного выравнивания мы увидим визуализацию, похожую на предыдущие. Вот интересный кусок:
Видно, что есть разрывы, свидетельствующие о том, что есть куски текста на марийском (mhr), которых нет в тексте на русском. Кроме того, сами линии довольно «пушистые», давайте это поправим.
Выполним код, который разрешит конфликты — несоответствия между цельными цепочками,  которые мы считаем правильно подобранными парами.
Из нового. В последней версии библиотеки я добавил возможность исправления случая, когда модель случайно выбирает две последовательные пары в неправильном месте и такой конфликт не разрешается, это еще немного улучшило качество выравнивания.
ПервичноеС разрывами мы ничего сделать не сможем, тут надо добавлять пропавшие куски в текст. Отрезки без разрывов выглядят так:
Все. Теперь можем извлечь пары предложений:
Дополнительно откинем пары, которые сильно отличаются по длине предложений и получим массив кандидатов в параллельный корпус.
Теперь нужно привлечь носителя языка и отсмотреть полученные пары. Айгиз Кунафин при работе с башкирским корпусом использовал telegram-бота для валидации, чтобы массово провалидировать кандидаты при помощи большого числа носителей языка.
Код.
3_extract_pairs.ipynb.
Пример телеграм-бота для валидации результатов.
Глава 3. Улучшаем выравнивание и делаем из него книгу.
Как вы заметили на визуализации, не все конфликты удалось разрешить автоматически. Чтобы вручную довести выравнивание до идеала, из которого можно сделать полноценную параллельную книгу, нужно воспользоваться еще одной наработкой — Lingtrain Alignment Studio. Это веб-приложение, в котором можно делать все то же самое, что и с библиотекой, но через интерфейс. Плюс там есть редактор, возможность загрузить и выгрузить внешнее выравнивание в .lt формате. И сделать параллельную книгу с подсветкой соответствий предложений. Выглядит результат так:
Подробнее про интерфейс и систему разметки можно почитать в статье Lingtrain. Приложение для создания мультиязычных книг и параллельных корпусов.
Надеюсь, вам понравится проект и у вас появятся идеи по его развитию и улучшению. Напишите об этом в комментариях.
Запуск с кастомной моделью.
Из нового. Появилась возможность подключить внешнюю модель. Например, обученную в начале статьи и размещенную на HuggingFace. Это позволит нам использовать приложение для нового языка, разрешить оставшиеся конфликты, получив еще более полный параллельный корпус, а при желании и книжку.
Сделать это проще всего так. Клонируем репозиторий проекта:
Меняем параметр MODEL_NAME в файле .env на вашу модель. Это должна быть SentenceTransformers модель, размещенная на HF (мы проделали это в первой части статьи).
Ваши данные будут храниться в папке ./dev/data, при перезапуске они не потеряются. При первом запуске процесса выравнивания модель скачается из интернета и сохранится в папке ./dev/models_cache.
Собираем образ:
Запускаем его:
Приложение запустится на http://localhost:80. Про интерфейс подробно рассказывается здесь.
Приложения.
По ходу развития проекта появлялись различные идеи. Например, генерация параллельной книги в PDF формате, с обложкой и содержанием. Была довольно муторная работа, но в итоге получилось создать вещи вот в таком виде:
Есть возможность добавлять подсветку и даже фонетические подсказки в случае иероглифических языков.
Примеры использования скриптов я описывал в статье DIY. Книги для всех, даром.
Глава 4. «Малый принц» и книжка-трансформер.
Отдельно хочу сказать про одно из ответвлений проекта. Как-то пришла идея, что можно сделать не просто параллельную книгу, а многоязычную. Написав дополнительные скрипты по объединению нескольких выравниваний в одно, получилось из кучи редакций популярного художественного произведения сгенерировать книжки на десяти и более языках. А попутно сделать и соответствующие мультиязычные параллельные корпуса.
К книге я добавил возможность менять шрифты, добавлять иллюстрации и подсветку. Все это собирается в веб-сайт и размещается на GitHub'е как обычный репозиторий.
Вот пример того, как выглядит «Маленький принц» на малых языках России:
Книжку можно почитать здесь — https://averkij.github.io/prince.
Благодаря языковому сообществу удалось собрать редакции этой книги на чувашском, татарском и башкирском, дигорском, эрзянском и мокшанском, марийском и горномарийском, коми, балкарском, якутском и других языках.
Все это я выровнял в нашем приложении и составил книгу. А параллельный корпус на этих языках доступен на HuggingFace — https://huggingface.co/datasets/lingtrain/minor-prince.
Похожую вещь я проделал с «Мастером и Маргаритой» Булгакова:
В редакциях романа на популярных языках недостатка нет, хотя в некоторых из них, например, в найденном польском переводе, не оказалось нескольких кусков, поэтому пришлось его не включать.
Такой подход еще сыграет свою роль при создании датасетов инструкций на редких языках.
Глава 5. Проект SuperMinor.
Здесь я хочу анонсировать еще один проект по развитию малых языков, который направлен на сбор и перевод данных для обучения языковых моделей в инструктивном формате. Виртуальные ассистенты, будь то ChatGPT или GigaChat, наиболее качественно работают только с популярными языками, да и то не со всеми. Про малые языки типа якутского или чувашского и говорить не приходится.
На каникулах я написал небольшой портал, на котором носители малых языков смогут без лишних заморочек переводить и редактировать специально выбранные небольшие тексты в виде инструкций. Инструкции представляют собой наборы фраз, имитирующих общение с ассистентом. Обучаясь на таких текстах, модель учится общаться на новом для себя языке, что позволит создавать все виды генеративного контента и нейросетевых приложений на башкирском, татарском, марийских, осетинских и других языках, для которых мы соберем данные.
Данные будут дополнительно валидироваться, все будет находиться в открытом доступе. Сейчас портал подготавливается для запуска и после тестирования с первыми языковыми энтузиастами будет запущен для всех желающих. Про это я напишу в наши каналы — Lingtrain и градиент обреченный , в которых рассказываю про проекты, связанные с языками, и про машинное обучение в целом.
Ссылки.
Чат языковых энтузиастов. Вам сюда.
Jupyter Notebook. Загрузка датасета на HuggingFace.
Jupyter Notebook. Дообучение LaBSE модели.
Jupyter Notebook. Автоматическое извлечение параллельного корпуса.
Colab. Извлечение корпуса (подложите свои файлы).
Пример tg-бота для валидации параллельного корпуса от Айгиза.
HuggingFace. Lingtrain. Модели и параллельные датасеты.
GitHub. Lingtrain Alignment Studio. Веб-приложение.
GitHub. lingtrain-aligner. Python библиотека.
Статья про интерфейс приложения.
Статья с примером генерации PDF книги.
Статья Давида Дале про дообучение mBART на перевод для эрзянского языка.
Статья Давида про дообучение NLLB для тувинского языка.
Чат проекта Lingtrain.
Канал проекта Lingtrain.
градиент обреченный. Мой tg-канал.
