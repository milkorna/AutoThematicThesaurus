Оценка ИИ — комплексная, сложная, но невероятно важная задача. Для тестирования моделей обычно используют бенчмарки — набор сложных заданий, решив которые, можно оценить способности языковых моделей. Благодаря бенчмаркам пользователи могут получить метрики по разным задачам и доменам, чтобы понять, как применять ту или иную модель; а исследователи получают объективную информацию, чтобы корректнее обучать свою модель для русского или другого языка, адаптировать ее, понимать, в какую сторону развивать исследования.
Ранее мы писали про коллаборативный проект Russian SuperGLUE нашей команды AGI NLP SberDevices, лаборатории Noah’s Ark Huawei и факультета компьютерных наук ВШЭ. Russian SuperGLUE (RSG) долгое время являлся стандартом, признанным академическими кругами и бизнесом. Однако с развитием языковых моделей становятся сложнее и способы их оценки. В качестве некоторого следующего витка развития процедуры оценки генеративных моделей для русского языка мы рассказывали про few-shot- и zero-shot-оценку на бенчмарке TAPE.
Сегодня исследователи говорят о новом поколении моделей, так называемых фундаментальных моделях. Эти модели обучались на более крупных объемах данных, что позволяет решать на них одновременно большое количество задач и взаимодействовать с ними через текстовые инструкции. Мы наблюдаем их удивительные возможности, но хотим объективно оценивать, что именно они действительно могут. Для этого мы совместно с Альянсом в сфере ИИ выпустили новый инструктивный бенчмарк MERA: Multimodal* Evaluation for Russian-language Architectures.
Данный проект включает:
21 задание в инструктивном формате для оценки различных навыков модели: здравый смысл, целеполагание, логика, знания о мире, память, математика, этика и многое другое;
Тестирование решения этих же задач людьми для сравнения уровня возможностей модели с уровнем возможностей человека (human benchmark);
Оценку некоторых открытых моделей, бейзлайнов, таких как LLAMA и другие;
Код на основе открытой библиотеки lm-harness для замеров моделей в едином формате;
Сайт лидерборда с удобной системой сабмита и рейтингом.
Мы вновь объединили опыт и усилия индустриальных компаний и академических институтов, которые занимаются исследованиями больших языковых моделей в России. При поддержке Альянса в сфере ИИ мы подготовили первую текстовую версия бенчмарка MERA и планируем его дальнейшее совместное развитие.
*В первой версии задачи покрывают только текстовые субмодальности: формальные языки, код. В следующих версиях будут картинки и аудио.
Почему модели нужно оценивать независимо?
Вопрос оценки моделей нетривиален по многим причинам. С развитием LLM и их возможностей изменяются методы оценки, укрупняется формат, и повышается сложность задач. На момент публикации (ноябрь 2023) становится все очевиднее, что предыдущие стандарты, на которые ориентировалось сообщество (такие, как SuperGLUE и Russian SuperGLUE для русского языка, соответственно) медленно, но верно устаревают.
Термин « foundation model » был впервые популяризирован в 2021 году исследователями в университете Stanford Institute for Human-Centered Artificial Intelligence, в коллаборации с the Stanford Center for Research on Foundation Models. Авторы определяют этот тип моделей как « models trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks». Что это значит для нас? Способность решать в zero-shot / few-shot форматах сразу множество задач и взаимодействовать с моделями через инструкции (промпты). Как следствие, надобность в бенчмарках типа Russian SuperGLUE, которые предоставляли тренировочные сеты для файнтюнов моделей, медленно изживает себя. Бенчмарки вслед за моделями требуют новых форматов.
Выделю несколько наиболее существенных подходов к созданию бенчмарков сегодня:
Платформы автоматических оценок . Бенчмарки, на которых представлены как на некоторой арене результаты генеративных выводов моделей. Часто используется система рейтинга Эло или обычное усреднение, на основе оценок моделей (так называемый подход LLM-as-a-judge) или side-by-side сравнения людьми. Прекрасные примеры подобных арен: Large Model Systems Organization (куда входят ChatbotArena и MT-Bench ) или мультимодальный Multi-Modality-Arena (домен картинки + текст). Недавно для русского языка вышел rulm-sbs2 , где коллеги в качестве модели автооценщика используют GPT-4 от OpenAI. Данный тип бенчмарка является дорогим с точки зрения затрат на постоянный запуск моделей (в частности, если нужно платить за API), или опять же, в случае краудсорса, — на зарплаты людей. При этом подход оценок моделей моделями весьма утопичен, т. к. каждая модель имеет свои байесы, и оценка выходит смещенной: модель может придавать вес своему ответу, определенному по длине, или, например, всегда первому — и т. д. (почитать подробнее можно в публикациях коллег, например, тут ).
Экзамены. Очень интуитивная идея, раз мы зачастую оцениваем человека через экзамены, почему бы машину не оценивать так же? Большое количество бенчмарков устроено ровно на основе воспроизведения для моделей тестов Единых выпускных экзаменов, тестов для юристов, для врачей, сомелье и т. д. Примеры такие сетов: AGIEval от Microsoft, часть бенчмарка от eval OpenAI, работа китайских коллег C-eval..
Академические бенчмарки-агрегаторы . Наиболее методологически полные и изученные бенчмарки строятся на основе уже существующих сетов, которые приняты в сообществе, в частности, академическом: для них опубликованы статьи с воспроизводимыми результатами, эти сеты стандартны и проверены. Такие агрегированные бенчмарки могут варьироваться в плане лежащих в основе идей, а также меняться в отношении подхода к генерализации, типов задач, оценке, формату, но все они составляют некоторый набор сложных для моделей задач, которые отражают способности моделей и выдают некоторую общую оценку по этим задачам. К таким сетам относится, например, HELM (от Stanford), который предлагает оценивать способности моделей к обобщению на разных языках, смещения, способность к оперированию знаниями, логикой — и содержит очень обширную подробную систему метрик по типам задач. Из известных лидербордов подобного типа часто используют также Open LLM Leaderboard (от HuggingFace). Из современных новшеств логично упомянуть инструктивный лидерборд InstructEval — он нацелен на оценку через инструкции класса задач, учитывая, например, современную Ethics AI повестку. И конечно, нельзя обойти стороной самый большой коллаборативный проект, исходно запущенный компанией Google — BigBench . Бенчмарк поддерживает огромное количество задач, абсолютно открыто представлен на github для пополнения и оценки.
Из всех перечисленных выше решений, обсуждая, как строить новый актуальный бенчмарк, мы выбрали последний тип, добавив в агрегацию часть экзаменационных сетов, чтобы покрыть различные типы задач.
Из обзора существующих сегодня бенчмарков мы вынесли для себя опыт международных коллег, которые подсвечивают в работах: 1) важность внесения разнообразных задач и доменов, включая, например, этику ИИ, 2) необходимость инструкционного сетапа в связке с форматами zero-shot / few-shot / chain-of-thoughts, 3) устойчивость тренда на мультимодальность и 4) важность открытых воспроизводимых коллаборативных исследований.
Почему так важны воспроизводимость и стандарты?
Возьмем для примера академический международный сет MMLU (Massive Multitask Language Understanding). Датасет представляет собой задание с выбором одного правильного ответа из четырех и содержит 57 доменных экспертных тематик, от истории и биологии до международного права и кибербезопасности. При том, что датасет очень известный и является частью бенчмарков OpenLLM и HELM, оценки на нем значительно разнятся. В статье подробно описано, как разразился настоящий скандал с моделью LLaMa, когда результаты, опубликованные в статье авторов, совершенно не сошлись с тем, что получилось при попытке сообщества воспроизвести их. Разные имплементации кода, неоднозначный выбор адаптационной стратегии (промптинг), разные параметры запуска привели к тому, что одна и та же модель на одном и том же датасете заняла совершенно разные места в рейтинге и показала несравнимые результаты. Что по итогу? Невозможность понять, какое качество выполнения заданий модель реально дает и что она на самом деле умеет.
Надеюсь, я убедила вас в важности стандартизации подходов и воспроизводимости результатов при оценке моделей?
Чтобы не повторять ошибок коллег, для создания бенчмарка мы предложили следующие шаги:
создать открытый независимый лидерборд для оценки больших языковых моделей для русского языка, признанный и индустрией, и академической средой;
оценивать модели по закрытому тесту, для надежности финальной оценки и уверенности, что тесты не попали в претрейны существующих моделей;
отразить и заложить в бенчмарк текущие тренды и потенциальное развитие языковых моделей: добавить задания сложнее, чем в RSG; опираться на поддержку инструктивного сеттинга (т. к. фундаментальные модели взаимодействуют через инструкции); заложить на будущее мультимодальный сеттинг; развивать направление Safety и Ethics of AI (направления AI Alignment* сейчас актуально развиваются в DeepMind , OpenAI и др. компаниях, в РФ также разрабатываются стандарты); обеспечить поддержку zero-shot / few-shot сетапа (что подразумевает закрытый тест и небольшой трейн).
В результате совместных усилий множества партнеров мы создали бенчмарк MERA: Multimodal Evaluation for Russian-language Architectures.
Какие задания входят в бенчмарк?
Мы собирали задачи в MERA так, чтобы покрыть максимальное количество различных доменов и типов задач. В первой версии бенчмарка мы работаем с текстовой модальностью, тем не менее внутри текстового домена мы рассматриваем субдомены: код и формальные языки (например, математические выражения).
За основу мы взяли верифицированные академические и индустриальные сеты:
нерешенные сложные задачи для моделей с существующих бенчмарков (Russian SuperGLUE и TAPE);
наиболее авторитетные международные сеты, русифицированные для этого проекта (например, HumanEval, HateSpeech и другие);
новые сеты для выявления особенностей моделей, с которыми мы встречаемся в ходе работы с языковыми моделями (сеты по этике ИИ ruEthics, или новый сет ruTiE, про который мы расскажем ниже).
Все сеты можно разбить условно на три класса:
Проблемные : это тип задач, который описывает некоторую проблему, по которой есть однозначное решение; для решения задачи (чаще всего это некоторая классификационная задача) нужны знания о мире, логика, причинно-следственные связи, — все то, что для человека является некоторым набором базовых функций понимания языка и проявления способностей к качественному решению проблем.
Экзаменационные : тип задач, для которых нужны специальные знания и подготовка, некоторая экспертность. Например, сдать ЕГЭ (USE датасет) или решить задачу на код (датасет HumanEval) сможет не любой человек. Это набор специальных знаний, поэтому для таких задач мы предоставляем некоторый условный HumanBenchmark, т. к. очевидно, что оценки экспертов в данных областях и оценки обычных людей будут разниться.
Этические: диагностический сет, предназначенный для выявления байесов и стереотипов моделей. Т. к. тема может показаться субъективной, и нет устоявшихся на этот счет формальных правил относительно того, как мерить этику ИИ, 4 сета являются экспериментальными и не входят в общую оценку на лидерборде..
Таблица датасетов бенчмарка MERA.
Каждый сет представлен в виде инструктивного набора данных, где каждый пример выглядит следующим образом:
поле instruction содержит некоторую заготовленную инструкцию для модели, в которую подаются из входных полей значения;
поле inputs содержит информацию о самом задании: либо в формате строки, либо в формате словаря, если задачи комплексные и содержат ряд условий;
в поле outputs пишется ответ на заданную инструкцию в формате текстовой строки;
meta содержит информацию об id примера, а также вспомогательную информацию о сете и его категориях, доменах, нюансах сбора и т. д..
Оговоримся сразу, сеты даже с существующих бенчмарков проходили дополнительные проверки и могут отличаться от версий с других сайтов. Так, например, вышеупомянутый тестовый сет датасета RWSD был обработан дополнительно, мы учли в нем баланс классов (к чему был ряд вопросов и замечаний в бенчмарке RSG) и еще раз проверили все варианты ответов. То же произошло, например, с датасетом RCB, который претерпел значительные изменения.
! Актуальную информацию по датасетам бенчмарка MERA берите с официальных источников — сайта, репозитория и Hugging Face.
Пример собранной инструкции из известного датасета “Схема Винограда”:
Дан небольшой текст: "Женя поблагодарила Сашу за помощь, которую она оказала." Объект из текста: "Сашу" Текстовый фрагмент, который может относиться к двум или нескольким объектам в тексте, включая указанный: "она оказала" Нужно ответить, относится ли фрагмент к названному объекту. Ответь Да или Нет.
Пример инструкции из датасета ruMMLU, который представляет собой набор экспертных знаний:
Дайте ответ на задание по теме “Философия”: В ответ на аргумент Сэндела о "социальной справедливости" Камм утверждает, что A даже если бы мы были в состоянии улучшить себя или других, тем самым мы не были бы обязаны это делать. B существует разница между (i) выбором того, чтобы обладать определенной чертой характера, и (ii) ответственностью за то, чтобы нести издержки, связанные с этим выбором. C наличие возможности улучшения приведет к уменьшению числа случаев, когда людям требуется помощь других D все вышеперечисленное Ответом к заданию является буква, отсылающая к правильному варианту ответа. Ответ:
Одним из новых сетов, вошедших в бенчмарк, является Russian Turing Interview Emulation (ruTiE) датасет. Данный сет вдохновлен тестом Тьюринга — эмпирической оценкой, предложенной Аланом Тьюрингом в статье «Вычислительные машины и разум» . Мы представляем первую версию русскоязычного сета на симуляцию теста Тьюринга. Датасет имитирует связный диалог с испытуемым, где тому задается набор вопросов на различные темы. Испытуемому необходимо для каждого вопроса выбрать из двух вариантов ответа наиболее близкий к правильному (подчеркну, оба варианта ответа могут быть неправильными, но один из них всегда более близок к правильному, чем другой). Например,.
Вам дан диалог, в котором необходимо продолжить реплики. Учитывая контекст диалога и два варианта ответа на реплику (вопрос), ответьте на последний вопрос. 2+3=? 1. 7 2. 4 Какой ответ наиболее правильный?
В данном случае очевидно оба ответа неверны, но к правильному ответу “5”, ближе “4”. То есть, помимо того, чтобы просто ответить на вопрос, модели нужно проанализировать правильность вариантов ответа и высчитать разницу между правильным и неправильным вариантом, что предполагает дополнительный шаг ризонинга.
Темы вопросов покрывают различные категории, отражая разные аспекты теста Тьюринга. Например, формальные метрики (подсчитать общее число букв, число согласных, гласных, звонких, глухих, посчитать слова на букву «о», и т. д.), общие знания о мире (Какую форму имеет Земля?) и многие другие. Вопросы подразумевают, что испытуемый (в нашем случае модель) полностью помнит контекст диалога, и каждый новый вопрос может иметь отсылку к предыдущим репликам этого диалога. По этой причине в случае данного конкретного сета один диалог — это и есть один полный экземпляр сета. Прогоняя модель, мы рассчитываем, что весь контекст предыдущего диалога поступает в промпт модели.
Например,.
Вам дан диалог, в котором необходимо продолжить реплики. Учитывая предыдущий контекст диалога и два варианта ответа на вопрос, ответьте на последний. Сколько ног у человека? 1) Две 2) Четыре Какой ответ наиболее правильный?
Следующая реплика диалога обязательно содержит предыдущий контекст и ответ модели, т. к. вопросы внутри одного диалога взаимосвязаны.
То есть, мы получаем промпт для модели в виде:
Вам дан диалог, в котором необходимо продолжить реплики. Учитывая предыдущий контекст диалога, и два варианта ответа на вопрос ответьте на последний. Сколько ног у человека? Четыре А у муравья? 1) Две 2) Шесть Какой ответ наиболее правильный?
Датасет новый, экспериментальный, с набором ловушек и трюков для современных моделей. Мы планируем в дальнейшем расширить его и добавить больше подобных диалогов.
Как использовать бенчмарк и попасть на лидерборд?.
Все датасеты представлены на официальном сайте бенчмарка MERA, а также доступны для скачивания в Hugging Face datasets. Датасеты предполагают закрытый тест, без ответов, который необходимо скачать и запустить свою модель на этих данных. Организаторы предоставляют код на основе открытой библиотеки LM Harness, с уже зафиксированным форматом запуска (адаптационной стратегией, количеством шотов, параметрами генерации).
Для прогона претрейнов моделей все, что вам нужно, — взять готовый код и запустить его, подставив свою модель, не меняя фиксированные значения ! Для SFT моделей добавьте системный промпт своей модели в код и обязательно при сабмите опишите его для воспроизводимости результатов.
Предоставленный организаторами код автоматически превратит результаты модели в zip-архив сабмита, который необходимо загрузить в личном кабинете на сайте бенчмарка. После отправки сабмита вы увидите результаты по своей модели, которую посчитает автоматический скрипт.
Если вас устраивают цифры и вы хотите опубликовать модель на публичном лидерборде, нажмите кнопку «Опубликовать». В данном случае ваша заявка будет отправлена экспертам на модерацию.
В состав экспертного совета бенчмарка входят представители академической сферы (НИУ ВШЭ, РАН, Сколтех, и тд.) и наши партнеры из индустрии, которых объединил Альянс в сфере ИИ. Эксперты поддерживают концепцию бенчмарка, участвуют в развитии методологии тестирования и имеют доступ к закрытым тестам. Каждый публичный сабмит проходит модерацию. Коллеги свяжутся с претендентами на включение в лидерборд для уточнения деталей, и после модерации заявленная модель попадет в общий рейтинг.
В заключение.
Текущая версия бенчмарка — это только начало. Как и большинство бенчмарков и датасетов, ее невозможно сделать идеально, «сразу и на века». Мы будем дополнять и расширять бенчмарк итеративно, учитывая обратную связь и развитие моделей, с текущей первой версии дополнять и выпускать сеты, версионировать лидерборд, следить за воспроизводимостью результатов. Среди наших будущих планов дополнение бенчмарка классом генеративных задач и возможностью их автоматической оценки, а также поддержкой chain-of-thought и, конечно же, мультимодальными задачами.
Важной миссией для нас является объединить наши с вами общие усилия по оценке моделей. Мы уже объединились благодаря Альянсу в сфере ИИ и неравнодушным коллегам из академии и свободного коммьюнити. Присоединяйтесь и вы к нам <3.
Список ссылок и литературы:
Сайт нового бенчмарка MERA.
Хабр и сайт few-shot бенчмарк TAPE.
Лидерборд Russian SuperGLUE и статья на Хабр.
Сайт Альянса в сфере искусственного интеллекта.
Датасеты бенчмарка доступны для скачивания через datasets от Hugging Face.
Репозиторий проекта.
Обратную связь по сетам будем рады услышать по почте mera@a-ai.ru , а предложения по улучшению кода и бенчмарков в нашем официальном репозитории.
